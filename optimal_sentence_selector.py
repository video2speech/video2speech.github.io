#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import re
from collections import defaultdict, Counter
import nltk
try:
    from nltk.corpus import cmudict
    cmu_dict = cmudict.dict()
except:
    print("æ­£åœ¨ä¸‹è½½CMUè¯å…¸...")
    nltk.download('cmudict')
    from nltk.corpus import cmudict
    cmu_dict = cmudict.dict()

class OptimalSentenceSelector:
    def __init__(self):
        self.target_words = self.load_target_words()
        self.sentences = self.load_sentences()
        self.selected_sentences = []
        self.word_coverage = defaultdict(list)  # word -> list of sentence indices
        
        # CMUéŸ³ç´ é›†åˆï¼ˆ39ä¸ªï¼‰
        self.cmu_phonemes = {
            # å…ƒéŸ³ (15ä¸ª)
            'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW',
            # è¾…éŸ³ (24ä¸ª)
            'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH'
        }
    
    def load_target_words(self):
        """åŠ è½½ç›®æ ‡è¯æ±‡"""
        try:
            with open('selected_words.json', 'r', encoding='utf-8') as f:
                words = json.load(f)
                return set(word.lower().strip() for word in words if word.strip())
        except Exception as e:
            print(f"âŒ åŠ è½½ç›®æ ‡è¯æ±‡å¤±è´¥: {e}")
            return set()
    
    def load_sentences(self):
        """åŠ è½½å¥å­"""
        try:
            with open('selected_sentences_analyzer.json', 'r', encoding='utf-8') as f:
                sentences = json.load(f)
                print(f"ğŸ“ åŠ è½½äº† {len(sentences)} ä¸ªå¥å­")
                return sentences
        except Exception as e:
            print(f"âŒ åŠ è½½å¥å­å¤±è´¥: {e}")
            return []
    
    def clean_sentence(self, sentence):
        """æ¸…ç†å¥å­"""
        sentence = sentence.lower()
        sentence = re.sub(r'[^\w\s]', '', sentence)
        sentence = re.sub(r'\s+', ' ', sentence).strip()
        return sentence
    
    def extract_words_from_sentence(self, sentence):
        """ä»å¥å­ä¸­æå–è¯æ±‡"""
        clean_text = self.clean_sentence(sentence)
        return set(clean_text.split())
    
    def build_word_sentence_mapping(self):
        """æ„å»ºè¯æ±‡-å¥å­æ˜ å°„"""
        print("ğŸ”„ æ„å»ºè¯æ±‡-å¥å­æ˜ å°„...")
        word_sentence_map = defaultdict(list)
        
        for i, sentence in enumerate(self.sentences):
            words = self.extract_words_from_sentence(sentence)
            for word in words:
                if word in self.target_words:
                    word_sentence_map[word].append(i)
        
        # ç»Ÿè®¡æ¯ä¸ªç›®æ ‡è¯å‡ºç°åœ¨å¤šå°‘å¥å­ä¸­
        coverage_stats = {}
        for word in self.target_words:
            count = len(word_sentence_map[word])
            coverage_stats[word] = count
            if count == 0:
                print(f"âš ï¸  è¯æ±‡ '{word}' æœªåœ¨ä»»ä½•å¥å­ä¸­æ‰¾åˆ°")
        
        print(f"ğŸ“Š è¯æ±‡è¦†ç›–ç»Ÿè®¡:")
        covered_words = sum(1 for count in coverage_stats.values() if count > 0)
        print(f"   ç›®æ ‡è¯æ±‡æ€»æ•°: {len(self.target_words)}")
        print(f"   æœ‰è¦†ç›–çš„è¯æ±‡: {covered_words}")
        print(f"   è¦†ç›–ç‡: {covered_words/len(self.target_words)*100:.1f}%")
        
        return word_sentence_map
    
    def greedy_sentence_selection(self, word_sentence_map, max_sentences=50, min_word_coverage=2):
        """è´ªå¿ƒç®—æ³•é€‰æ‹©å¥å­"""
        print(f"\nğŸ”„ å¼€å§‹è´ªå¿ƒé€‰æ‹©ç®—æ³•...")
        print(f"   ç›®æ ‡å¥å­æ•°: {max_sentences}")
        print(f"   æ¯ä¸ªè¯æœ€å°‘è¦†ç›–æ¬¡æ•°: {min_word_coverage}")
        
        selected_indices = set()
        word_coverage_count = defaultdict(int)
        
        # ä¼˜å…ˆé€‰æ‹©è¦†ç›–ç¨€æœ‰è¯æ±‡çš„å¥å­
        word_rarity = {}
        for word, sentence_list in word_sentence_map.items():
            word_rarity[word] = len(sentence_list)
        
        while len(selected_indices) < max_sentences:
            best_sentence = -1
            best_score = -1
            
            for i, sentence in enumerate(self.sentences):
                if i in selected_indices:
                    continue
                
                # è®¡ç®—è¿™ä¸ªå¥å­çš„ä»·å€¼å¾—åˆ†
                score = 0
                words_in_sentence = self.extract_words_from_sentence(sentence)
                
                for word in words_in_sentence:
                    if word in self.target_words:
                        # å¦‚æœè¿™ä¸ªè¯è¿˜æ²¡è¾¾åˆ°æœ€å°è¦†ç›–è¦æ±‚ï¼Œç»™æ›´é«˜åˆ†
                        if word_coverage_count[word] < min_word_coverage:
                            # ç¨€æœ‰è¯æ±‡ç»™æ›´é«˜åˆ†
                            rarity_bonus = 1 / max(word_rarity.get(word, 1), 1)
                            score += 10 + rarity_bonus * 5
                        else:
                            # å·²ç»æ»¡è¶³æœ€å°è¦†ç›–çš„è¯æ±‡ç»™è¾ƒä½åˆ†
                            score += 1
                
                if score > best_score:
                    best_score = score
                    best_sentence = i
            
            if best_sentence == -1:
                print("âš ï¸  æ— æ³•æ‰¾åˆ°æ›´å¤šæœ‰ä»·å€¼çš„å¥å­")
                break
            
            # æ·»åŠ æœ€ä½³å¥å­
            selected_indices.add(best_sentence)
            words_in_best = self.extract_words_from_sentence(self.sentences[best_sentence])
            
            for word in words_in_best:
                if word in self.target_words:
                    word_coverage_count[word] += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            covered_enough = sum(1 for count in word_coverage_count.values() if count >= min_word_coverage)
            total_target_words = len([w for w in self.target_words if w in word_sentence_map])
            
            print(f"   é€‰æ‹©ç¬¬ {len(selected_indices)} å¥: å¾—åˆ† {best_score:.1f} | "
                  f"å……åˆ†è¦†ç›–è¯æ±‡: {covered_enough}/{total_target_words}")
        
        # æ£€æŸ¥è¦†ç›–æƒ…å†µ
        print(f"\nğŸ“Š æœ€ç»ˆè¦†ç›–æƒ…å†µ:")
        insufficient_words = []
        sufficient_words = []
        
        for word in self.target_words:
            if word in word_coverage_count:
                count = word_coverage_count[word]
                if count >= min_word_coverage:
                    sufficient_words.append((word, count))
                else:
                    insufficient_words.append((word, count))
        
        print(f"   å……åˆ†è¦†ç›–çš„è¯æ±‡ (â‰¥{min_word_coverage}æ¬¡): {len(sufficient_words)}")
        print(f"   ä¸å……åˆ†è¦†ç›–çš„è¯æ±‡ (<{min_word_coverage}æ¬¡): {len(insufficient_words)}")
        
        if insufficient_words:
            print(f"   ğŸŸ¡ ä¸å……åˆ†è¦†ç›–çš„è¯æ±‡:")
            for word, count in sorted(insufficient_words):
                print(f"      {word}: {count} æ¬¡")
        
        return list(selected_indices), word_coverage_count
    
    def get_word_phonemes(self, word):
        """è·å–è¯æ±‡çš„éŸ³ç´ """
        word_lower = word.lower()
        if word_lower in cmu_dict:
            # å–ç¬¬ä¸€ä¸ªå‘éŸ³
            phonemes = cmu_dict[word_lower][0]
            # æ¸…ç†éŸ³ç´ ï¼ˆå»é™¤é‡éŸ³æ ‡è®°ï¼‰
            clean_phonemes = []
            for phoneme in phonemes:
                clean_phoneme = re.sub(r'\d', '', phoneme)
                if clean_phoneme in self.cmu_phonemes:
                    clean_phonemes.append(clean_phoneme)
            return clean_phonemes
        return []
    
    def analyze_selected_sentences(self, selected_indices):
        """åˆ†æé€‰ä¸­çš„å¥å­"""
        selected_sentences = [self.sentences[i] for i in selected_indices]
        
        print(f"\nğŸ“Š åˆ†æé€‰ä¸­çš„ {len(selected_sentences)} ä¸ªå¥å­...")
        
        # è¯é¢‘åˆ†æ
        word_counter = Counter()
        phoneme_counter = Counter()
        
        for sentence in selected_sentences:
            words = self.extract_words_from_sentence(sentence)
            for word in words:
                word_counter[word] += 1
                # éŸ³ç´ åˆ†æ
                phonemes = self.get_word_phonemes(word)
                for phoneme in phonemes:
                    phoneme_counter[phoneme] += 1
        
        # ç›®æ ‡è¯æ±‡è¦†ç›–åˆ†æ
        target_word_coverage = {}
        for word in self.target_words:
            count = word_counter.get(word, 0)
            target_word_coverage[word] = count
        
        return {
            'sentences': selected_sentences,
            'word_frequencies': dict(word_counter),
            'phoneme_frequencies': dict(phoneme_counter),
            'target_word_coverage': target_word_coverage
        }
    
    def display_analysis(self, analysis):
        """æ˜¾ç¤ºåˆ†æç»“æœ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š é€‰ä¸­å¥å­åˆ†æç»“æœ")
        print("=" * 80)
        
        # æ˜¾ç¤ºé€‰ä¸­çš„å¥å­
        print(f"\nğŸ“ é€‰ä¸­çš„ {len(analysis['sentences'])} ä¸ªå¥å­:")
        print("-" * 60)
        for i, sentence in enumerate(analysis['sentences'], 1):
            print(f"{i:2d}. {sentence}")
        
        # è¯é¢‘åˆ†æ
        print(f"\nğŸ“š è¯é¢‘åˆ†æ (æŒ‰é¢‘ç‡æ’åº):")
        print("-" * 60)
        word_freq = Counter(analysis['word_frequencies'])
        for word, freq in word_freq.most_common(30):  # æ˜¾ç¤ºå‰30ä¸ª
            status = "âœ…" if word in self.target_words else "â“"
            print(f"   {word:15s}: {freq:3d} æ¬¡ {status}")
        
        # ç›®æ ‡è¯æ±‡è¦†ç›–æƒ…å†µ
        coverage = analysis['target_word_coverage']
        covered_words = {w: c for w, c in coverage.items() if c > 0}
        uncovered_words = {w: c for w, c in coverage.items() if c == 0}
        sufficient_coverage = {w: c for w, c in coverage.items() if c >= 2}
        
        print(f"\nğŸ“‹ ç›®æ ‡è¯æ±‡è¦†ç›–æƒ…å†µ:")
        print(f"   æ€»ç›®æ ‡è¯æ±‡: {len(self.target_words)}")
        print(f"   æœ‰è¦†ç›–è¯æ±‡: {len(covered_words)} ({len(covered_words)/len(self.target_words)*100:.1f}%)")
        print(f"   å……åˆ†è¦†ç›–è¯æ±‡ (â‰¥2æ¬¡): {len(sufficient_coverage)} ({len(sufficient_coverage)/len(self.target_words)*100:.1f}%)")
        print(f"   æœªè¦†ç›–è¯æ±‡: {len(uncovered_words)} ({len(uncovered_words)/len(self.target_words)*100:.1f}%)")
        
        if uncovered_words:
            print(f"   ğŸŸ¡ æœªè¦†ç›–çš„ç›®æ ‡è¯æ±‡:")
            uncovered_list = sorted(list(uncovered_words.keys()))
            for i in range(0, len(uncovered_list), 8):
                group = uncovered_list[i:i+8]
                print(f"      {', '.join(group)}")
        
        # éŸ³ç´ é¢‘ç‡åˆ†æ
        print(f"\nğŸ”Š éŸ³ç´ é¢‘ç‡åˆ†æ (æŒ‰é¢‘ç‡æ’åº):")
        print("-" * 60)
        phoneme_freq = Counter(analysis['phoneme_frequencies'])
        for phoneme, freq in phoneme_freq.most_common():
            print(f"   /{phoneme:3s}/: {freq:3d} æ¬¡")
        
        # éŸ³ç´ è¦†ç›–æƒ…å†µ
        covered_phonemes = set(analysis['phoneme_frequencies'].keys())
        uncovered_phonemes = self.cmu_phonemes - covered_phonemes
        
        print(f"\nğŸ¯ 39éŸ³ç´ è¦†ç›–æƒ…å†µ:")
        print(f"   æ€»éŸ³ç´ : 39")
        print(f"   å·²è¦†ç›–: {len(covered_phonemes)} ({len(covered_phonemes)/39*100:.1f}%)")
        print(f"   æœªè¦†ç›–: {len(uncovered_phonemes)} ({len(uncovered_phonemes)/39*100:.1f}%)")
        
        if uncovered_phonemes:
            print(f"   ğŸŸ¡ æœªè¦†ç›–çš„éŸ³ç´ :")
            uncovered_list = sorted(list(uncovered_phonemes))
            for i in range(0, len(uncovered_list), 8):
                group = uncovered_list[i:i+8]
                print(f"      {', '.join('/' + p + '/' for p in group)}")
        
        # ç»Ÿè®¡æ‘˜è¦
        total_words = sum(analysis['word_frequencies'].values())
        total_phonemes = sum(analysis['phoneme_frequencies'].values())
        unique_words = len(analysis['word_frequencies'])
        unique_phonemes = len(analysis['phoneme_frequencies'])
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»è¯æ±‡å®ä¾‹: {total_words}")
        print(f"   ä¸åŒè¯æ±‡æ•°: {unique_words}")
        print(f"   æ€»éŸ³ç´ å®ä¾‹: {total_phonemes}")
        print(f"   ä¸åŒéŸ³ç´ æ•°: {unique_phonemes}")
    
    def save_results(self, selected_indices, analysis, filename="selected_50_sentences.json"):
        """ä¿å­˜ç»“æœ"""
        results = {
            'selected_sentence_indices': selected_indices,
            'selected_sentences': analysis['sentences'],
            'word_frequencies': analysis['word_frequencies'],
            'phoneme_frequencies': analysis['phoneme_frequencies'],
            'target_word_coverage': analysis['target_word_coverage'],
            'statistics': {
                'total_sentences': len(analysis['sentences']),
                'total_words': sum(analysis['word_frequencies'].values()),
                'unique_words': len(analysis['word_frequencies']),
                'total_phonemes': sum(analysis['phoneme_frequencies'].values()),
                'unique_phonemes': len(analysis['phoneme_frequencies']),
                'target_words_covered': len([w for w, c in analysis['target_word_coverage'].items() if c > 0]),
                'target_words_sufficient': len([w for w, c in analysis['target_word_coverage'].items() if c >= 2])
            }
        }
        
        # ä¿å­˜JSON
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_filename = filename.replace('.json', '_report.txt')
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write("é€‰ä¸­å¥å­åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("é€‰ä¸­çš„å¥å­:\n")
            f.write("-" * 30 + "\n")
            for i, sentence in enumerate(analysis['sentences'], 1):
                f.write(f"{i:2d}. {sentence}\n")
            
            f.write("\nè¯é¢‘ç»Ÿè®¡ (æŒ‰é¢‘ç‡æ’åº):\n")
            f.write("-" * 30 + "\n")
            word_freq = Counter(analysis['word_frequencies'])
            for word, freq in word_freq.most_common():
                status = "âœ…" if word in self.target_words else "â“"
                f.write(f"{word:15s}: {freq:3d} æ¬¡ {status}\n")
            
            f.write("\néŸ³ç´ é¢‘ç‡ç»Ÿè®¡ (æŒ‰é¢‘ç‡æ’åº):\n")
            f.write("-" * 30 + "\n")
            phoneme_freq = Counter(analysis['phoneme_frequencies'])
            for phoneme, freq in phoneme_freq.most_common():
                f.write(f"/{phoneme:3s}/: {freq:3d} æ¬¡\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"   ğŸ“„ {filename} (JSONæ ¼å¼)")
        print(f"   ğŸ“„ {txt_filename} (æ–‡æœ¬æŠ¥å‘Š)")

def main():
    print("ğŸ¯ æœ€ä¼˜å¥å­é€‰æ‹©å™¨")
    print("=" * 50)
    print("ç›®æ ‡: ä»350ä¸ªå¥å­ä¸­é€‰å‡º50å¥ï¼Œè¦†ç›–50ä¸ªç›®æ ‡è¯æ±‡ï¼Œæ¯ä¸ªè¯è‡³å°‘å‡ºç°2æ¬¡")
    
    selector = OptimalSentenceSelector()
    
    if not selector.target_words:
        print("âŒ æ— æ³•åŠ è½½ç›®æ ‡è¯æ±‡")
        return
    
    if not selector.sentences:
        print("âŒ æ— æ³•åŠ è½½å¥å­")
        return
    
    print(f"ğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"   ç›®æ ‡è¯æ±‡æ•°: {len(selector.target_words)}")
    print(f"   å€™é€‰å¥å­æ•°: {len(selector.sentences)}")
    
    # æ„å»ºè¯æ±‡-å¥å­æ˜ å°„
    word_sentence_map = selector.build_word_sentence_mapping()
    
    # è´ªå¿ƒé€‰æ‹©å¥å­
    selected_indices, word_coverage = selector.greedy_sentence_selection(word_sentence_map)
    
    # åˆ†æç»“æœ
    analysis = selector.analyze_selected_sentences(selected_indices)
    
    # æ˜¾ç¤ºåˆ†æ
    selector.display_analysis(analysis)
    
    # ä¿å­˜ç»“æœ
    save_choice = input("\nğŸ’¾ æ˜¯å¦ä¿å­˜ç»“æœ? (y/N): ").strip().lower()
    if save_choice in ['y', 'yes']:
        selector.save_results(selected_indices, analysis)

if __name__ == "__main__":
    main()


