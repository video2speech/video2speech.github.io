#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer

def analyze_expanded_vocab_results():
    """åˆ†æä½¿ç”¨æ‰©å±•è¯æ±‡è¡¨çš„ç­›é€‰ç»“æœ"""
    
    print("=" * 80)
    print("ğŸ¯ æ‰©å±•è¯æ±‡è¡¨ç­›é€‰ç»“æœæ€»ç»“")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('sentences_expanded_vocab.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å¥å­æ•°: {len(sentences)}")
        
        # ä½¿ç”¨åˆ†è¯å™¨åˆ†ææ¯ä¸ªå¥å­
        tokenizer = TreebankWordTokenizer()
        word_counts = []
        
        for sentence in sentences:
            tokens = tokenizer.tokenize(sentence)
            # åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            words = [token for token in tokens if token not in ".,!?;:()\"'-"]
            word_counts.append(len(words))
        
        print(f"   å¹³å‡è¯æ•°: {sum(word_counts)/len(word_counts):.2f} è¯")
        print(f"   æœ€çŸ­å¥å­: {min(word_counts)} è¯")
        print(f"   æœ€é•¿å¥å­: {max(word_counts)} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        dist_3 = sum(1 for c in word_counts if c == 3)
        dist_4 = sum(1 for c in word_counts if c == 4)
        dist_5 = sum(1 for c in word_counts if c == 5)
        dist_6_10 = sum(1 for c in word_counts if 6 <= c <= 10)
        
        print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒ:")
        print(f"   3è¯å¥å­: {dist_3} ({dist_3/len(sentences)*100:.1f}%)")
        print(f"   4è¯å¥å­: {dist_4} ({dist_4/len(sentences)*100:.1f}%)")
        print(f"   5è¯å¥å­: {dist_5} ({dist_5/len(sentences)*100:.1f}%)")
        print(f"   6-10è¯å¥å­: {dist_6_10} ({dist_6_10/len(sentences)*100:.1f}%)")
        
        # ä¸ä¹‹å‰ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸ä¹‹å‰å¯¹æ¯”:")
        print(f"   ä¹‹å‰ (48è¯è¯æ±‡è¡¨, >3è¯): 283ä¸ªå¥å­")
        print(f"   ç°åœ¨ (85è¯è¯æ±‡è¡¨, â‰¥3è¯): {len(sentences)}ä¸ªå¥å­")
        print(f"   å¢é•¿: {len(sentences) - 283} ä¸ªå¥å­ ({(len(sentences) - 283)/283*100:.1f}%)")
        
        # æ˜¾ç¤ºä¸åŒé•¿åº¦çš„å¥å­æ ·æœ¬
        print(f"\nğŸ“ å¥å­æ ·æœ¬:")
        
        # 3è¯å¥å­æ ·æœ¬
        three_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 3]
        if three_word_sentences:
            print(f"\n   3è¯å¥å­æ ·æœ¬:")
            for sentence in three_word_sentences[:8]:
                print(f"   â€¢ {sentence}")
        
        # 4è¯å¥å­æ ·æœ¬
        four_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 4]
        if four_word_sentences:
            print(f"\n   4è¯å¥å­æ ·æœ¬:")
            for sentence in four_word_sentences[:8]:
                print(f"   â€¢ {sentence}")
        
        # 5è¯å¥å­æ ·æœ¬
        five_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 5]
        if five_word_sentences:
            print(f"\n   5è¯å¥å­æ ·æœ¬:")
            for sentence in five_word_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # 6-10è¯å¥å­æ ·æœ¬
        longer_sentences = [sentences[i] for i, c in enumerate(word_counts) if 6 <= c <= 10]
        if longer_sentences:
            print(f"\n   6-10è¯å¥å­æ ·æœ¬:")
            for sentence in longer_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # ç­›é€‰æ¡ä»¶æ€»ç»“
        print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶æ€»ç»“:")
        print(f"   âœ… è¯æ±‡è¡¨: æ‰©å±•ç‰ˆ newtopwords.txt (85ä¸ªè¯)")
        print(f"   âœ… æ ¸å¿ƒè¯æ±‡: 50ä¸ªæœ€é«˜é¢‘è¯ (50%å£è¯­è¦†ç›–ç‡)")
        print(f"   âœ… æ‰©å±•è¯æ±‡: 37ä¸ªå®ç”¨è¯æ±‡ (hello, good, helpç­‰)")
        print(f"   âœ… é•¿åº¦é™åˆ¶: â‰¥3è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        print(f"   âœ… æ•°æ®æº: ç”µå½±å°è¯ (movie_lines.tsv)")
        print(f"   âœ… å¤„ç†è¡Œæ•°: 20,000è¡Œå¯¹è¯")
        
        print(f"\nğŸ’¡ è¿™{len(sentences)}ä¸ªå¥å­çš„ç‰¹ç‚¹:")
        print(f"   â€¢ ä½¿ç”¨85ä¸ªç²¾é€‰è¯æ±‡ï¼ˆæ ¸å¿ƒ+æ‰©å±•ï¼‰")
        print(f"   â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ3-10è¯ï¼‰")
        print(f"   â€¢ æ¥è‡ªçœŸå®çš„ç”µå½±å¯¹è¯")
        print(f"   â€¢ åŒ…å«3è¯çŸ­å¥ï¼Œæ›´é€‚åˆåˆå­¦è€…")
        print(f"   â€¢ è¯æ±‡æ›´ä¸°å¯Œï¼Œè¡¨è¾¾æ›´å¤šæ ·")
        
        return sentences, word_counts
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°å¥å­æ–‡ä»¶")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    sentences, word_counts = analyze_expanded_vocab_results()
    
    if sentences:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶: sentences_expanded_vocab.txt")
        print(f"ğŸ¯ {len(sentences)} ä¸ªé«˜è´¨é‡çš„åŸºç¡€è‹±è¯­å¥å­å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"ğŸš€ ç›¸æ¯”ä¹‹å‰å¢åŠ äº† {len(sentences) - 283} ä¸ªå¥å­ï¼Œè¯æ±‡æ›´ä¸°å¯Œï¼")

if __name__ == "__main__":
    main()
