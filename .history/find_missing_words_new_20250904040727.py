#!/usr/bin/env python3
# -*- coding: utf-8 -*-

def find_missing_words():
    """æ‰¾å‡ºåœ¨150è¯è¡¨ä¸­ä½†æ²¡æœ‰åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­å‡ºç°çš„è¯æ±‡"""
    
    print("ğŸ” æŸ¥æ‰¾åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡")
    print("=" * 70)
    
    try:
        # è¯»å–150è¯è¡¨
        with open('materials/150_words_list.txt', 'r', encoding='utf-8') as f:
            wordlist_150 = [line.strip() for line in f.readlines() if line.strip()]
        
        # è½¬æ¢ä¸ºå°å†™å¹¶å»é‡
        wordlist_150_lower = [word.lower() for word in wordlist_150]
        wordlist_150_unique = list(dict.fromkeys(wordlist_150_lower))  # ä¿æŒé¡ºåºå»é‡
        
        print(f"150è¯è¡¨ä¸­çš„è¯æ±‡æ•°: {len(wordlist_150)} (åŸå§‹)")
        print(f"150è¯è¡¨ä¸­çš„å”¯ä¸€è¯æ±‡æ•°: {len(wordlist_150_unique)} (å»é‡å)")
        
        # è¯»å–æ–°çš„è¯é¢‘ç»Ÿè®¡ç»“æœ
        frequency_words = set()
        with open('filtered_sentences_150_words_frequency.txt', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
            # æ‰¾åˆ°è¯é¢‘åˆ—è¡¨çš„å¼€å§‹ä½ç½®
            start_reading = False
            for line in lines:
                if "å®Œæ•´è¯é¢‘åˆ—è¡¨" in line:
                    start_reading = True
                    continue
                if start_reading and line.strip() and line.strip()[0].isdigit():
                    # è§£æè¯é¢‘è¡Œï¼Œæ ¼å¼: "    1. you                     1699    8.05%"
                    parts = line.strip().split()
                    if len(parts) >= 2 and parts[0].endswith('.'):
                        word = parts[1]  # ç¬¬äºŒéƒ¨åˆ†æ˜¯è¯æ±‡
                        frequency_words.add(word.lower())
        
        print(f"æ–°è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡æ•°: {len(frequency_words)}")
        
        # æ‰¾å‡ºåœ¨150è¯è¡¨ä¸­ä½†ä¸åœ¨è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡
        missing_words = []
        for word in wordlist_150_unique:
            if word not in frequency_words:
                missing_words.append(word)
        
        # æ‰¾å‡ºåœ¨è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡
        extra_words = []
        for word in frequency_words:
            if word not in wordlist_150_lower:
                extra_words.append(word)
        
        print(f"\nğŸ“Š åˆ†æç»“æœ:")
        print(f"   åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­: {len(missing_words)} ä¸ª")
        print(f"   åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­: {len(extra_words)} ä¸ª")
        
        if missing_words:
            print(f"\nâŒ åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨å¥å­ä¸­çš„è¯æ±‡:")
            print("-" * 60)
            for i, word in enumerate(sorted(missing_words), 1):
                # æ‰¾åˆ°åŸå§‹å½¢å¼
                original_forms = [w for w in wordlist_150 if w.lower() == word]
                original_form = original_forms[0] if original_forms else word
                print(f"   {i:>2}. {word} (åŸå§‹: '{original_form}')")
        else:
            print(f"\nâœ… æ‰€æœ‰150è¯è¡¨ä¸­çš„è¯æ±‡éƒ½åœ¨å¥å­ä¸­å‡ºç°äº†ï¼")
        
        if extra_words:
            print(f"\nâš ï¸  åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡:")
            print("-" * 60)
            for i, word in enumerate(sorted(extra_words), 1):
                print(f"   {i:>2}. {word}")
        
        # æ£€æŸ¥150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡
        duplicates = []
        seen = {}
        for i, word in enumerate(wordlist_150):
            word_lower = word.lower()
            if word_lower in seen:
                duplicates.append((i+1, word, word_lower, seen[word_lower]))
            else:
                seen[word_lower] = (i+1, word)
        
        if duplicates:
            print(f"\nğŸ”„ 150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡:")
            print("-" * 60)
            for line_num, word, word_lower, (first_line, first_word) in duplicates:
                print(f"   ç¬¬{line_num}è¡Œ: '{word}' (é‡å¤ï¼Œé¦–æ¬¡å‡ºç°åœ¨ç¬¬{first_line}è¡Œ: '{first_word}')")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open('missing_words_analysis_new.txt', 'w', encoding='utf-8') as f:
            f.write("150è¯è¡¨ä¸æ–°è¯é¢‘ç»Ÿè®¡å¯¹æ¯”åˆ†æ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"150è¯è¡¨ä¸­çš„è¯æ±‡æ•°: {len(wordlist_150)} (åŸå§‹)\n")
            f.write(f"150è¯è¡¨ä¸­çš„å”¯ä¸€è¯æ±‡æ•°: {len(wordlist_150_unique)} (å»é‡å)\n")
            f.write(f"æ–°è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡æ•°: {len(frequency_words)}\n\n")
            
            f.write(f"åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡ ({len(missing_words)} ä¸ª):\n")
            f.write("-" * 60 + "\n")
            for i, word in enumerate(sorted(missing_words), 1):
                original_forms = [w for w in wordlist_150 if w.lower() == word]
                original_form = original_forms[0] if original_forms else word
                f.write(f"{i:>2}. {word} (åŸå§‹: '{original_form}')\n")
            
            f.write(f"\nåœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡ ({len(extra_words)} ä¸ª):\n")
            f.write("-" * 60 + "\n")
            for i, word in enumerate(sorted(extra_words), 1):
                f.write(f"{i:>2}. {word}\n")
            
            if duplicates:
                f.write(f"\n150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡ ({len(duplicates)} ä¸ª):\n")
                f.write("-" * 60 + "\n")
                for line_num, word, word_lower, (first_line, first_word) in duplicates:
                    f.write(f"ç¬¬{line_num}è¡Œ: '{word}' (é‡å¤ï¼Œé¦–æ¬¡å‡ºç°åœ¨ç¬¬{first_line}è¡Œ: '{first_word}')\n")
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: missing_words_analysis_new.txt")
        
        return missing_words, extra_words, duplicates
        
    except FileNotFoundError as e:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {e}")
        return None, None, None
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None, None

def main():
    """ä¸»å‡½æ•°"""
    missing_words, extra_words, duplicates = find_missing_words()
    
    if missing_words is not None:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        if missing_words:
            print(f"ğŸš¨ å‘ç° {len(missing_words)} ä¸ªè¯æ±‡åœ¨150è¯è¡¨ä¸­ä½†æœªåœ¨æ–°ç­›é€‰çš„å¥å­ä¸­å‡ºç°")
        else:
            print(f"ğŸ‰ æ‰€æœ‰150è¯è¡¨ä¸­çš„è¯æ±‡éƒ½åœ¨æ–°ç­›é€‰çš„å¥å­ä¸­å‡ºç°äº†ï¼")
        
        if extra_words:
            print(f"âš ï¸  å‘ç° {len(extra_words)} ä¸ªé¢å¤–è¯æ±‡åœ¨æ–°è¯é¢‘ç»Ÿè®¡ä¸­")
        
        if duplicates:
            print(f"ğŸ”„ å‘ç° {len(duplicates)} ä¸ªé‡å¤è¯æ±‡åœ¨150è¯è¡¨ä¸­")

if __name__ == "__main__":
    main()
