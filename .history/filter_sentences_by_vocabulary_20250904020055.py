#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer, sent_tokenize
import nltk
from tqdm import tqdm

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨: {vocab_file}")
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        print(f"æˆåŠŸåŠ è½½ {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    except Exception as e:
        print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
        return set()

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;\(\)\"]+', ' ', text)  # ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    
    return text.strip()

def is_valid_token(token, vocab):
    """æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆï¼ˆåœ¨è¯æ±‡è¡¨ä¸­æˆ–æ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰"""
    token_lower = token.lower()
    
    # æ ‡ç‚¹ç¬¦å·æ€»æ˜¯æœ‰æ•ˆçš„
    if token in ".,!?;:()\"'-":
        return True
    
    # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
    if token_lower in vocab:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆæ•°å­—ä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼‰
    if token.isdigit():
        return True
    
    return False

def check_sentence_vocabulary(sentence, vocab, tokenizer):
    """æ£€æŸ¥å¥å­ä¸­çš„æ‰€æœ‰è¯æ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­"""
    if not sentence or len(sentence.strip()) < 3:
        return False, [], []
    
    # åˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    # æ£€æŸ¥æ¯ä¸ªtoken
    valid_tokens = []
    invalid_tokens = []
    
    for token in tokens:
        if is_valid_token(token, vocab):
            valid_tokens.append(token)
        else:
            invalid_tokens.append(token)
    
    # å¦‚æœæ‰€æœ‰tokenéƒ½æœ‰æ•ˆï¼Œè¿”å›True
    is_valid = len(invalid_tokens) == 0
    
    return is_valid, valid_tokens, invalid_tokens

def process_movie_lines(movie_file, vocab_file, output_file, max_lines=None):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    
    print("=" * 80)
    print("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤å™¨")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab = load_vocabulary(vocab_file)
    if not vocab:
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # è¯»å–ç”µå½±å°è¯æ–‡ä»¶
    print(f"\næ­£åœ¨è¯»å–ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
    
    try:
        df = pd.read_csv(movie_file, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æˆåŠŸè¯»å– {len(df)} è¡Œå¯¹è¯")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåˆ™æˆªå–
        if max_lines and max_lines < len(df):
            df = df.head(max_lines)
            print(f"å¤„ç†å‰ {max_lines} è¡Œå¯¹è¯")
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸€è¡Œ
    valid_sentences = []
    invalid_sentences = []
    
    print(f"\nå¼€å§‹å¤„ç†å¯¹è¯...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
        text = clean_text(row['text'])
        
        if not text:
            continue
        
        # å°†æ¯è¡Œæ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = sent_tokenize(text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            # è·³è¿‡å¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­
            if len(sentence) < 5 or len(sentence) > 200:
                continue
            
            # æ£€æŸ¥å¥å­è¯æ±‡
            is_valid, valid_tokens, invalid_tokens = check_sentence_vocabulary(sentence, vocab, tokenizer)
            
            # åªä¿ç•™è¯æ±‡æœ‰æ•ˆä¸”é•¿åº¦åœ¨8-12è¯ä¹‹é—´çš„å¥å­
            if is_valid and 8 <= len(valid_tokens) <= 12:
                valid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'tokens': valid_tokens,
                    'token_count': len(valid_tokens)
                })
            else:
                invalid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'invalid_tokens': invalid_tokens,
                    'all_tokens': valid_tokens + invalid_tokens
                })
    
    # ä¿å­˜ç»“æœ
    print(f"\nå¤„ç†å®Œæˆï¼")
    print(f"æœ‰æ•ˆå¥å­: {len(valid_sentences)}")
    print(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}")
    print(f"æœ‰æ•ˆç‡: {len(valid_sentences)/(len(valid_sentences)+len(invalid_sentences))*100:.2f}%")
    
    # ä¿å­˜æœ‰æ•ˆå¥å­
    print(f"\næ­£åœ¨ä¿å­˜æœ‰æ•ˆå¥å­åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ç”µå½±å°è¯ - ä»…åŒ…å«Top1200è¯æ±‡ä¸”é•¿åº¦ä¸º8-12è¯çš„å¥å­\n")
        f.write(f"# æ€»å…± {len(valid_sentences)} ä¸ªæœ‰æ•ˆå¥å­\n")
        f.write(f"# è¯æ±‡è¡¨: {vocab_file}\n")
        f.write(f"# é•¿åº¦é™åˆ¶: 8-12è¯\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        for i, item in enumerate(valid_sentences, 1):
            f.write(f"{i}. {item['sentence']}\n")
            # f.write(f"   [è¯æ•°: {item['token_count']}, è§’è‰²: {item['character_name']}]\n\n")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = output_file.replace('.txt', '_stats.txt')
    print(f"æ­£åœ¨ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°: {stats_file}")
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æºæ–‡ä»¶: {movie_file}\n")
        f.write(f"è¯æ±‡è¡¨: {vocab_file}\n")
        f.write(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)} ä¸ªè¯\n")
        f.write(f"é•¿åº¦è¿‡æ»¤: ä»…ä¿ç•™8-12è¯çš„å¥å­\n\n")
        f.write(f"å¤„ç†è¡Œæ•°: {len(df)}\n")
        f.write(f"æœ‰æ•ˆå¥å­: {len(valid_sentences)}\n")
        f.write(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}\n")
        f.write(f"æœ‰æ•ˆç‡: {len(valid_sentences)/(len(valid_sentences)+len(invalid_sentences))*100:.2f}%\n\n")
        
        # å¥å­é•¿åº¦ç»Ÿè®¡
        if valid_sentences:
            lengths = [item['token_count'] for item in valid_sentences]
            f.write("æœ‰æ•ˆå¥å­é•¿åº¦ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.2f} è¯\n")
            f.write(f"  æœ€çŸ­å¥å­: {min(lengths)} è¯\n")
            f.write(f"  æœ€é•¿å¥å­: {max(lengths)} è¯\n\n")
            
            # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
            f.write("æœ‰æ•ˆå¥å­ç¤ºä¾‹:\n")
            for i, item in enumerate(valid_sentences[:10], 1):
                f.write(f"{i:2d}. {item['sentence'][:80]}{'...' if len(item['sentence']) > 80 else ''}\n")
        
        f.write("\næ— æ•ˆå¥å­ç¤ºä¾‹ (åŒ…å«è¯æ±‡è¡¨å¤–çš„è¯):\n")
        for i, item in enumerate(invalid_sentences[:10], 1):
            f.write(f"{i:2d}. {item['sentence'][:60]}{'...' if len(item['sentence']) > 60 else ''}\n")
            f.write(f"    æ— æ•ˆè¯: {item['invalid_tokens'][:5]}\n")
    
    # ä¿å­˜æ— æ•ˆå¥å­æ ·æœ¬ï¼ˆç”¨äºåˆ†æï¼‰
    invalid_sample_file = output_file.replace('.txt', '_invalid_samples.txt')
    print(f"æ­£åœ¨ä¿å­˜æ— æ•ˆå¥å­æ ·æœ¬åˆ°: {invalid_sample_file}")
    
    with open(invalid_sample_file, 'w', encoding='utf-8') as f:
        f.write(f"# æ— æ•ˆå¥å­æ ·æœ¬ (å‰100ä¸ª)\n")
        f.write(f"# è¿™äº›å¥å­åŒ…å«ä¸åœ¨Top1200è¯æ±‡è¡¨ä¸­çš„è¯\n\n")
        
        for i, item in enumerate(invalid_sentences[:100], 1):
            f.write(f"{i}. {item['sentence']}\n")
            f.write(f"   æ— æ•ˆè¯: {item['invalid_tokens']}\n\n")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ {output_file} - æœ‰æ•ˆå¥å­")
    print(f"   ğŸ“Š {stats_file} - ç»Ÿè®¡æŠ¥å‘Š") 
    print(f"   ğŸ” {invalid_sample_file} - æ— æ•ˆå¥å­æ ·æœ¬")
    
    return len(valid_sentences), len(invalid_sentences)

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    movie_file = "movie_lines.tsv"
    vocab_file = "top_1200_words_simple.txt"
    output_file = "filtered_movie_sentences_8to12words.txt"  # æ›´æ–°æ–‡ä»¶åä»¥åæ˜ é•¿åº¦è¿‡æ»¤
    max_lines = 10000  # é™åˆ¶å¤„ç†è¡Œæ•°ï¼Œè®¾ä¸ºNoneå¤„ç†å…¨éƒ¨
    
    # è¿è¡Œå¤„ç†
    process_movie_lines(movie_file, vocab_file, output_file, max_lines)
