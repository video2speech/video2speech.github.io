#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import nltk
from nltk.tokenize import TreebankWordTokenizer
import pandas as pd
import random
import re

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTKæ•°æ®...")
        nltk.download('punkt')

def load_movie_lines(filename, num_samples=20):
    """ä»movie_lines.tsvæ–‡ä»¶ä¸­åŠ è½½å¥å­"""
    print(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {filename}")
    
    try:
        # è¯»å–TSVæ–‡ä»¶
        df = pd.read_csv(filename, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æ€»å…±åŠ è½½äº† {len(df)} è¡Œå¯¹è¯")
        
        # æ¸…ç†æ–‡æœ¬æ•°æ®
        df['text'] = df['text'].astype(str)
        df = df[df['text'].str.len() > 10]  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
        df = df[df['text'].str.len() < 200]  # è¿‡æ»¤å¤ªé•¿çš„å¥å­
        
        print(f"è¿‡æ»¤åå‰©ä½™ {len(df)} è¡Œæœ‰æ•ˆå¯¹è¯")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        if len(df) > num_samples:
            sample_df = df.sample(n=num_samples, random_state=42)
        else:
            sample_df = df
        
        return sample_df['text'].tolist()
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def demonstrate_tokenization():
    """æ¼”ç¤ºTreebankWordTokenizerçš„ä½¿ç”¨"""
    
    print("=" * 80)
    print("TreebankWordTokenizer åˆ†è¯æ¼”ç¤º")
    print("=" * 80)
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # æ¼”ç¤ºåŸºæœ¬ç”¨æ³•
    print("\nğŸ¯ åŸºæœ¬æ¼”ç¤º:")
    print("-" * 40)
    text = "I'm sure you're not going, don't you?"
    tokens = tokenizer.tokenize(text)
    print(f"åŸæ–‡: {text}")
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    print(f"è¯æ•°: {len(tokens)}")
    
    # ä»movie_lines.tsvåŠ è½½å¥å­è¿›è¡Œæ¼”ç¤º
    print(f"\nğŸ¬ ç”µå½±å°è¯åˆ†è¯æ¼”ç¤º:")
    print("-" * 40)
    
    movie_sentences = load_movie_lines("movie_lines.tsv", num_samples=10)
    
    if not movie_sentences:
        print("æ— æ³•åŠ è½½ç”µå½±å°è¯ï¼Œä½¿ç”¨ç¤ºä¾‹å¥å­...")
        movie_sentences = [
            "What are you talking about?",
            "I don't know what you mean.",
            "That's absolutely incredible!",
            "You've got to be kidding me.",
            "I can't believe this is happening."
        ]
    
    for i, sentence in enumerate(movie_sentences[:10], 1):
        # æ¸…ç†å¥å­ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        clean_sentence = re.sub(r'[^\w\s\'\-\.\,\?\!\:]', '', sentence)
        clean_sentence = clean_sentence.strip()
        
        if len(clean_sentence) > 5:  # ç¡®ä¿å¥å­ä¸ä¸ºç©º
            tokens = tokenizer.tokenize(clean_sentence)
            
            print(f"\n{i:2d}. åŸæ–‡: {clean_sentence}")
            print(f"    åˆ†è¯: {tokens}")
            print(f"    è¯æ•°: {len(tokens)}")

def analyze_tokenization_patterns():
    """åˆ†æåˆ†è¯æ¨¡å¼"""
    
    print(f"\nğŸ” åˆ†è¯æ¨¡å¼åˆ†æ:")
    print("-" * 40)
    
    tokenizer = TreebankWordTokenizer()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æœ¬
    test_cases = [
        "I'm going to the store.",
        "Don't you think it's wonderful?",
        "She said, 'Hello, world!'",
        "The cost is $19.99.",
        "Visit www.example.com for more info.",
        "Call me at 555-123-4567.",
        "It's a state-of-the-art system.",
        "U.S.A. is a country in North America."
    ]
    
    for i, text in enumerate(test_cases, 1):
        tokens = tokenizer.tokenize(text)
        print(f"{i}. {text}")
        print(f"   â†’ {tokens}")
        print()

def compare_with_simple_split():
    """ä¸ç®€å•çš„split()æ–¹æ³•æ¯”è¾ƒ"""
    
    print(f"\nâš–ï¸  TreebankWordTokenizer vs ç®€å•split()æ¯”è¾ƒ:")
    print("-" * 50)
    
    tokenizer = TreebankWordTokenizer()
    
    test_sentences = [
        "I'm not sure if you're right.",
        "The price is $29.99, isn't it?",
        "She said, 'I don't know.'"
    ]
    
    for sentence in test_sentences:
        nltk_tokens = tokenizer.tokenize(sentence)
        simple_tokens = sentence.split()
        
        print(f"åŸæ–‡: {sentence}")
        print(f"TreebankWordTokenizer: {nltk_tokens} ({len(nltk_tokens)} è¯)")
        print(f"ç®€å•split():           {simple_tokens} ({len(simple_tokens)} è¯)")
        print("-" * 50)

def tokenize_and_analyze_movie_corpus():
    """åˆ†è¯å¹¶åˆ†æç”µå½±è¯­æ–™åº“"""
    
    print(f"\nğŸ“Š ç”µå½±è¯­æ–™åº“åˆ†è¯ç»Ÿè®¡:")
    print("-" * 40)
    
    tokenizer = TreebankWordTokenizer()
    movie_sentences = load_movie_lines("movie_lines.tsv", num_samples=1000)
    
    if not movie_sentences:
        print("æ— æ³•åŠ è½½ç”µå½±è¯­æ–™åº“")
        return
    
    all_tokens = []
    sentence_lengths = []
    
    for sentence in movie_sentences[:100]:  # åˆ†æå‰100å¥
        clean_sentence = re.sub(r'[^\w\s\'\-\.\,\?\!\:]', '', sentence)
        clean_sentence = clean_sentence.strip()
        
        if len(clean_sentence) > 5:
            tokens = tokenizer.tokenize(clean_sentence)
            all_tokens.extend(tokens)
            sentence_lengths.append(len(tokens))
    
    if all_tokens:
        print(f"åˆ†æå¥å­æ•°: {len(sentence_lengths)}")
        print(f"æ€»è¯æ•°: {len(all_tokens)}")
        print(f"å¹³å‡å¥é•¿: {sum(sentence_lengths)/len(sentence_lengths):.2f} è¯")
        print(f"æœ€çŸ­å¥å­: {min(sentence_lengths)} è¯")
        print(f"æœ€é•¿å¥å­: {max(sentence_lengths)} è¯")
        
        # è¯é¢‘ç»Ÿè®¡
        from collections import Counter
        word_freq = Counter(token.lower() for token in all_tokens)
        print(f"\nå‰10ä¸ªæœ€å¸¸è§çš„è¯:")
        for word, freq in word_freq.most_common(10):
            print(f"  {word}: {freq}")

if __name__ == "__main__":
    # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
    download_nltk_data()
    
    # è¿è¡Œæ¼”ç¤º
    demonstrate_tokenization()
    analyze_tokenization_patterns()
    compare_with_simple_split()
    tokenize_and_analyze_movie_corpus()
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("TreebankWordTokenizer çš„ä¸»è¦ç‰¹ç‚¹:")
    print("- æ­£ç¡®å¤„ç†ç¼©å†™ (I'm â†’ I, 'm)")
    print("- åˆ†ç¦»æ ‡ç‚¹ç¬¦å·")
    print("- å¤„ç†ç‰¹æ®Šæ ¼å¼ (ä»·æ ¼ã€ç½‘å€ç­‰)")
    print("- æ¯”ç®€å•çš„split()æ›´æ™ºèƒ½")
