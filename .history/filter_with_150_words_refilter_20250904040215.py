#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
from nltk.tokenize import TreebankWordTokenizer
import nltk
from collections import Counter

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set()
            for line in f:
                word = line.strip()
                if word:
                    vocab.add(word.lower())
        return vocab
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°è¯æ±‡è¡¨æ–‡ä»¶: {vocab_file}")
        return set()

def check_sentence_vocabulary(sentence, vocab, tokenizer):
    """æ£€æŸ¥å¥å­ä¸­çš„æ‰€æœ‰è¯æ±‡æ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­"""
    # åˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    word_tokens = []
    punctuation_tokens = []
    invalid_tokens = []
    
    for token in tokens:
        if token in ".,!?;:()\"'-":
            punctuation_tokens.append(token)
        elif token.isdigit():
            # æ•°å­—ä¹Ÿç®—ä½œæœ‰æ•ˆè¯æ±‡
            word_tokens.append(token)
        elif token.lower() in vocab:
            word_tokens.append(token)
        else:
            invalid_tokens.append(token)
    
    # æ£€æŸ¥æ¡ä»¶ï¼š1. æ‰€æœ‰è¯æ±‡éƒ½æœ‰æ•ˆï¼ˆæ— invalid_tokensï¼‰2. è¯æ±‡æ•°â‰¥4ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    is_valid = len(invalid_tokens) == 0 and len(word_tokens) >= 4
    all_tokens = word_tokens + punctuation_tokens
    
    return is_valid, all_tokens, invalid_tokens

def process_movie_lines(movie_file, vocab, tokenizer):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    try:
        # è¯»å–TSVæ–‡ä»¶
        df = pd.read_csv(movie_file, sep='\t', encoding='utf-8', 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'])
        
        print(f"æ€»è¡Œæ•°: {len(df)}")
        
        valid_sentences = []
        invalid_sentences = []
        
        for index, row in df.iterrows():
            if pd.isna(row['text']):
                continue
            
            sentence = str(row['text']).strip()
            if not sentence:
                continue
            
            # æ£€æŸ¥å¥å­è¯æ±‡
            is_valid, valid_tokens, invalid_tokens = check_sentence_vocabulary(sentence, vocab, tokenizer)
            
            if is_valid:
                # è®¡ç®—å®é™…è¯æ±‡æ•°ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
                word_count = len([token for token in valid_tokens if token not in ".,!?;:()\"'-"])
                valid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'tokens': valid_tokens,
                    'token_count': len(valid_tokens),
                    'word_count': word_count  # å®é™…è¯æ±‡æ•°
                })
            else:
                invalid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'invalid_tokens': invalid_tokens,
                    'word_count': len([token for token in tokenizer.tokenize(sentence) if token not in ".,!?;:()\"'-"])
                })
        
        return valid_sentences, invalid_sentences
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
        return [], []
    except Exception as e:
        print(f"âŒ å¤„ç†ç”µå½±å°è¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return [], []

def save_results(valid_sentences, invalid_sentences, vocab_size):
    """ä¿å­˜ç­›é€‰ç»“æœ"""
    
    # ä¿å­˜æœ‰æ•ˆå¥å­ï¼ˆå¸¦ç¼–å·ï¼‰
    output_file = 'filtered_sentences_150_words_refilter.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"ä½¿ç”¨150è¯è¡¨é‡æ–°ç­›é€‰çš„å¥å­ (è¯æ±‡è¡¨å¤§å°: {vocab_size})\n")
        f.write(f"ç­›é€‰æ¡ä»¶: å¥å­é•¿åº¦â‰¥4ä¸ªè¯ï¼Œæ‰€æœ‰è¯æ±‡éƒ½åœ¨150è¯è¡¨ä¸­\n")
        f.write("=" * 70 + "\n\n")
        
        for i, item in enumerate(valid_sentences, 1):
            f.write(f"{i:>4}. {item['sentence']}\n")
            f.write(f"      è¯æ±‡æ•°: {item['word_count']} | æ€»tokenæ•°: {item['token_count']}\n")
            f.write(f"      è§’è‰²: {item['character_name']} | ID: {item['line_id']}\n\n")
    
    # ä¿å­˜å¹²å‡€çš„å¥å­åˆ—è¡¨ï¼ˆæ— ç¼–å·ï¼‰
    clean_output_file = 'filtered_sentences_150_words_refilter_clean.txt'
    with open(clean_output_file, 'w', encoding='utf-8') as f:
        for item in valid_sentences:
            f.write(f"{item['sentence']}\n")
    
    # å»é‡å¤„ç†
    unique_sentences = []
    seen_sentences = set()
    for item in valid_sentences:
        sentence_lower = item['sentence'].lower().strip()
        if sentence_lower not in seen_sentences:
            unique_sentences.append(item)
            seen_sentences.add(sentence_lower)
    
    # ä¿å­˜å»é‡åçš„å¥å­
    unique_output_file = 'filtered_sentences_150_words_refilter_unique.txt'
    with open(unique_output_file, 'w', encoding='utf-8') as f:
        for item in unique_sentences:
            f.write(f"{item['sentence']}\n")
    
    # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
    stats_file = 'filtered_sentences_150_words_refilter_stats.txt'
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("150è¯è¡¨é‡æ–°ç­›é€‰ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"è¯æ±‡è¡¨å¤§å°: {vocab_size} ä¸ªè¯æ±‡\n")
        f.write(f"ç­›é€‰æ¡ä»¶: å¥å­é•¿åº¦â‰¥4ä¸ªè¯ï¼Œæ‰€æœ‰è¯æ±‡éƒ½åœ¨150è¯è¡¨ä¸­\n\n")
        
        f.write(f"ç­›é€‰ç»“æœ:\n")
        f.write(f"  æœ‰æ•ˆå¥å­æ•°: {len(valid_sentences)} ä¸ª\n")
        f.write(f"  å»é‡åå¥å­æ•°: {len(unique_sentences)} ä¸ª\n")
        f.write(f"  æ— æ•ˆå¥å­æ•°: {len(invalid_sentences)} ä¸ª\n")
        f.write(f"  é‡å¤å¥å­æ•°: {len(valid_sentences) - len(unique_sentences)} ä¸ª\n\n")
        
        # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
        length_counter = Counter(item['word_count'] for item in valid_sentences)
        f.write("å¥å­é•¿åº¦åˆ†å¸ƒ:\n")
        for length in sorted(length_counter.keys()):
            count = length_counter[length]
            percentage = count / len(valid_sentences) * 100
            f.write(f"  {length:>2}è¯: {count:>4} ä¸ª ({percentage:>5.1f}%)\n")
        
        f.write(f"\nå¹³å‡å¥å­é•¿åº¦: {sum(item['word_count'] for item in valid_sentences) / len(valid_sentences):.1f} ä¸ªè¯\n")
    
    # ä¿å­˜æ— æ•ˆå¥å­æ ·æœ¬
    if invalid_sentences:
        invalid_samples_file = 'filtered_sentences_150_words_refilter_invalid_samples.txt'
        with open(invalid_samples_file, 'w', encoding='utf-8') as f:
            f.write("æ— æ•ˆå¥å­æ ·æœ¬ (å‰100ä¸ª)\n")
            f.write("=" * 50 + "\n\n")
            
            for i, item in enumerate(invalid_sentences[:100], 1):
                f.write(f"{i:>3}. {item['sentence']}\n")
                f.write(f"     è¯æ±‡æ•°: {item['word_count']} | æ— æ•ˆè¯æ±‡: {item['invalid_tokens']}\n")
                f.write(f"     è§’è‰²: {item['character_name']}\n\n")
    
    return len(unique_sentences)

def calculate_word_frequency(valid_sentences, vocab_size):
    """è®¡ç®—ç­›é€‰åå¥å­çš„è¯é¢‘"""
    print("\nğŸ“Š è®¡ç®—ç­›é€‰åå¥å­çš„è¯é¢‘...")
    
    tokenizer = TreebankWordTokenizer()
    word_counter = Counter()
    all_words = []
    
    for item in valid_sentences:
        sentence = item['sentence']
        tokens = tokenizer.tokenize(sentence)
        
        for token in tokens:
            # è·³è¿‡æ ‡ç‚¹ç¬¦å·
            if token in ".,!?;:()\"'-":
                continue
            
            word_lower = token.lower()
            all_words.append(word_lower)
            word_counter[word_lower] += 1
    
    # ä¿å­˜è¯é¢‘ç»Ÿè®¡
    frequency_file = 'filtered_sentences_150_words_refilter_frequency.txt'
    with open(frequency_file, 'w', encoding='utf-8') as f:
        f.write(f"150è¯è¡¨é‡æ–°ç­›é€‰å¥å­è¯é¢‘ç»Ÿè®¡\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"æ€»å¥å­æ•°: {len(valid_sentences)}\n")
        f.write(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰\n")
        f.write(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª\n")
        f.write(f"è¯æ±‡è¡¨å¤§å°: {vocab_size} ä¸ª\n\n")
        
        sorted_words = word_counter.most_common()
        f.write("å®Œæ•´è¯é¢‘åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡é™åºæ’åˆ—ï¼‰:\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'æ’å':<6} {'è¯æ±‡':<20} {'é¢‘ç‡':<8} {'å æ¯”':<8} {'ç´¯ç§¯å æ¯”'}\n")
        f.write("-" * 60 + "\n")
        
        cumulative_freq = 0
        for i, (word, freq) in enumerate(sorted_words, 1):
            percentage = freq / len(all_words) * 100
            cumulative_freq += freq
            cumulative_percentage = cumulative_freq / len(all_words) * 100
            f.write(f"{i:>5}. {word:<20} {freq:>7} {percentage:>7.2f}% {cumulative_percentage:>8.2f}%\n")
    
    # ä¿å­˜CSVæ ¼å¼
    csv_file = 'filtered_sentences_150_words_refilter_frequency.csv'
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write("æ’å,è¯æ±‡,é¢‘ç‡,å æ¯”,ç´¯ç§¯å æ¯”\n")
        cumulative_freq = 0
        sorted_words = word_counter.most_common()
        for i, (word, freq) in enumerate(sorted_words, 1):
            percentage = freq / len(all_words) * 100
            cumulative_freq += freq
            cumulative_percentage = cumulative_freq / len(all_words) * 100
            f.write(f"{i},{word},{freq},{percentage:.2f}%,{cumulative_percentage:.2f}%\n")
    
    return word_counter

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä½¿ç”¨150è¯è¡¨é‡æ–°ç­›é€‰å¥å­")
    print("=" * 60)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½150è¯æ±‡è¡¨
    vocab_file = 'materials/150_words_list.txt'
    vocab = load_vocabulary(vocab_file)
    
    if not vocab:
        print("âŒ è¯æ±‡è¡¨åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"ğŸ“š åŠ è½½è¯æ±‡è¡¨: {len(vocab)} ä¸ªå”¯ä¸€è¯æ±‡")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # å¤„ç†ç”µå½±å°è¯
    movie_file = 'others/movie_lines.tsv'
    print(f"ğŸ“– å¤„ç†ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
    
    valid_sentences, invalid_sentences = process_movie_lines(movie_file, vocab, tokenizer)
    
    if not valid_sentences:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå¥å­")
        return
    
    print(f"\nğŸ“Š ç­›é€‰ç»“æœ:")
    print(f"   æœ‰æ•ˆå¥å­: {len(valid_sentences)} ä¸ª")
    print(f"   æ— æ•ˆå¥å­: {len(invalid_sentences)} ä¸ª")
    print(f"   æœ‰æ•ˆç‡: {len(valid_sentences) / (len(valid_sentences) + len(invalid_sentences)) * 100:.1f}%")
    
    # ä¿å­˜ç»“æœ
    unique_count = save_results(valid_sentences, invalid_sentences, len(vocab))
    print(f"   å»é‡åå¥å­: {unique_count} ä¸ª")
    
    # è®¡ç®—è¯é¢‘
    word_counter = calculate_word_frequency(valid_sentences, len(vocab))
    
    print(f"\nğŸ’¾ æ–‡ä»¶å·²ç”Ÿæˆ:")
    print(f"   ğŸ“„ filtered_sentences_150_words_refilter.txt - è¯¦ç»†å¥å­åˆ—è¡¨")
    print(f"   ğŸ“ filtered_sentences_150_words_refilter_clean.txt - å¹²å‡€å¥å­åˆ—è¡¨")
    print(f"   ğŸ¯ filtered_sentences_150_words_refilter_unique.txt - å»é‡å¥å­åˆ—è¡¨")
    print(f"   ğŸ“Š filtered_sentences_150_words_refilter_stats.txt - ç»Ÿè®¡æŠ¥å‘Š")
    print(f"   ğŸ“ˆ filtered_sentences_150_words_refilter_frequency.txt - è¯é¢‘ç»Ÿè®¡")
    print(f"   ğŸ“Š filtered_sentences_150_words_refilter_frequency.csv - è¯é¢‘CSV")
    
    if invalid_sentences:
        print(f"   âš ï¸  filtered_sentences_150_words_refilter_invalid_samples.txt - æ— æ•ˆå¥å­æ ·æœ¬")
    
    print(f"\nâœ… 150è¯è¡¨é‡æ–°ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ¯ å…±ç­›é€‰å‡º {len(valid_sentences)} ä¸ªæœ‰æ•ˆå¥å­ï¼ˆå»é‡å {unique_count} ä¸ªï¼‰")
    print(f"ğŸ“Š è¯é¢‘ç»Ÿè®¡æ˜¾ç¤º {len(word_counter)} ä¸ªä¸åŒè¯æ±‡")

if __name__ == "__main__":
    main()
