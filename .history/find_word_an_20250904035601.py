#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import nltk

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')

def find_word_in_sentences(filename, target_word):
    """åœ¨å¥å­ä¸­æŸ¥æ‰¾ç‰¹å®šè¯æ±‡"""
    
    print(f"ğŸ” åœ¨ {filename} ä¸­æŸ¥æ‰¾è¯æ±‡: '{target_word}'")
    print("=" * 60)
    
    download_nltk_data()
    
    try:
        # è¯»å–å¥å­æ–‡ä»¶
        with open(filename, 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"æ€»å¥å­æ•°: {len(sentences)}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = TreebankWordTokenizer()
        
        found_sentences = []
        target_word_lower = target_word.lower()
        
        for i, sentence in enumerate(sentences, 1):
            # åˆ†è¯
            tokens = tokenizer.tokenize(sentence)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡è¯æ±‡ï¼ˆè½¬æ¢ä¸ºå°å†™æ¯”è¾ƒï¼‰
            for token in tokens:
                if token.lower() == target_word_lower:
                    found_sentences.append((i, sentence, tokens))
                    break
        
        print(f"\nğŸ“Š æŸ¥æ‰¾ç»“æœ:")
        print(f"   æ‰¾åˆ° {len(found_sentences)} ä¸ªåŒ…å« '{target_word}' çš„å¥å­")
        
        if found_sentences:
            print(f"\nğŸ“ åŒ…å« '{target_word}' çš„å¥å­:")
            print("-" * 60)
            
            for line_num, sentence, tokens in found_sentences:
                print(f"ç¬¬ {line_num} è¡Œ: {sentence}")
                
                # æ˜¾ç¤ºåˆ†è¯ç»“æœï¼Œé«˜äº®ç›®æ ‡è¯æ±‡
                token_display = []
                for token in tokens:
                    if token.lower() == target_word_lower:
                        token_display.append(f"[{token}]")  # ç”¨æ–¹æ‹¬å·é«˜äº®
                    else:
                        token_display.append(token)
                
                print(f"   åˆ†è¯: {' '.join(token_display)}")
                print()
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{target_word}' çš„å¥å­")
            
            # å°è¯•æŸ¥æ‰¾ç›¸ä¼¼çš„è¯æ±‡
            print(f"\nğŸ” æŸ¥æ‰¾åŒ…å« '{target_word}' çš„ç›¸ä¼¼è¯æ±‡...")
            similar_words = set()
            
            for sentence in sentences:
                tokens = tokenizer.tokenize(sentence)
                for token in tokens:
                    if target_word_lower in token.lower():
                        similar_words.add(token.lower())
            
            if similar_words:
                print(f"   æ‰¾åˆ°ç›¸ä¼¼è¯æ±‡: {sorted(similar_words)}")
            else:
                print(f"   æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{target_word}' çš„ç›¸ä¼¼è¯æ±‡")
        
        return found_sentences
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        return []
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def main():
    """ä¸»å‡½æ•°"""
    filename = 'filtered_sentences_150_words_clean.txt'
    target_word = 'an'
    
    found_sentences = find_word_in_sentences(filename, target_word)
    
    if found_sentences:
        print(f"âœ… æˆåŠŸæ‰¾åˆ° {len(found_sentences)} ä¸ªå¥å­åŒ…å« '{target_word}'")
    else:
        print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{target_word}' çš„å¥å­")

if __name__ == "__main__":
    main()
