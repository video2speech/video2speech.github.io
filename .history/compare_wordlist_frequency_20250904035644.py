#!/usr/bin/env python3
# -*- coding: utf-8 -*-

def compare_wordlist_and_frequency():
    """æ¯”è¾ƒ150è¯è¡¨å’Œè¯é¢‘ç»Ÿè®¡ç»“æœï¼Œæ‰¾å‡ºç¼ºå¤±çš„è¯æ±‡"""
    
    print("ğŸ” æ¯”è¾ƒ 150_words_list.txt å’Œè¯é¢‘ç»Ÿè®¡ç»“æœ")
    print("=" * 60)
    
    try:
        # è¯»å–150è¯è¡¨
        with open('materials/150_words_list.txt', 'r', encoding='utf-8') as f:
            wordlist_150 = [line.strip().lower() for line in f.readlines() if line.strip()]
        
        print(f"150è¯è¡¨ä¸­çš„è¯æ±‡æ•°: {len(wordlist_150)}")
        
        # è¯»å–è¯é¢‘ç»Ÿè®¡ç»“æœ
        frequency_words = set()
        with open('filtered_sentences_150_words_clean_word_frequency_new.txt', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
            # æ‰¾åˆ°è¯é¢‘åˆ—è¡¨çš„å¼€å§‹ä½ç½®
            start_reading = False
            for line in lines:
                if "å®Œæ•´è¯é¢‘åˆ—è¡¨" in line:
                    start_reading = True
                    continue
                if start_reading and line.strip().startswith(tuple('0123456789')):
                    # è§£æè¯é¢‘è¡Œï¼Œæ ¼å¼: "    1. you                     1699    8.05%     8.05%"
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        word = parts[1]  # ç¬¬äºŒéƒ¨åˆ†æ˜¯è¯æ±‡
                        frequency_words.add(word.lower())
        
        print(f"è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡æ•°: {len(frequency_words)}")
        
        # æ‰¾å‡ºåœ¨150è¯è¡¨ä¸­ä½†ä¸åœ¨è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡
        missing_from_frequency = []
        for word in wordlist_150:
            if word not in frequency_words:
                missing_from_frequency.append(word)
        
        # æ‰¾å‡ºåœ¨è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡
        missing_from_wordlist = []
        for word in frequency_words:
            if word not in wordlist_150:
                missing_from_wordlist.append(word)
        
        print(f"\nğŸ“Š æ¯”è¾ƒç»“æœ:")
        print(f"   åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡: {len(missing_from_frequency)} ä¸ª")
        print(f"   åœ¨è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡: {len(missing_from_wordlist)} ä¸ª")
        
        if missing_from_frequency:
            print(f"\nâŒ åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨å¥å­ä¸­çš„è¯æ±‡:")
            print("-" * 50)
            for i, word in enumerate(sorted(missing_from_frequency), 1):
                print(f"   {i:>2}. {word}")
        
        if missing_from_wordlist:
            print(f"\nâš ï¸  åœ¨è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡:")
            print("-" * 50)
            for i, word in enumerate(sorted(missing_from_wordlist), 1):
                print(f"   {i:>2}. {word}")
        
        # æ£€æŸ¥150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡
        wordlist_150_original = []
        with open('materials/150_words_list.txt', 'r', encoding='utf-8') as f:
            wordlist_150_original = [line.strip() for line in f.readlines() if line.strip()]
        
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œé‡å¤æ£€æŸ¥
        wordlist_150_lower = [word.lower() for word in wordlist_150_original]
        duplicates = []
        seen = set()
        for i, word in enumerate(wordlist_150_lower):
            if word in seen:
                duplicates.append((i+1, wordlist_150_original[i], word))
            else:
                seen.add(word)
        
        if duplicates:
            print(f"\nğŸ”„ 150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡:")
            print("-" * 50)
            for line_num, original_word, lower_word in duplicates:
                print(f"   ç¬¬{line_num}è¡Œ: '{original_word}' (å°å†™: '{lower_word}')")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open('wordlist_frequency_comparison.txt', 'w', encoding='utf-8') as f:
            f.write("150è¯è¡¨ä¸è¯é¢‘ç»Ÿè®¡å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"150è¯è¡¨ä¸­çš„è¯æ±‡æ•°: {len(wordlist_150)}\n")
            f.write(f"è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡æ•°: {len(frequency_words)}\n\n")
            
            f.write(f"åœ¨150è¯è¡¨ä¸­ä½†æœªå‡ºç°åœ¨è¯é¢‘ç»Ÿè®¡ä¸­çš„è¯æ±‡ ({len(missing_from_frequency)} ä¸ª):\n")
            f.write("-" * 50 + "\n")
            for i, word in enumerate(sorted(missing_from_frequency), 1):
                f.write(f"{i:>2}. {word}\n")
            
            f.write(f"\nåœ¨è¯é¢‘ç»Ÿè®¡ä¸­ä½†ä¸åœ¨150è¯è¡¨ä¸­çš„è¯æ±‡ ({len(missing_from_wordlist)} ä¸ª):\n")
            f.write("-" * 50 + "\n")
            for i, word in enumerate(sorted(missing_from_wordlist), 1):
                f.write(f"{i:>2}. {word}\n")
            
            if duplicates:
                f.write(f"\n150è¯è¡¨ä¸­çš„é‡å¤è¯æ±‡ ({len(duplicates)} ä¸ª):\n")
                f.write("-" * 50 + "\n")
                for line_num, original_word, lower_word in duplicates:
                    f.write(f"ç¬¬{line_num}è¡Œ: '{original_word}' (å°å†™: '{lower_word}')\n")
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: wordlist_frequency_comparison.txt")
        
        return missing_from_frequency, missing_from_wordlist, duplicates
        
    except FileNotFoundError as e:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {e}")
        return None, None, None
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None, None

def main():
    """ä¸»å‡½æ•°"""
    missing_freq, missing_wordlist, duplicates = compare_wordlist_and_frequency()
    
    if missing_freq is not None:
        print(f"\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")
        if missing_freq:
            print(f"ğŸš¨ å‘ç° {len(missing_freq)} ä¸ªè¯æ±‡åœ¨150è¯è¡¨ä¸­ä½†æœªåœ¨å¥å­ä¸­å‡ºç°")
        if duplicates:
            print(f"ğŸ”„ å‘ç° {len(duplicates)} ä¸ªé‡å¤è¯æ±‡åœ¨150è¯è¡¨ä¸­")

if __name__ == "__main__":
    main()
