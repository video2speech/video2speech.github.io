#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer

def analyze_merged_vocab_results():
    """åˆ†æä½¿ç”¨ merged_vocabulary.txt ç­›é€‰çš„ç»“æœ"""
    
    print("=" * 80)
    print("ğŸ¯ merged_vocabulary.txt ç­›é€‰ç»“æœæ€»ç»“")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('filtered_sentences_merged_vocab_clean.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å¥å­æ•°: {len(sentences):,} ä¸ªï¼ˆå·²å»é‡ï¼‰")
        print(f"   è¯æ±‡è¡¨: merged_vocabulary.txt (1,200ä¸ªè¯)")
        print(f"   é•¿åº¦è¦æ±‚: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        
        # ä½¿ç”¨åˆ†è¯å™¨åˆ†ææ¯ä¸ªå¥å­
        tokenizer = TreebankWordTokenizer()
        word_counts = []
        
        for sentence in sentences[:1000]:  # åˆ†æå‰1000ä¸ªå¥å­ä»¥æé«˜é€Ÿåº¦
            tokens = tokenizer.tokenize(sentence)
            # åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            words = [token for token in tokens if token not in ".,!?;:()\"'-"]
            word_counts.append(len(words))
        
        if word_counts:
            print(f"   å¹³å‡è¯æ•°: {sum(word_counts)/len(word_counts):.2f} è¯")
            print(f"   æœ€çŸ­å¥å­: {min(word_counts)} è¯")
            print(f"   æœ€é•¿å¥å­: {max(word_counts)} è¯")
        
        # é•¿åº¦åˆ†å¸ƒï¼ˆåŸºäºå‰1000ä¸ªå¥å­ï¼‰
        if word_counts:
            dist_4_5 = sum(1 for c in word_counts if 4 <= c <= 5)
            dist_6_10 = sum(1 for c in word_counts if 6 <= c <= 10)
            dist_11_plus = sum(1 for c in word_counts if c > 10)
            
            print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒï¼ˆåŸºäºå‰1000ä¸ªå¥å­æ ·æœ¬ï¼‰:")
            print(f"   4-5è¯å¥å­: {dist_4_5} ({dist_4_5/len(word_counts)*100:.1f}%)")
            print(f"   6-10è¯å¥å­: {dist_6_10} ({dist_6_10/len(word_counts)*100:.1f}%)")
            print(f"   >10è¯å¥å­: {dist_11_plus} ({dist_11_plus/len(word_counts)*100:.1f}%)")
        
        # ä¸å…¶ä»–è¯æ±‡è¡¨å¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸å…¶ä»–è¯æ±‡è¡¨å¯¹æ¯”:")
        print(f"   199è¯è¯æ±‡è¡¨ (â‰¥4è¯): 2,211ä¸ªå¥å­")
        print(f"   1,200è¯è¯æ±‡è¡¨ (â‰¥4è¯): {len(sentences):,}ä¸ªå¥å­")
        print(f"   å¢é•¿: {len(sentences) - 2211:,} ä¸ªå¥å­ ({(len(sentences) - 2211)/2211*100:.1f}%)")
        
        # æ˜¾ç¤ºä¸åŒç±»å‹çš„å¥å­æ ·æœ¬
        print(f"\nğŸ“ å¥å­æ ·æœ¬:")
        
        # çŸ­å¥æ ·æœ¬
        short_sentences = [sentences[i] for i, c in enumerate(word_counts) if 4 <= c <= 5]
        if short_sentences:
            print(f"\n   4-5è¯çŸ­å¥æ ·æœ¬:")
            for sentence in short_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # ä¸­ç­‰é•¿åº¦å¥å­æ ·æœ¬
        medium_sentences = [sentences[i] for i, c in enumerate(word_counts) if 6 <= c <= 10]
        if medium_sentences:
            print(f"\n   6-10è¯ä¸­å¥æ ·æœ¬:")
            for sentence in medium_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # é•¿å¥æ ·æœ¬
        long_sentences = [sentences[i] for i, c in enumerate(word_counts) if c > 10]
        if long_sentences:
            print(f"\n   >10è¯é•¿å¥æ ·æœ¬:")
            for sentence in long_sentences[:4]:
                print(f"   â€¢ {sentence}")
        
        # éšæœºå¥å­æ ·æœ¬
        print(f"\n   éšæœºå¥å­æ ·æœ¬:")
        import random
        random_samples = random.sample(sentences, min(8, len(sentences)))
        for sentence in random_samples:
            print(f"   â€¢ {sentence}")
        
        # ç­›é€‰æ¡ä»¶æ€»ç»“
        print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶æ€»ç»“:")
        print(f"   âœ… è¯æ±‡è¡¨: merged_vocabulary.txt (1,200ä¸ªç²¾é€‰è¯æ±‡)")
        print(f"   âœ… åŒ…å«: test.txt (1,284è¯) + æ–°å¢50è¯ï¼Œå»é‡å1,200è¯")
        print(f"   âœ… é•¿åº¦é™åˆ¶: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        print(f"   âœ… è¯æ±‡é™åˆ¶: æ‰€æœ‰è¯æ±‡éƒ½å¿…é¡»åœ¨è¯æ±‡è¡¨ä¸­")
        print(f"   âœ… æ•°æ®æº: ç”µå½±å°è¯ (movie_lines.tsv)")
        print(f"   âœ… å¤„ç†è¡Œæ•°: 50,000è¡Œå¯¹è¯")
        print(f"   âœ… å·²å»é‡: ç§»é™¤äº†1,722ä¸ªé‡å¤å¥å­")
        
        print(f"\nğŸ’¡ è¿™{len(sentences):,}ä¸ªå¥å­çš„ç‰¹ç‚¹:")
        print(f"   â€¢ ä½¿ç”¨1,200ä¸ªç²¾é€‰è¯æ±‡ï¼ˆæœ€ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼‰")
        print(f"   â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ4-29è¯ï¼‰")
        print(f"   â€¢ æ¥è‡ªçœŸå®çš„ç”µå½±å¯¹è¯")
        print(f"   â€¢ è¯æ±‡ä¸°å¯Œï¼Œè¡¨è¾¾å¤šæ ·")
        print(f"   â€¢ é€‚åˆå„ç§è‹±è¯­å­¦ä¹ çº§åˆ«")
        
        return sentences, word_counts
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°å¥å­æ–‡ä»¶")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    sentences, word_counts = analyze_merged_vocab_results()
    
    if sentences:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶: filtered_sentences_merged_vocab_clean.txt")
        print(f"ğŸ¯ {len(sentences):,} ä¸ªé«˜è´¨é‡çš„è‹±è¯­å¥å­å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"ğŸš€ è¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€ä¸°å¯Œçš„å¥å­é›†åˆï¼")

if __name__ == "__main__":
    main()
