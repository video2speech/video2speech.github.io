#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import nltk
from collections import Counter

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')

def analyze_word_frequency_in_file(filename):
    """é‡æ–°åˆ†ææ–‡ä»¶ä¸­çš„è¯é¢‘"""
    
    print("=" * 80)
    print(f"é‡æ–°åˆ†æ {filename} ä¸­çš„è¯é¢‘")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    try:
        # è¯»å–å¥å­æ–‡ä»¶
        with open(filename, 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"æ€»å¥å­æ•°: {len(sentences)}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = TreebankWordTokenizer()
        
        # ç»Ÿè®¡æ‰€æœ‰è¯æ±‡
        all_words = []
        word_counter = Counter()
        
        for sentence in sentences:
            # åˆ†è¯
            tokens = tokenizer.tokenize(sentence)
            
            # å¤„ç†æ¯ä¸ªè¯æ±‡
            for token in tokens:
                # è·³è¿‡æ ‡ç‚¹ç¬¦å·
                if token in ".,!?;:()\"'-":
                    continue
                
                # è½¬æ¢ä¸ºå°å†™è¿›è¡Œç»Ÿè®¡
                word_lower = token.lower()
                all_words.append(word_lower)
                word_counter[word_lower] += 1
        
        print(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰")
        print(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª")
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = word_counter.most_common()
        
        print(f"\nğŸ“Š è¯é¢‘ç»Ÿè®¡ç»“æœ:")
        print(f"   æœ€é«˜é¢‘è¯: '{sorted_words[0][0]}' å‡ºç° {sorted_words[0][1]} æ¬¡")
        print(f"   æœ€ä½é¢‘è¯: '{sorted_words[-1][0]}' å‡ºç° {sorted_words[-1][1]} æ¬¡")
        
        # é¢‘ç‡åˆ†å¸ƒç»Ÿè®¡
        freq_distribution = Counter(word_counter.values())
        print(f"\nğŸ“ˆ é¢‘ç‡åˆ†å¸ƒ:")
        for freq in sorted(freq_distribution.keys(), reverse=True)[:10]:
            count = freq_distribution[freq]
            print(f"   å‡ºç° {freq:>3} æ¬¡çš„è¯æ±‡: {count:>3} ä¸ª")
        
        # æ˜¾ç¤ºå‰30ä¸ªé«˜é¢‘è¯
        print(f"\nğŸ” å‰30ä¸ªé«˜é¢‘è¯:")
        print("-" * 60)
        print(f"{'æ’å':<4} {'è¯æ±‡':<15} {'é¢‘ç‡':<6} {'å æ¯”':<8} {'ç´¯ç§¯å æ¯”'}")
        print("-" * 60)
        
        cumulative_freq = 0
        for i, (word, freq) in enumerate(sorted_words[:30], 1):
            percentage = freq / len(all_words) * 100
            cumulative_freq += freq
            cumulative_percentage = cumulative_freq / len(all_words) * 100
            print(f"{i:>3}. {word:<15} {freq:>5} {percentage:>7.2f}% {cumulative_percentage:>8.2f}%")
        
        # æ˜¾ç¤ºå20ä¸ªä½é¢‘è¯
        print(f"\nğŸ”» å20ä¸ªä½é¢‘è¯:")
        print("-" * 50)
        print(f"{'æ’å':<4} {'è¯æ±‡':<15} {'é¢‘ç‡':<6}")
        print("-" * 50)
        
        for i, (word, freq) in enumerate(sorted_words[-20:], len(sorted_words) - 19):
            print(f"{i:>3}. {word:<15} {freq:>5}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        output_file = filename.replace('.txt', '_word_frequency_new.txt')
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"é‡æ–°è¯é¢‘åˆ†æç»“æœ - {filename}\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"æ€»å¥å­æ•°: {len(sentences)}\n")
            f.write(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰\n")
            f.write(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª\n\n")
            
            f.write("å®Œæ•´è¯é¢‘åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡é™åºæ’åˆ—ï¼‰:\n")
            f.write("-" * 70 + "\n")
            f.write(f"{'æ’å':<6} {'è¯æ±‡':<20} {'é¢‘ç‡':<8} {'å æ¯”':<8} {'ç´¯ç§¯å æ¯”'}\n")
            f.write("-" * 70 + "\n")
            
            cumulative_freq = 0
            for i, (word, freq) in enumerate(sorted_words, 1):
                percentage = freq / len(all_words) * 100
                cumulative_freq += freq
                cumulative_percentage = cumulative_freq / len(all_words) * 100
                f.write(f"{i:>5}. {word:<20} {freq:>7} {percentage:>7.2f}% {cumulative_percentage:>8.2f}%\n")
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = filename.replace('.txt', '_word_frequency_new.csv')
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("æ’å,è¯æ±‡,é¢‘ç‡,å æ¯”,ç´¯ç§¯å æ¯”\n")
            cumulative_freq = 0
            for i, (word, freq) in enumerate(sorted_words, 1):
                percentage = freq / len(all_words) * 100
                cumulative_freq += freq
                cumulative_percentage = cumulative_freq / len(all_words) * 100
                f.write(f"{i},{word},{freq},{percentage:.2f}%,{cumulative_percentage:.2f}%\n")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   ğŸ“„ {output_file} - è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š")
        print(f"   ğŸ“Š {csv_file} - CSVæ ¼å¼æ•°æ®")
        
        # åˆ†æä¸€äº›æœ‰è¶£çš„ç»Ÿè®¡
        print(f"\nğŸ” æœ‰è¶£çš„ç»Ÿè®¡:")
        
        # è®¡ç®—å‰10ä¸ªè¯æ±‡çš„ç´¯ç§¯å æ¯”
        top_10_freq = sum(freq for _, freq in sorted_words[:10])
        top_10_percentage = top_10_freq / len(all_words) * 100
        print(f"   å‰10ä¸ªé«˜é¢‘è¯å æ€»è¯æ±‡çš„ {top_10_percentage:.1f}%")
        
        # è®¡ç®—å‰20ä¸ªè¯æ±‡çš„ç´¯ç§¯å æ¯”
        top_20_freq = sum(freq for _, freq in sorted_words[:20])
        top_20_percentage = top_20_freq / len(all_words) * 100
        print(f"   å‰20ä¸ªé«˜é¢‘è¯å æ€»è¯æ±‡çš„ {top_20_percentage:.1f}%")
        
        # è®¡ç®—å‰50ä¸ªè¯æ±‡çš„ç´¯ç§¯å æ¯”
        if len(sorted_words) >= 50:
            top_50_freq = sum(freq for _, freq in sorted_words[:50])
            top_50_percentage = top_50_freq / len(all_words) * 100
            print(f"   å‰50ä¸ªé«˜é¢‘è¯å æ€»è¯æ±‡çš„ {top_50_percentage:.1f}%")
        
        # åªå‡ºç°ä¸€æ¬¡çš„è¯æ±‡æ•°é‡
        once_words = sum(1 for freq in word_counter.values() if freq == 1)
        once_percentage = once_words / len(word_counter) * 100
        print(f"   åªå‡ºç°1æ¬¡çš„è¯æ±‡: {once_words} ä¸ª ({once_percentage:.1f}%)")
        
        # å‡ºç°æ¬¡æ•°â‰¤5çš„è¯æ±‡æ•°é‡
        low_freq_words = sum(1 for freq in word_counter.values() if freq <= 5)
        low_freq_percentage = low_freq_words / len(word_counter) * 100
        print(f"   å‡ºç°â‰¤5æ¬¡çš„è¯æ±‡: {low_freq_words} ä¸ª ({low_freq_percentage:.1f}%)")
        
        # å‡ºç°æ¬¡æ•°â‰¥100çš„è¯æ±‡æ•°é‡
        high_freq_words = sum(1 for freq in word_counter.values() if freq >= 100)
        print(f"   å‡ºç°â‰¥100æ¬¡çš„è¯æ±‡: {high_freq_words} ä¸ª")
        
        return sorted_words, word_counter, all_words
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        return None, None, None
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None, None

def main():
    """ä¸»å‡½æ•°"""
    filename = 'filtered_sentences_150_words_clean.txt'
    sorted_words, word_counter, all_words = analyze_word_frequency_in_file(filename)
    
    if sorted_words:
        print(f"\nâœ… è¯é¢‘åˆ†æå®Œæˆï¼")
        print(f"ğŸ¯ å…±å‘ç° {len(word_counter)} ä¸ªä¸åŒçš„è¯æ±‡")
        print(f"ğŸ“Š æ€»è®¡ {len(all_words)} ä¸ªè¯æ±‡ä½¿ç”¨å®ä¾‹")

if __name__ == "__main__":
    main()
