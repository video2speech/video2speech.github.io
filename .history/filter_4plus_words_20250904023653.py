#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer

def filter_sentences_4plus_words():
    """ç­›é€‰â‰¥4è¯çš„å¥å­ï¼Œå»é‡ï¼Œç”Ÿæˆç®€å•åˆ—è¡¨"""
    
    print("ç­›é€‰â‰¥4è¯å¥å­å¹¶å»é‡...")
    
    # è¯»å–å¥å­æ–‡ä»¶
    with open('sentences_expanded_vocab.txt', 'r', encoding='utf-8') as f:
        sentences = [line.strip() for line in f.readlines() if line.strip()]
    
    tokenizer = TreebankWordTokenizer()
    filtered_sentences = []
    seen_sentences = set()  # ç”¨äºå»é‡
    
    for sentence in sentences:
        # åˆ†è¯å¹¶åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
        tokens = tokenizer.tokenize(sentence)
        words = [token for token in tokens if token not in ".,!?;:()\"'-"]
        
        # åªä¿ç•™â‰¥4è¯çš„å¥å­
        if len(words) >= 4:
            # å»é‡ï¼ˆè½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼Œä½†ä¿ç•™åŸå§‹æ ¼å¼ï¼‰
            sentence_lower = sentence.lower()
            if sentence_lower not in seen_sentences:
                seen_sentences.add(sentence_lower)
                filtered_sentences.append(sentence)
    
    print(f'ç­›é€‰å‰: {len(sentences)} ä¸ªå¥å­')
    print(f'â‰¥4è¯å¥å­: {len(filtered_sentences)} ä¸ª')
    
    # ä¿å­˜ä¸ºç®€å•åˆ—è¡¨ï¼ˆæ— åºå·ï¼‰
    with open('sentences_4plus_words.txt', 'w', encoding='utf-8') as f:
        for sentence in filtered_sentences:
            f.write(sentence + '\n')
    
    print(f'âœ… å·²ç”Ÿæˆ sentences_4plus_words.txt')
    
    # æ˜¾ç¤ºå‰10ä¸ªå¥å­ä½œä¸ºæ ·æœ¬
    print(f'\nğŸ“ å‰10ä¸ªå¥å­æ ·æœ¬:')
    for i, sentence in enumerate(filtered_sentences[:10], 1):
        print(f'   {sentence}')
    
    return filtered_sentences

if __name__ == "__main__":
    filtered_sentences = filter_sentences_4plus_words()
