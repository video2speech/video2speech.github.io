#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer, sent_tokenize
import nltk
import json
from tqdm import tqdm

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨: {vocab_file}")
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        print(f"æˆåŠŸåŠ è½½ {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    except Exception as e:
        print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
        return set()

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def is_valid_word(word, vocab):
    """æ£€æŸ¥è¯æ˜¯å¦æœ‰æ•ˆ"""
    # è½¬æ¢ä¸ºå°å†™
    word_lower = word.lower()
    
    # å¦‚æœç›´æ¥åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¿”å›True
    if word_lower in vocab:
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ ‡ç‚¹ç¬¦å·ï¼ˆå…è®¸ï¼‰
    if re.match(r'^[^\w\s]+$', word):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—ï¼ˆå…è®¸ï¼‰
    if word.isdigit():
        return True
    
    # å…¶ä»–æƒ…å†µè¿”å›False
    return False

def validate_sentence(sentence, tokenizer, vocab):
    """éªŒè¯å¥å­æ˜¯å¦æ»¡è¶³è¯æ±‡è¦æ±‚"""
    if not sentence.strip():
        return False, []
    
    # TreeBankåˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    # æ£€æŸ¥æ¯ä¸ªè¯
    invalid_words = []
    for token in tokens:
        if not is_valid_word(token, vocab):
            invalid_words.append(token)
    
    is_valid = len(invalid_words) == 0
    return is_valid, invalid_words

def process_movie_lines(movie_file, vocab_file, max_lines=10000):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    print(f"å¼€å§‹å¤„ç†ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab = load_vocabulary(vocab_file)
    if not vocab:
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # è¯»å–ç”µå½±å°è¯
    try:
        df = pd.read_csv(movie_file, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æˆåŠŸåŠ è½½ {len(df)} è¡Œç”µå½±å°è¯")
        
        # é™åˆ¶å¤„ç†è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_lines and len(df) > max_lines:
            df = df.head(max_lines)
            print(f"é™åˆ¶å¤„ç†å‰ {max_lines} è¡Œ")
        
    except Exception as e:
        print(f"è¯»å–ç”µå½±å°è¯æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # ç»“æœåˆ—è¡¨
    validation_results = []
    
    print("å¼€å§‹éªŒè¯å¥å­...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
        line_id = row['line_id']
        original_text = clean_text(row['text'])
        
        if not original_text:
            validation_results.append({
                'line_id': line_id,
                'original_text': '',
                'is_valid': False,
                'reason': 'empty_text',
                'invalid_words': [],
                'sentence_count': 0,
                'sentences': []
            })
            continue
        
        # åˆ†å‰²æˆå¥å­
        try:
            sentences = sent_tokenize(original_text)
        except:
            sentences = [original_text]  # å¦‚æœåˆ†å¥å¤±è´¥ï¼Œå°±å½“ä½œä¸€ä¸ªå¥å­
        
        # éªŒè¯æ¯ä¸ªå¥å­
        sentence_results = []
        all_sentences_valid = True
        all_invalid_words = []
        
        for sentence in sentences:
            is_valid, invalid_words = validate_sentence(sentence, tokenizer, vocab)
            sentence_results.append({
                'sentence': sentence,
                'is_valid': is_valid,
                'invalid_words': invalid_words
            })
            
            if not is_valid:
                all_sentences_valid = False
                all_invalid_words.extend(invalid_words)
        
        # æ•´ä½“ç»“æœ
        validation_results.append({
            'line_id': line_id,
            'original_text': original_text,
            'is_valid': all_sentences_valid,
            'reason': 'valid' if all_sentences_valid else 'contains_invalid_words',
            'invalid_words': list(set(all_invalid_words)),
            'sentence_count': len(sentences),
            'sentences': sentence_results
        })
    
    return validation_results

def save_results(results, output_file):
    """ä¿å­˜ç»“æœ"""
    print(f"ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_lines = len(results)
    valid_lines = sum(1 for r in results if r['is_valid'])
    invalid_lines = total_lines - valid_lines
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ç”µå½±å°è¯éªŒè¯ç»“æœ\n")
        f.write(f"# æ€»è¡Œæ•°: {total_lines}\n")
        f.write(f"# æœ‰æ•ˆè¡Œæ•°: {valid_lines} ({valid_lines/total_lines*100:.2f}%)\n")
        f.write(f"# æ— æ•ˆè¡Œæ•°: {invalid_lines} ({invalid_lines/total_lines*100:.2f}%)\n")
        f.write("#" + "="*80 + "\n\n")
        
        for i, result in enumerate(results):
            f.write(f"{i+1}. è¡ŒID: {result['line_id']}\n")
            f.write(f"   åŸæ–‡: {result['original_text'][:100]}{'...' if len(result['original_text']) > 100 else ''}\n")
            f.write(f"   æ˜¯å¦æœ‰æ•ˆ: {'âœ… æ˜¯' if result['is_valid'] else 'âŒ å¦'}\n")
            
            if not result['is_valid'] and result['invalid_words']:
                f.write(f"   æ— æ•ˆè¯æ±‡: {result['invalid_words'][:10]}{'...' if len(result['invalid_words']) > 10 else ''}\n")
            
            f.write(f"   å¥å­æ•°é‡: {result['sentence_count']}\n")
            f.write("\n")
    
    # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºè¯»å–ï¼‰
    json_file = output_file.replace('.txt', '.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"  - æ–‡æœ¬æ ¼å¼: {output_file}")
    print(f"  - JSONæ ¼å¼: {json_file}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è¡Œæ•°: {total_lines}")
    print(f"  æœ‰æ•ˆè¡Œæ•°: {valid_lines} ({valid_lines/total_lines*100:.2f}%)")
    print(f"  æ— æ•ˆè¡Œæ•°: {invalid_lines} ({invalid_lines/total_lines*100:.2f}%)")

def main():
    """ä¸»å‡½æ•°"""
    movie_file = 'movie_lines.tsv'
    vocab_file = 'top_1200_words_simple.txt'
    output_file = 'sentence_validation_results.txt'
    
    # å¤„ç†ç”µå½±å°è¯ï¼ˆé™åˆ¶å¤„ç†è¡Œæ•°ä»¥æé«˜é€Ÿåº¦ï¼‰
    results = process_movie_lines(movie_file, vocab_file, max_lines=5000)
    
    if results:
        # ä¿å­˜ç»“æœ
        save_results(results, output_file)
        
        # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
        print(f"\nğŸ“ å‰5ä¸ªç»“æœæ ·æœ¬:")
        for i, result in enumerate(results[:5]):
            status = "âœ… æœ‰æ•ˆ" if result['is_valid'] else "âŒ æ— æ•ˆ"
            print(f"{i+1}. {status} - {result['original_text'][:50]}...")
            if not result['is_valid'] and result['invalid_words']:
                print(f"   æ— æ•ˆè¯: {result['invalid_words'][:5]}")

if __name__ == "__main__":
    main()
