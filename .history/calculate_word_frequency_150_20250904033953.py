#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
from collections import Counter
import csv

def calculate_word_frequency():
    """è®¡ç®— filtered_sentences_150_words_clean.txt ä¸­æ‰€æœ‰è¯çš„è¯é¢‘"""
    
    print("=" * 80)
    print("ğŸ“Š è®¡ç®— filtered_sentences_150_words_clean.txt è¯é¢‘ç»Ÿè®¡")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('filtered_sentences_150_words_clean.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"âœ… æˆåŠŸè¯»å– {len(sentences):,} ä¸ªå¥å­")
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶ filtered_sentences_150_words_clean.txt")
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œè®¡æ•°å™¨
    tokenizer = TreebankWordTokenizer()
    word_counts = Counter()
    total_tokens = 0
    total_words = 0
    
    print("ğŸ”„ æ­£åœ¨åˆ†æè¯é¢‘...")
    
    # å¤„ç†æ¯ä¸ªå¥å­
    for sentence in sentences:
        # åˆ†è¯
        tokens = tokenizer.tokenize(sentence)
        total_tokens += len(tokens)
        
        # åˆ†ææ¯ä¸ªtoken
        for token in tokens:
            # åªè®¡ç®—å­—æ¯è¯æ±‡ï¼Œå¿½ç•¥çº¯æ ‡ç‚¹ç¬¦å·
            if token.isalpha():
                word_counts[token.lower()] += 1
                total_words += 1
    
    print(f"âœ… åˆ†æå®Œæˆï¼")
    print(f"   æ€»tokenæ•°: {total_tokens:,}")
    print(f"   æ€»è¯æ±‡æ•°: {total_words:,} (ä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·)")
    print(f"   å”¯ä¸€è¯æ±‡æ•°: {len(word_counts):,}")
    
    # æŒ‰é¢‘ç‡æ’åº
    sorted_words = word_counts.most_common()
    
    # è®¡ç®—ç´¯ç§¯é¢‘ç‡
    cumulative_freq = 0
    word_stats = []
    
    for rank, (word, count) in enumerate(sorted_words, 1):
        cumulative_freq += count
        percentage = (count / total_words) * 100
        cumulative_percentage = (cumulative_freq / total_words) * 100
        
        word_stats.append({
            'rank': rank,
            'word': word,
            'count': count,
            'percentage': percentage,
            'cumulative_count': cumulative_freq,
            'cumulative_percentage': cumulative_percentage
        })
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°TXTæ–‡ä»¶
    output_txt = 'filtered_sentences_150_words_frequency.txt'
    with open(output_txt, 'w', encoding='utf-8') as f:
        f.write("filtered_sentences_150_words_clean.txt è¯é¢‘ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"æ•°æ®æº: filtered_sentences_150_words_clean.txt\n")
        f.write(f"å¥å­æ€»æ•°: {len(sentences):,}\n")
        f.write(f"æ€»è¯æ±‡æ•°: {total_words:,} (ä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·)\n")
        f.write(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counts):,}\n")
        f.write(f"å¹³å‡æ¯å¥è¯æ•°: {total_words/len(sentences):.2f}\n\n")
        
        f.write("æ’å  è¯æ±‡        é¢‘æ¬¡    å æ¯”      ç´¯ç§¯é¢‘æ¬¡  ç´¯ç§¯å æ¯”\n")
        f.write("-" * 70 + "\n")
        
        for stats in word_stats:
            f.write(f"{stats['rank']:3d}. {stats['word']:<12} {stats['count']:4d} {stats['percentage']:6.2f}%  "
                   f"{stats['cumulative_count']:6d}  {stats['cumulative_percentage']:6.2f}%\n")
    
    # ä¿å­˜CSVæ ¼å¼
    output_csv = 'filtered_sentences_150_words_frequency.csv'
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['æ’å', 'è¯æ±‡', 'é¢‘æ¬¡', 'å æ¯”(%)', 'ç´¯ç§¯é¢‘æ¬¡', 'ç´¯ç§¯å æ¯”(%)'])
        
        for stats in word_stats:
            writer.writerow([
                stats['rank'],
                stats['word'],
                stats['count'],
                f"{stats['percentage']:.2f}",
                stats['cumulative_count'],
                f"{stats['cumulative_percentage']:.2f}"
            ])
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   ğŸ“„ {output_txt} - è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š")
    print(f"   ğŸ“Š {output_csv} - CSVæ ¼å¼æ•°æ®")
    
    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    print(f"\nğŸ“ˆ è¯é¢‘ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æœ€é«˜é¢‘è¯æ±‡: '{sorted_words[0][0]}' ({sorted_words[0][1]}æ¬¡, {sorted_words[0][1]/total_words*100:.2f}%)")
    
    # æ˜¾ç¤ºå‰20é«˜é¢‘è¯
    print(f"\nğŸ” å‰20é«˜é¢‘è¯æ±‡:")
    for i, (word, count) in enumerate(sorted_words[:20], 1):
        percentage = (count / total_words) * 100
        print(f"   {i:2d}. {word:<12} {count:4d}æ¬¡ ({percentage:5.2f}%)")
    
    # é¢‘ç‡åˆ†å¸ƒåˆ†æ
    freq_1 = sum(1 for count in word_counts.values() if count == 1)
    freq_2_5 = sum(1 for count in word_counts.values() if 2 <= count <= 5)
    freq_6_10 = sum(1 for count in word_counts.values() if 6 <= count <= 10)
    freq_11_20 = sum(1 for count in word_counts.values() if 11 <= count <= 20)
    freq_21_plus = sum(1 for count in word_counts.values() if count > 20)
    
    print(f"\nğŸ“Š è¯é¢‘åˆ†å¸ƒ:")
    print(f"   å‡ºç°1æ¬¡: {freq_1}ä¸ªè¯ ({freq_1/len(word_counts)*100:.1f}%)")
    print(f"   å‡ºç°2-5æ¬¡: {freq_2_5}ä¸ªè¯ ({freq_2_5/len(word_counts)*100:.1f}%)")
    print(f"   å‡ºç°6-10æ¬¡: {freq_6_10}ä¸ªè¯ ({freq_6_10/len(word_counts)*100:.1f}%)")
    print(f"   å‡ºç°11-20æ¬¡: {freq_11_20}ä¸ªè¯ ({freq_11_20/len(word_counts)*100:.1f}%)")
    print(f"   å‡ºç°>20æ¬¡: {freq_21_plus}ä¸ªè¯ ({freq_21_plus/len(word_counts)*100:.1f}%)")
    
    # è¦†ç›–ç‡åˆ†æ
    print(f"\nğŸ¯ è¦†ç›–ç‡åˆ†æ:")
    coverage_points = [10, 20, 30, 50, 80, 100]
    for point in coverage_points:
        if point <= len(sorted_words):
            coverage = sum(count for _, count in sorted_words[:point])
            coverage_pct = (coverage / total_words) * 100
            print(f"   å‰{point:3d}ä¸ªé«˜é¢‘è¯è¦†ç›–: {coverage:5d}è¯æ¬¡ ({coverage_pct:5.1f}%)")
    
    # ä¸150è¯æ±‡è¡¨å¯¹æ¯”
    print(f"\nğŸ” ä¸150è¯æ±‡è¡¨å¯¹æ¯”:")
    try:
        with open('materials/150_words_list.txt', 'r', encoding='utf-8') as f:
            vocab_150 = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        found_in_vocab = sum(1 for word in word_counts.keys() if word in vocab_150)
        not_in_vocab = len(word_counts) - found_in_vocab
        
        print(f"   150è¯æ±‡è¡¨ä¸­çš„è¯: {found_in_vocab}ä¸ª")
        print(f"   ä¸åœ¨150è¯æ±‡è¡¨ä¸­: {not_in_vocab}ä¸ª")
        print(f"   è¦†ç›–ç‡: {found_in_vocab/len(word_counts)*100:.1f}%")
        
        # æ˜¾ç¤ºä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if not_in_vocab > 0:
            missing_words = [word for word in word_counts.keys() if word not in vocab_150]
            print(f"   ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯: {', '.join(missing_words[:10])}{'...' if len(missing_words) > 10 else ''}")
        
    except FileNotFoundError:
        print("   âš ï¸  æ— æ³•è¯»å–150è¯æ±‡è¡¨æ–‡ä»¶è¿›è¡Œå¯¹æ¯”")
    
    return word_stats

def main():
    """ä¸»å‡½æ•°"""
    word_stats = calculate_word_frequency()
    
    if word_stats:
        print(f"\nâœ… è¯é¢‘åˆ†æå®Œæˆï¼")
        print(f"ğŸ¯ å…±åˆ†æäº† {len(word_stats)} ä¸ªä¸åŒçš„è¯æ±‡")
        print(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")

if __name__ == "__main__":
    main()
