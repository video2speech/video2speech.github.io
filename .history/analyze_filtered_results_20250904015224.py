#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from collections import Counter
import re

def analyze_filtered_results():
    """åˆ†æè¿‡æ»¤ç»“æœ"""
    
    print("=" * 80)
    print("ç”µå½±å°è¯è¿‡æ»¤ç»“æœåˆ†æ")
    print("=" * 80)
    
    # è¯»å–æœ‰æ•ˆå¥å­
    try:
        with open('filtered_movie_sentences.txt', 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # æå–å¥å­ï¼ˆè·³è¿‡æ³¨é‡Šè¡Œï¼‰
        valid_sentences = []
        for line in lines:
            if line.strip() and not line.startswith('#'):
                # ç§»é™¤è¡Œå·
                sentence = re.sub(r'^\d+\.\s*', '', line.strip())
                valid_sentences.append(sentence)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_sentences)} ä¸ªæœ‰æ•ˆå¥å­")
        
    except Exception as e:
        print(f"âŒ è¯»å–æœ‰æ•ˆå¥å­å¤±è´¥: {e}")
        return
    
    # åˆ†æå¥å­ç‰¹å¾
    print(f"\nğŸ“Š æœ‰æ•ˆå¥å­åˆ†æ:")
    print("-" * 40)
    
    # é•¿åº¦åˆ†æ
    lengths = [len(sentence.split()) for sentence in valid_sentences]
    print(f"å¥å­é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.2f} è¯")
    print(f"  æœ€çŸ­å¥å­: {min(lengths)} è¯")
    print(f"  æœ€é•¿å¥å­: {max(lengths)} è¯")
    
    # é•¿åº¦åˆ†å¸ƒ
    length_dist = Counter(lengths)
    print(f"\né•¿åº¦åˆ†å¸ƒ (å‰10):")
    for length, count in length_dist.most_common(10):
        print(f"  {length} è¯: {count} å¥ ({count/len(valid_sentences)*100:.1f}%)")
    
    # å¥å­ç±»å‹åˆ†æ
    questions = sum(1 for s in valid_sentences if s.endswith('?'))
    exclamations = sum(1 for s in valid_sentences if s.endswith('!'))
    statements = len(valid_sentences) - questions - exclamations
    
    print(f"\nå¥å­ç±»å‹åˆ†å¸ƒ:")
    print(f"  é™ˆè¿°å¥: {statements} ({statements/len(valid_sentences)*100:.1f}%)")
    print(f"  ç–‘é—®å¥: {questions} ({questions/len(valid_sentences)*100:.1f}%)")
    print(f"  æ„Ÿå¹å¥: {exclamations} ({exclamations/len(valid_sentences)*100:.1f}%)")
    
    # æ˜¾ç¤ºä¸åŒé•¿åº¦çš„ç¤ºä¾‹
    print(f"\nğŸ“ ä¸åŒé•¿åº¦å¥å­ç¤ºä¾‹:")
    print("-" * 40)
    
    for target_length in [2, 5, 8, 12, 15]:
        examples = [s for s in valid_sentences if len(s.split()) == target_length]
        if examples:
            print(f"\n{target_length}è¯å¥å­ç¤ºä¾‹:")
            for i, example in enumerate(examples[:3], 1):
                print(f"  {i}. {example}")
    
    # å¸¸è§å¼€å¤´è¯
    first_words = [sentence.split()[0].lower() for sentence in valid_sentences if sentence.split()]
    first_word_freq = Counter(first_words)
    
    print(f"\nğŸ”¤ æœ€å¸¸è§çš„å¼€å¤´è¯ (å‰15):")
    print("-" * 30)
    for word, count in first_word_freq.most_common(15):
        print(f"  '{word}': {count} æ¬¡ ({count/len(valid_sentences)*100:.1f}%)")
    
    # ä¿å­˜åˆ†æç»“æœ
    with open('filtered_sentences_analysis.txt', 'w', encoding='utf-8') as f:
        f.write("ç”µå½±å°è¯è¿‡æ»¤ç»“æœè¯¦ç»†åˆ†æ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æœ‰æ•ˆå¥å­æ€»æ•°: {len(valid_sentences)}\n\n")
        
        f.write("é•¿åº¦ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.2f} è¯\n")
        f.write(f"  æœ€çŸ­å¥å­: {min(lengths)} è¯\n")
        f.write(f"  æœ€é•¿å¥å­: {max(lengths)} è¯\n\n")
        
        f.write("å¥å­ç±»å‹åˆ†å¸ƒ:\n")
        f.write(f"  é™ˆè¿°å¥: {statements} ({statements/len(valid_sentences)*100:.1f}%)\n")
        f.write(f"  ç–‘é—®å¥: {questions} ({questions/len(valid_sentences)*100:.1f}%)\n")
        f.write(f"  æ„Ÿå¹å¥: {exclamations} ({exclamations/len(valid_sentences)*100:.1f}%)\n\n")
        
        f.write("é•¿åº¦åˆ†å¸ƒ:\n")
        for length, count in sorted(length_dist.items()):
            f.write(f"  {length:2d} è¯: {count:4d} å¥ ({count/len(valid_sentences)*100:.1f}%)\n")
        
        f.write(f"\næœ€å¸¸è§å¼€å¤´è¯:\n")
        for word, count in first_word_freq.most_common(20):
            f.write(f"  '{word}': {count} æ¬¡ ({count/len(valid_sentences)*100:.1f}%)\n")
        
        f.write(f"\néšæœºå¥å­æ ·æœ¬ (100ä¸ª):\n")
        import random
        random.seed(42)
        samples = random.sample(valid_sentences, min(100, len(valid_sentences)))
        for i, sentence in enumerate(samples, 1):
            f.write(f"{i:2d}. {sentence}\n")
    
    print(f"\nâœ… è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: filtered_sentences_analysis.txt")
    
    return valid_sentences

def create_training_data():
    """åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®æ ¼å¼"""
    
    print(f"\nğŸš€ åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼:")
    print("-" * 30)
    
    try:
        with open('filtered_movie_sentences.txt', 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # æå–å¥å­
        sentences = []
        for line in lines:
            if line.strip() and not line.startswith('#'):
                sentence = re.sub(r'^\d+\.\s*', '', line.strip())
                sentences.append(sentence)
        
        # åˆ›å»ºä¸åŒæ ¼å¼çš„è®­ç»ƒæ•°æ®
        
        # 1. çº¯æ–‡æœ¬æ ¼å¼ï¼ˆæ¯è¡Œä¸€å¥ï¼‰
        with open('training_sentences_clean.txt', 'w', encoding='utf-8') as f:
            for sentence in sentences:
                f.write(sentence + '\n')
        
        # 2. æŒ‰é•¿åº¦åˆ†ç»„
        length_groups = {}
        for sentence in sentences:
            length = len(sentence.split())
            if length not in length_groups:
                length_groups[length] = []
            length_groups[length].append(sentence)
        
        # ä¿å­˜çŸ­å¥ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰
        short_sentences = []
        for length in range(1, 8):  # 1-7è¯çš„çŸ­å¥
            if length in length_groups:
                short_sentences.extend(length_groups[length])
        
        with open('training_sentences_short.txt', 'w', encoding='utf-8') as f:
            f.write(f"# çŸ­å¥è®­ç»ƒæ•°æ® (1-7è¯ï¼Œå…±{len(short_sentences)}å¥)\n")
            for sentence in short_sentences:
                f.write(sentence + '\n')
        
        # ä¿å­˜ä¸­ç­‰é•¿åº¦å¥å­ï¼ˆé€‚åˆè¿›é˜¶ï¼‰
        medium_sentences = []
        for length in range(8, 15):  # 8-14è¯çš„ä¸­ç­‰å¥å­
            if length in length_groups:
                medium_sentences.extend(length_groups[length])
        
        with open('training_sentences_medium.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ä¸­ç­‰é•¿åº¦å¥å­è®­ç»ƒæ•°æ® (8-14è¯ï¼Œå…±{len(medium_sentences)}å¥)\n")
            for sentence in medium_sentences:
                f.write(sentence + '\n')
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ç”Ÿæˆ:")
        print(f"   ğŸ“„ training_sentences_clean.txt - å…¨éƒ¨å¥å­ ({len(sentences)}å¥)")
        print(f"   ğŸ“„ training_sentences_short.txt - çŸ­å¥ ({len(short_sentences)}å¥)")
        print(f"   ğŸ“„ training_sentences_medium.txt - ä¸­ç­‰å¥å­ ({len(medium_sentences)}å¥)")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºè®­ç»ƒæ•°æ®å¤±è´¥: {e}")

if __name__ == "__main__":
    valid_sentences = analyze_filtered_results()
    if valid_sentences:
        create_training_data()
    
    print(f"\n" + "="*80)
    print("ğŸ“‹ æ€»ç»“:")
    print("="*80)
    print("âœ… æˆåŠŸä»10,000è¡Œç”µå½±å¯¹è¯ä¸­è¿‡æ»¤å‡º5,319ä¸ªæœ‰æ•ˆå¥å­")
    print("âœ… æœ‰æ•ˆç‡: 32.97% (çº¦1/3çš„å¥å­ä»…ä½¿ç”¨Top1200è¯æ±‡)")
    print("âœ… è¿™äº›å¥å­å¯ç”¨äº:")
    print("   â€¢ è‹±è¯­å­¦ä¹ ææ–™")
    print("   â€¢ è¯­è¨€æ¨¡å‹è®­ç»ƒ")
    print("   â€¢ å£è¯­ç»ƒä¹ ")
    print("   â€¢ è¯æ±‡å­¦ä¹ è¾…åŠ©")
