#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import nltk
from collections import Counter
import re

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')

def remove_sentences_with_numbers_and_recount(input_file):
    """ç§»é™¤åŒ…å«æ•°å­—çš„å¥å­å¹¶é‡æ–°ç»Ÿè®¡è¯é¢‘"""
    
    print("=" * 80)
    print(f"ç§»é™¤åŒ…å«æ•°å­—çš„å¥å­å¹¶é‡æ–°ç»Ÿè®¡è¯é¢‘")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    try:
        # è¯»å–åŸå§‹å¥å­
        with open(input_file, 'r', encoding='utf-8') as f:
            original_sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"åŸå§‹å¥å­æ•°: {len(original_sentences)}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = TreebankWordTokenizer()
        
        # è¿‡æ»¤åŒ…å«æ•°å­—çš„å¥å­
        clean_sentences = []
        removed_sentences = []
        
        for sentence in original_sentences:
            # åˆ†è¯æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—
            tokens = tokenizer.tokenize(sentence)
            contains_number = False
            
            for token in tokens:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆåŒ…æ‹¬çº¯æ•°å­—å’ŒåŒ…å«æ•°å­—çš„è¯ï¼‰
                if re.search(r'\d', token):
                    contains_number = True
                    break
            
            if contains_number:
                removed_sentences.append(sentence)
            else:
                clean_sentences.append(sentence)
        
        print(f"ç§»é™¤å¥å­æ•°: {len(removed_sentences)}")
        print(f"ä¿ç•™å¥å­æ•°: {len(clean_sentences)}")
        print(f"ä¿ç•™ç‡: {len(clean_sentences)/len(original_sentences)*100:.1f}%")
        
        # ä¿å­˜æ¸…ç†åçš„å¥å­
        output_file = input_file.replace('.txt', '_no_numbers.txt')
        with open(output_file, 'w', encoding='utf-8') as f:
            for sentence in clean_sentences:
                f.write(sentence + '\n')
        
        print(f"\nâœ… æ¸…ç†åçš„å¥å­å·²ä¿å­˜: {output_file}")
        
        # æ˜¾ç¤ºä¸€äº›è¢«ç§»é™¤çš„å¥å­æ ·æœ¬
        if removed_sentences:
            print(f"\nğŸ—‘ï¸ è¢«ç§»é™¤çš„å¥å­æ ·æœ¬ (å‰10ä¸ª):")
            for i, sentence in enumerate(removed_sentences[:10], 1):
                print(f"{i:2d}. {sentence}")
            
            if len(removed_sentences) > 10:
                print(f"... è¿˜æœ‰ {len(removed_sentences) - 10} ä¸ªå¥å­è¢«ç§»é™¤")
        
        # é‡æ–°ç»Ÿè®¡è¯é¢‘
        print(f"\nå¼€å§‹é‡æ–°ç»Ÿè®¡è¯é¢‘...")
        
        all_words = []
        word_counter = Counter()
        
        for sentence in clean_sentences:
            # åˆ†è¯
            tokens = tokenizer.tokenize(sentence)
            
            # å¤„ç†æ¯ä¸ªè¯æ±‡
            for token in tokens:
                # è·³è¿‡æ ‡ç‚¹ç¬¦å·
                if token in ".,!?;:()\"'-":
                    continue
                
                # è½¬æ¢ä¸ºå°å†™è¿›è¡Œç»Ÿè®¡
                word_lower = token.lower()
                all_words.append(word_lower)
                word_counter[word_lower] += 1
        
        print(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰")
        print(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª")
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = word_counter.most_common()
        
        print(f"\nğŸ“Š æ–°çš„è¯é¢‘ç»Ÿè®¡ç»“æœ:")
        print(f"   æœ€é«˜é¢‘è¯: '{sorted_words[0][0]}' å‡ºç° {sorted_words[0][1]} æ¬¡")
        if len(sorted_words) > 1:
            print(f"   æœ€ä½é¢‘è¯: '{sorted_words[-1][0]}' å‡ºç° {sorted_words[-1][1]} æ¬¡")
        
        # æ˜¾ç¤ºå‰20ä¸ªé«˜é¢‘è¯
        print(f"\nğŸ” å‰20ä¸ªé«˜é¢‘è¯:")
        print("-" * 50)
        print(f"{'æ’å':<4} {'è¯æ±‡':<15} {'é¢‘ç‡':<6} {'å æ¯”'}")
        print("-" * 50)
        
        for i, (word, freq) in enumerate(sorted_words[:20], 1):
            percentage = freq / len(all_words) * 100
            print(f"{i:>3}. {word:<15} {freq:>5} {percentage:>6.2f}%")
        
        # ä¿å­˜è¯é¢‘åˆ†æç»“æœ
        freq_output_file = output_file.replace('.txt', '_frequency.txt')
        with open(freq_output_file, 'w', encoding='utf-8') as f:
            f.write(f"æ¸…ç†åå¥å­è¯é¢‘åˆ†æç»“æœï¼ˆç§»é™¤å«æ•°å­—å¥å­ï¼‰\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"åŸå§‹å¥å­æ•°: {len(original_sentences)}\n")
            f.write(f"ç§»é™¤å¥å­æ•°: {len(removed_sentences)}\n")
            f.write(f"ä¿ç•™å¥å­æ•°: {len(clean_sentences)}\n")
            f.write(f"ä¿ç•™ç‡: {len(clean_sentences)/len(original_sentences)*100:.1f}%\n\n")
            f.write(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰\n")
            f.write(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª\n\n")
            
            f.write("å®Œæ•´è¯é¢‘åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡é™åºæ’åˆ—ï¼‰:\n")
            f.write("-" * 60 + "\n")
            f.write(f"{'æ’å':<6} {'è¯æ±‡':<20} {'é¢‘ç‡':<8} {'å æ¯”'}\n")
            f.write("-" * 60 + "\n")
            
            for i, (word, freq) in enumerate(sorted_words, 1):
                percentage = freq / len(all_words) * 100
                f.write(f"{i:>5}. {word:<20} {freq:>7} {percentage:>7.2f}%\n")
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = output_file.replace('.txt', '_frequency.csv')
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("æ’å,è¯æ±‡,é¢‘ç‡,å æ¯”\n")
            for i, (word, freq) in enumerate(sorted_words, 1):
                percentage = freq / len(all_words) * 100
                f.write(f"{i},{word},{freq},{percentage:.2f}%\n")
        
        # åˆ›å»ºç§»é™¤æŠ¥å‘Š
        report_file = output_file.replace('.txt', '_removal_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æ•°å­—å¥å­ç§»é™¤æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å¤„ç†æ–‡ä»¶: {input_file}\n")
            f.write(f"å¤„ç†æ—¶é—´: {pd.Timestamp.now() if 'pd' in globals() else 'N/A'}\n\n")
            
            f.write(f"ç»Ÿè®¡ä¿¡æ¯:\n")
            f.write("-" * 30 + "\n")
            f.write(f"åŸå§‹å¥å­æ•°: {len(original_sentences)}\n")
            f.write(f"ç§»é™¤å¥å­æ•°: {len(removed_sentences)}\n")
            f.write(f"ä¿ç•™å¥å­æ•°: {len(clean_sentences)}\n")
            f.write(f"ä¿ç•™ç‡: {len(clean_sentences)/len(original_sentences)*100:.1f}%\n\n")
            
            f.write("è¢«ç§»é™¤çš„å¥å­åˆ—è¡¨:\n")
            f.write("-" * 30 + "\n")
            for i, sentence in enumerate(removed_sentences, 1):
                f.write(f"{i:3d}. {sentence}\n")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   ğŸ“„ {output_file} - æ¸…ç†åçš„å¥å­")
        print(f"   ğŸ“Š {freq_output_file} - è¯é¢‘åˆ†ææŠ¥å‘Š")
        print(f"   ğŸ“Š {csv_file} - è¯é¢‘CSVæ•°æ®")
        print(f"   ğŸ“‹ {report_file} - ç§»é™¤æŠ¥å‘Š")
        
        # åˆ†æä¸€äº›æœ‰è¶£çš„ç»Ÿè®¡
        print(f"\nğŸ” æœ‰è¶£çš„ç»Ÿè®¡:")
        
        # è®¡ç®—å‰10ä¸ªè¯æ±‡çš„ç´¯ç§¯å æ¯”
        if len(sorted_words) >= 10:
            top_10_freq = sum(freq for _, freq in sorted_words[:10])
            top_10_percentage = top_10_freq / len(all_words) * 100
            print(f"   å‰10ä¸ªé«˜é¢‘è¯å æ€»è¯æ±‡çš„ {top_10_percentage:.1f}%")
        
        # è®¡ç®—å‰50ä¸ªè¯æ±‡çš„ç´¯ç§¯å æ¯”
        if len(sorted_words) >= 50:
            top_50_freq = sum(freq for _, freq in sorted_words[:50])
            top_50_percentage = top_50_freq / len(all_words) * 100
            print(f"   å‰50ä¸ªé«˜é¢‘è¯å æ€»è¯æ±‡çš„ {top_50_percentage:.1f}%")
        
        # åªå‡ºç°ä¸€æ¬¡çš„è¯æ±‡æ•°é‡
        once_words = sum(1 for freq in word_counter.values() if freq == 1)
        if len(word_counter) > 0:
            once_percentage = once_words / len(word_counter) * 100
            print(f"   åªå‡ºç°1æ¬¡çš„è¯æ±‡: {once_words} ä¸ª ({once_percentage:.1f}%)")
        
        return clean_sentences, sorted_words, word_counter
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")
        return None, None, None
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None, None

def main():
    """ä¸»å‡½æ•°"""
    try:
        import pandas as pd
        globals()['pd'] = pd
    except ImportError:
        pass
    
    input_file = 'filtered_sentences_150_words_v2_clean.txt'
    clean_sentences, sorted_words, word_counter = remove_sentences_with_numbers_and_recount(input_file)
    
    if clean_sentences and sorted_words:
        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ¯ æ¸…ç†åå¥å­æ•°: {len(clean_sentences):,}")
        print(f"ğŸ“Š å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)}")

if __name__ == "__main__":
    main()
