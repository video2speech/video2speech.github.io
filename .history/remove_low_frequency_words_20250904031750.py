#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import nltk

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')

def remove_low_frequency_words():
    """ç§»é™¤ä½é¢‘è¯æ±‡å¹¶è¿‡æ»¤å¥å­"""
    
    print("=" * 80)
    print("ç§»é™¤ä½é¢‘è¯æ±‡å¹¶è¿‡æ»¤å¥å­")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # è¦ç§»é™¤çš„ä½é¢‘è¯æ±‡åˆ—è¡¨
    words_to_remove = [
        "through", "years", "his", "says", "which", "actually", "used", "has", 
        "family", "into", "bring", "those", "off", "hungry", "day", "went",
        "many", "hundred", "by", "our", "next", "week", "outside", "nurse",
        "ah", "three", "five", "year", "six", "goodbye", "twenty", "closer",
        "38", "bit", "new", "sort", "faith", "their", "7", "1", "2", "48",
        "28", "quite", "computer", "ten", "glasses", "being", "300", "four"
    ]
    
    # è½¬æ¢ä¸ºå°å†™é›†åˆï¼Œä¾¿äºæ¯”è¾ƒ
    words_to_remove_set = set(word.lower() for word in words_to_remove)
    
    print(f"è¦ç§»é™¤çš„è¯æ±‡æ•°: {len(words_to_remove)} ä¸ª")
    
    # 1. å¤„ç†è¯æ±‡åˆ—è¡¨
    try:
        with open('materials/200_words_list.txt', 'r', encoding='utf-8') as f:
            original_words = [word.strip() for word in f.readlines() if word.strip()]
        
        print(f"åŸå§‹è¯æ±‡æ•°: {len(original_words)}")
        
        # è¿‡æ»¤è¯æ±‡
        filtered_words = []
        removed_words = []
        
        for word in original_words:
            if word.lower() not in words_to_remove_set:
                filtered_words.append(word)
            else:
                removed_words.append(word)
        
        print(f"è¿‡æ»¤åè¯æ±‡æ•°: {len(filtered_words)}")
        print(f"å®é™…ç§»é™¤è¯æ±‡æ•°: {len(removed_words)}")
        
        # ä¿å­˜è¿‡æ»¤åçš„è¯æ±‡åˆ—è¡¨
        with open('materials/150_words_list.txt', 'w', encoding='utf-8') as f:
            for word in filtered_words:
                f.write(word + '\n')
        
        print(f"âœ… å·²ä¿å­˜è¿‡æ»¤åçš„è¯æ±‡åˆ—è¡¨: materials/150_words_list.txt")
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°è¯æ±‡åˆ—è¡¨æ–‡ä»¶")
        return
    
    # 2. å¤„ç†å¥å­åˆ—è¡¨
    try:
        with open('materials/200_sentences_list.txt', 'r', encoding='utf-8') as f:
            original_sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"\nåŸå§‹å¥å­æ•°: {len(original_sentences)}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = TreebankWordTokenizer()
        
        # è¿‡æ»¤å¥å­
        filtered_sentences = []
        removed_sentences = []
        
        for sentence in original_sentences:
            # åˆ†è¯
            tokens = tokenizer.tokenize(sentence)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦ç§»é™¤çš„è¯æ±‡
            sentence_words = set(token.lower() for token in tokens if token not in ".,!?;:()\"'-")
            
            # å¦‚æœå¥å­ä¸­æ²¡æœ‰è¦ç§»é™¤çš„è¯æ±‡ï¼Œåˆ™ä¿ç•™
            if not sentence_words.intersection(words_to_remove_set):
                filtered_sentences.append(sentence)
            else:
                # æ‰¾å‡ºå¥å­ä¸­åŒ…å«çš„è¦ç§»é™¤çš„è¯æ±‡
                found_words = sentence_words.intersection(words_to_remove_set)
                removed_sentences.append((sentence, list(found_words)))
        
        print(f"è¿‡æ»¤åå¥å­æ•°: {len(filtered_sentences)}")
        print(f"ç§»é™¤å¥å­æ•°: {len(removed_sentences)}")
        
        # ä¿å­˜è¿‡æ»¤åçš„å¥å­åˆ—è¡¨
        with open('materials/150_sentences_list.txt', 'w', encoding='utf-8') as f:
            for sentence in filtered_sentences:
                f.write(sentence + '\n')
        
        print(f"âœ… å·²ä¿å­˜è¿‡æ»¤åçš„å¥å­åˆ—è¡¨: materials/150_sentences_list.txt")
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°å¥å­åˆ—è¡¨æ–‡ä»¶")
        return
    
    # 3. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_file = 'materials/removal_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ä½é¢‘è¯æ±‡ç§»é™¤æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        f.write("ç§»é™¤çš„è¯æ±‡åˆ—è¡¨:\n")
        f.write("-" * 30 + "\n")
        for i, word in enumerate(removed_words, 1):
            f.write(f"{i:2d}. {word}\n")
        
        f.write(f"\nè¯æ±‡ç»Ÿè®¡:\n")
        f.write("-" * 30 + "\n")
        f.write(f"åŸå§‹è¯æ±‡æ•°: {len(original_words)}\n")
        f.write(f"ç§»é™¤è¯æ±‡æ•°: {len(removed_words)}\n")
        f.write(f"ä¿ç•™è¯æ±‡æ•°: {len(filtered_words)}\n")
        
        f.write(f"\nå¥å­ç»Ÿè®¡:\n")
        f.write("-" * 30 + "\n")
        f.write(f"åŸå§‹å¥å­æ•°: {len(original_sentences)}\n")
        f.write(f"ç§»é™¤å¥å­æ•°: {len(removed_sentences)}\n")
        f.write(f"ä¿ç•™å¥å­æ•°: {len(filtered_sentences)}\n")
        
        f.write(f"\nç§»é™¤çš„å¥å­æ ·æœ¬ (å‰20ä¸ª):\n")
        f.write("-" * 50 + "\n")
        for i, (sentence, found_words) in enumerate(removed_sentences[:20], 1):
            f.write(f"{i:2d}. {sentence[:60]}{'...' if len(sentence) > 60 else ''}\n")
            f.write(f"    åŒ…å«è¯æ±‡: {', '.join(found_words)}\n\n")
    
    print(f"ğŸ“„ å·²ä¿å­˜è¯¦ç»†æŠ¥å‘Š: {report_file}")
    
    # 4. æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    print(f"\nğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
    print(f"   è¯æ±‡åˆ—è¡¨: {len(original_words)} â†’ {len(filtered_words)} ä¸ª")
    print(f"   å¥å­åˆ—è¡¨: {len(original_sentences)} â†’ {len(filtered_sentences)} ä¸ª")
    print(f"   ç§»é™¤è¯æ±‡: {len(removed_words)} ä¸ª")
    print(f"   ç§»é™¤å¥å­: {len(removed_sentences)} ä¸ª")
    
    # æ˜¾ç¤ºä¸€äº›è¢«ç§»é™¤çš„å¥å­æ ·æœ¬
    print(f"\nğŸ—‘ï¸ è¢«ç§»é™¤çš„å¥å­æ ·æœ¬ (å‰5ä¸ª):")
    for i, (sentence, found_words) in enumerate(removed_sentences[:5], 1):
        print(f"{i}. {sentence}")
        print(f"   åŒ…å«è¯æ±‡: {', '.join(found_words)}")
    
    # æ˜¾ç¤ºä¿ç•™çš„å¥å­æ ·æœ¬
    print(f"\nâœ… ä¿ç•™çš„å¥å­æ ·æœ¬ (å‰5ä¸ª):")
    for i, sentence in enumerate(filtered_sentences[:5], 1):
        print(f"{i}. {sentence}")
    
    return filtered_words, filtered_sentences

def main():
    """ä¸»å‡½æ•°"""
    filtered_words, filtered_sentences = remove_low_frequency_words()
    
    if filtered_words and filtered_sentences:
        print(f"\nğŸ¯ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   â€¢ materials/150_words_list.txt - è¿‡æ»¤åçš„è¯æ±‡åˆ—è¡¨")
        print(f"   â€¢ materials/150_sentences_list.txt - è¿‡æ»¤åçš„å¥å­åˆ—è¡¨")
        print(f"   â€¢ materials/removal_report.txt - è¯¦ç»†ç§»é™¤æŠ¥å‘Š")

if __name__ == "__main__":
    main()
