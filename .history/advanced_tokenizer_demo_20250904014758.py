#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import pandas as pd
import re
from collections import Counter

def load_sample_movie_lines(filename, num_samples=50):
    """åŠ è½½ç”µå½±å°è¯æ ·æœ¬"""
    try:
        df = pd.read_csv(filename, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        # æ¸…ç†å’Œè¿‡æ»¤
        df['text'] = df['text'].astype(str)
        df = df[df['text'].str.len().between(20, 150)]  # é€‰æ‹©ä¸­ç­‰é•¿åº¦çš„å¥å­
        
        # é€‰æ‹©åŒ…å«å¸¸è§ç¼©å†™çš„å¥å­
        contractions = ["'m", "'re", "'ve", "'ll", "'d", "n't", "'s"]
        mask = df['text'].str.contains('|'.join(contractions), case=False, na=False)
        contraction_sentences = df[mask].head(num_samples//2)
        
        # é€‰æ‹©åŒ…å«æ ‡ç‚¹çš„å¥å­  
        punctuation_mask = df['text'].str.contains('[,;:!?]', na=False)
        punctuation_sentences = df[punctuation_mask].head(num_samples//2)
        
        # åˆå¹¶æ ·æœ¬
        samples = pd.concat([contraction_sentences, punctuation_sentences]).drop_duplicates()
        
        return samples['text'].tolist()[:num_samples]
        
    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶å‡ºé”™: {e}")
        return []

def demonstrate_advanced_tokenization():
    """é«˜çº§åˆ†è¯æ¼”ç¤º"""
    
    print("=" * 80)
    print("ğŸ¬ ç”µå½±å°è¯ TreebankWordTokenizer é«˜çº§åˆ†è¯æ¼”ç¤º")
    print("=" * 80)
    
    tokenizer = TreebankWordTokenizer()
    
    # åŠ è½½ç”µå½±å°è¯
    movie_lines = load_sample_movie_lines("movie_lines.tsv", 20)
    
    if not movie_lines:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é¢„è®¾çš„ç¤ºä¾‹
        movie_lines = [
            "I'm telling you, we're gonna be rich!",
            "Don't you dare leave me here alone.",
            "What's the matter? You look upset.",
            "I can't believe you'd do this to me.",
            "She's the most beautiful woman I've ever seen.",
            "You're absolutely right about that.",
            "I won't let them hurt you, I promise.",
            "That's not what I meant, and you know it.",
            "We've been through this before, haven't we?",
            "I'd rather die than give up now."
        ]
    
    print(f"\nğŸ“ åˆ†è¯è¯¦ç»†åˆ†æ (å…±{len(movie_lines)}ä¸ªå¥å­):")
    print("-" * 80)
    
    total_words_treebank = 0
    total_words_simple = 0
    
    for i, sentence in enumerate(movie_lines, 1):
        # æ¸…ç†å¥å­
        clean_sentence = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;]', '', sentence).strip()
        
        if len(clean_sentence) < 10:
            continue
            
        # TreebankWordTokenizeråˆ†è¯
        treebank_tokens = tokenizer.tokenize(clean_sentence)
        
        # ç®€å•splitåˆ†è¯
        simple_tokens = clean_sentence.split()
        
        total_words_treebank += len(treebank_tokens)
        total_words_simple += len(simple_tokens)
        
        print(f"\n{i:2d}. åŸå¥: {clean_sentence}")
        print(f"    TreeBank: {treebank_tokens}")
        print(f"    ç®€å•split: {simple_tokens}")
        print(f"    è¯æ•°å¯¹æ¯”: TreeBank({len(treebank_tokens)}) vs Split({len(simple_tokens)})")
        
        # åˆ†æå·®å¼‚
        if len(treebank_tokens) != len(simple_tokens):
            print(f"    ğŸ’¡ å·®å¼‚åˆ†æ:")
            # æ‰¾å‡ºTreeBankå¤šåˆ†å‡ºçš„è¯
            treebank_set = set(treebank_tokens)
            simple_set = set(simple_tokens)
            
            if len(treebank_tokens) > len(simple_tokens):
                print(f"       TreeBankæ›´ç»†è‡´åœ°å¤„ç†äº†ç¼©å†™å’Œæ ‡ç‚¹")
                # æ˜¾ç¤ºç¼©å†™æ‹†åˆ†
                contractions_found = [token for token in treebank_tokens if token in ["'m", "'re", "'ve", "'ll", "'d", "n't", "'s"]]
                if contractions_found:
                    print(f"       å‘ç°ç¼©å†™: {contractions_found}")
                
                # æ˜¾ç¤ºæ ‡ç‚¹åˆ†ç¦»
                punctuation_found = [token for token in treebank_tokens if token in ".,!?;:"]
                if punctuation_found:
                    print(f"       åˆ†ç¦»æ ‡ç‚¹: {punctuation_found}")

def analyze_tokenization_benefits():
    """åˆ†æåˆ†è¯çš„å¥½å¤„"""
    
    print(f"\nğŸ” TreebankWordTokenizer çš„ä¼˜åŠ¿åˆ†æ:")
    print("-" * 50)
    
    tokenizer = TreebankWordTokenizer()
    
    test_cases = [
        {
            "text": "I'm not gonna lie, you're absolutely right!",
            "focus": "ç¼©å†™å¤„ç†"
        },
        {
            "text": "The price is $19.99, isn't it?",
            "focus": "ä»·æ ¼å’Œæ ‡ç‚¹"
        },
        {
            "text": "Visit our website: www.example.com today!",
            "focus": "ç½‘å€å’Œæ ‡ç‚¹"
        },
        {
            "text": "Call us at (555) 123-4567 for help.",
            "focus": "ç”µè¯å·ç "
        },
        {
            "text": "It's a state-of-the-art system, don't you think?",
            "focus": "è¿å­—ç¬¦å’Œç¼©å†™"
        }
    ]
    
    for case in test_cases:
        text = case["text"]
        focus = case["focus"]
        
        treebank_tokens = tokenizer.tokenize(text)
        simple_tokens = text.split()
        
        print(f"\nğŸ“Œ é‡ç‚¹: {focus}")
        print(f"åŸæ–‡: {text}")
        print(f"TreeBank ({len(treebank_tokens)}è¯): {treebank_tokens}")
        print(f"ç®€å•split ({len(simple_tokens)}è¯): {simple_tokens}")

def word_frequency_analysis():
    """è¯é¢‘åˆ†æ"""
    
    print(f"\nğŸ“Š åŸºäºåˆ†è¯çš„è¯é¢‘åˆ†æ:")
    print("-" * 40)
    
    tokenizer = TreebankWordTokenizer()
    movie_lines = load_sample_movie_lines("movie_lines.tsv", 100)
    
    if not movie_lines:
        return
    
    # æ”¶é›†æ‰€æœ‰è¯æ±‡
    all_tokens = []
    for line in movie_lines:
        clean_line = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;]', '', line).strip()
        if len(clean_line) > 5:
            tokens = tokenizer.tokenize(clean_line.lower())
            all_tokens.extend(tokens)
    
    # è¯é¢‘ç»Ÿè®¡
    word_freq = Counter(all_tokens)
    
    print(f"æ€»è¯æ•°: {len(all_tokens)}")
    print(f"ä¸é‡å¤è¯æ•°: {len(word_freq)}")
    
    print(f"\nå‰20ä¸ªæœ€å¸¸è§çš„è¯:")
    for i, (word, freq) in enumerate(word_freq.most_common(20), 1):
        print(f"{i:2d}. '{word}': {freq} æ¬¡")
    
    # åˆ†æç¼©å†™è¯é¢‘
    contractions = [word for word in word_freq.keys() if word in ["'m", "'re", "'ve", "'ll", "'d", "n't", "'s"]]
    if contractions:
        print(f"\nç¼©å†™è¯é¢‘ç»Ÿè®¡:")
        for contraction in contractions:
            print(f"   '{contraction}': {word_freq[contraction]} æ¬¡")

def practical_applications():
    """å®é™…åº”ç”¨ç¤ºä¾‹"""
    
    print(f"\nğŸš€ å®é™…åº”ç”¨åœºæ™¯:")
    print("-" * 30)
    
    tokenizer = TreebankWordTokenizer()
    
    print("1. æ–‡æœ¬é¢„å¤„ç† (ç”¨äºæœºå™¨å­¦ä¹ )")
    print("2. æœç´¢å¼•æ“æŸ¥è¯¢å¤„ç†")
    print("3. è‡ªç„¶è¯­è¨€å¤„ç†ç®¡é“")
    print("4. æ–‡æœ¬ç»Ÿè®¡å’Œåˆ†æ")
    
    # ç¤ºä¾‹ï¼šæ–‡æœ¬æ¸…ç†ç®¡é“
    sample_text = "I'm really excited about this project! Don't you think it's amazing?"
    
    print(f"\nğŸ’¡ æ–‡æœ¬å¤„ç†ç®¡é“ç¤ºä¾‹:")
    print(f"åŸæ–‡: {sample_text}")
    
    # æ­¥éª¤1: åˆ†è¯
    tokens = tokenizer.tokenize(sample_text)
    print(f"1. åˆ†è¯: {tokens}")
    
    # æ­¥éª¤2: è½¬å°å†™
    lower_tokens = [token.lower() for token in tokens]
    print(f"2. è½¬å°å†™: {lower_tokens}")
    
    # æ­¥éª¤3: è¿‡æ»¤æ ‡ç‚¹
    word_tokens = [token for token in lower_tokens if token.isalnum() or token in ["'m", "'re", "'ve", "'ll", "'d", "n't", "'s"]]
    print(f"3. è¿‡æ»¤æ ‡ç‚¹: {word_tokens}")
    
    # æ­¥éª¤4: è¯é¢‘ç»Ÿè®¡
    freq = Counter(word_tokens)
    print(f"4. è¯é¢‘: {dict(freq)}")

if __name__ == "__main__":
    demonstrate_advanced_tokenization()
    analyze_tokenization_benefits()
    word_frequency_analysis()
    practical_applications()
    
    print(f"\n" + "="*80)
    print("âœ… æ€»ç»“: TreebankWordTokenizer vs ç®€å•split()")
    print("="*80)
    print("TreebankWordTokenizer ä¼˜åŠ¿:")
    print("âœ“ æ­£ç¡®æ‹†åˆ†ç¼©å†™ (I'm â†’ ['I', \"'m\"])")
    print("âœ“ åˆ†ç¦»æ ‡ç‚¹ç¬¦å· (Hello! â†’ ['Hello', '!'])")
    print("âœ“ ä¿æŒç‰¹æ®Šæ ¼å¼ (URLs, ç”µè¯å·ç ç­‰)")
    print("âœ“ æ›´å‡†ç¡®çš„è¯æ•°ç»Ÿè®¡")
    print("âœ“ é€‚åˆNLPä¸‹æ¸¸ä»»åŠ¡")
    print("\nç®€å•split()çš„å±€é™:")
    print("âœ— æ— æ³•å¤„ç†ç¼©å†™")
    print("âœ— æ ‡ç‚¹ç²˜è¿åœ¨è¯ä¸Š")
    print("âœ— è¯æ•°ç»Ÿè®¡ä¸å‡†ç¡®")
    print("âœ— ä¸é€‚åˆç²¾ç¡®çš„æ–‡æœ¬åˆ†æ")
