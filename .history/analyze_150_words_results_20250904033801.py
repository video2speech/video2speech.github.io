#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer
import random

def analyze_150_words_results():
    """åˆ†æä½¿ç”¨ 150_words_list.txt ç­›é€‰çš„ç»“æœ"""
    
    print("=" * 80)
    print("ğŸ¯ 150è¯æ±‡è¡¨ç­›é€‰ç»“æœæ€»ç»“")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('filtered_sentences_150_words_clean.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å¥å­æ•°: {len(sentences):,} ä¸ªï¼ˆå·²å»é‡ï¼‰")
        print(f"   è¯æ±‡è¡¨: 150_words_list.txt (149ä¸ªè¯)")
        print(f"   é•¿åº¦è¦æ±‚: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        
        # ä½¿ç”¨åˆ†è¯å™¨åˆ†æå¥å­é•¿åº¦
        tokenizer = TreebankWordTokenizer()
        word_counts = []
        
        # åˆ†æå‰1000ä¸ªå¥å­ä»¥æé«˜é€Ÿåº¦
        sample_size = min(1000, len(sentences))
        sample_sentences = sentences[:sample_size]
        
        for sentence in sample_sentences:
            tokens = tokenizer.tokenize(sentence)
            # åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            words = [token for token in tokens if token not in ".,!?;:()\"'-"]
            word_counts.append(len(words))
        
        if word_counts:
            print(f"   å¹³å‡è¯æ•°: {sum(word_counts)/len(word_counts):.2f} è¯")
            print(f"   æœ€çŸ­å¥å­: {min(word_counts)} è¯")
            print(f"   æœ€é•¿å¥å­: {max(word_counts)} è¯")
        
        # é•¿åº¦åˆ†å¸ƒï¼ˆåŸºäºæ ·æœ¬ï¼‰
        if word_counts:
            dist_4_5 = sum(1 for c in word_counts if 4 <= c <= 5)
            dist_6_10 = sum(1 for c in word_counts if 6 <= c <= 10)
            dist_11_plus = sum(1 for c in word_counts if c > 10)
            
            print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒï¼ˆåŸºäºå‰{sample_size}ä¸ªå¥å­æ ·æœ¬ï¼‰:")
            print(f"   4-5è¯å¥å­: {dist_4_5} ({dist_4_5/len(word_counts)*100:.1f}%)")
            print(f"   6-10è¯å¥å­: {dist_6_10} ({dist_6_10/len(word_counts)*100:.1f}%)")
            print(f"   >10è¯å¥å­: {dist_11_plus} ({dist_11_plus/len(word_counts)*100:.1f}%)")
        
        # ä¸å…¶ä»–è¯æ±‡è¡¨å¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸å…¶ä»–è¯æ±‡è¡¨å¯¹æ¯”:")
        print(f"   50è¯è¯æ±‡è¡¨ (â‰¥4è¯): çº¦315ä¸ªå¥å­")
        print(f"   199è¯è¯æ±‡è¡¨ (â‰¥4è¯): 2,211ä¸ªå¥å­")
        print(f"   1,200è¯è¯æ±‡è¡¨ (â‰¥4è¯): 14,410ä¸ªå¥å­")
        print(f"   149è¯è¯æ±‡è¡¨ (â‰¥4è¯): {len(sentences):,}ä¸ªå¥å­")
        
        # æ˜¾ç¤ºä¸åŒç±»å‹çš„å¥å­æ ·æœ¬
        print(f"\nğŸ“ å¥å­æ ·æœ¬:")
        
        # 4-5è¯çŸ­å¥æ ·æœ¬
        short_sentences = [sentences[i] for i, c in enumerate(word_counts) if 4 <= c <= 5]
        if short_sentences:
            print(f"\n   4-5è¯çŸ­å¥æ ·æœ¬:")
            for sentence in short_sentences[:8]:
                print(f"   â€¢ {sentence}")
        
        # 6-10è¯ä¸­å¥æ ·æœ¬
        medium_sentences = [sentences[i] for i, c in enumerate(word_counts) if 6 <= c <= 10]
        if medium_sentences:
            print(f"\n   6-10è¯ä¸­å¥æ ·æœ¬:")
            for sentence in medium_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # é•¿å¥æ ·æœ¬
        long_sentences = [sentences[i] for i, c in enumerate(word_counts) if c > 10]
        if long_sentences:
            print(f"\n   >10è¯é•¿å¥æ ·æœ¬:")
            for sentence in long_sentences[:4]:
                print(f"   â€¢ {sentence}")
        
        # éšæœºå¥å­æ ·æœ¬
        print(f"\n   éšæœºå¥å­æ ·æœ¬:")
        random_samples = random.sample(sentences, min(10, len(sentences)))
        for sentence in random_samples:
            print(f"   â€¢ {sentence}")
        
        # ç­›é€‰æ¡ä»¶æ€»ç»“
        print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶æ€»ç»“:")
        print(f"   âœ… è¯æ±‡è¡¨: 150_words_list.txt (149ä¸ªé«˜é¢‘è¯æ±‡)")
        print(f"   âœ… åŸºäº: 200è¯æ±‡è¡¨ç§»é™¤50ä¸ªä½é¢‘è¯åçš„ç»“æœ")
        print(f"   âœ… é•¿åº¦é™åˆ¶: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        print(f"   âœ… è¯æ±‡é™åˆ¶: æ‰€æœ‰è¯æ±‡éƒ½å¿…é¡»åœ¨è¯æ±‡è¡¨ä¸­")
        print(f"   âœ… æ•°æ®æº: ç”µå½±å°è¯ (movie_lines.tsv)")
        print(f"   âœ… å¤„ç†è¡Œæ•°: 50,000è¡Œå¯¹è¯")
        print(f"   âœ… å·²å»é‡: ç§»é™¤äº†1,093ä¸ªé‡å¤å¥å­")
        
        print(f"\nğŸ’¡ è¿™{len(sentences):,}ä¸ªå¥å­çš„ç‰¹ç‚¹:")
        print(f"   â€¢ ä½¿ç”¨149ä¸ªç²¾é€‰é«˜é¢‘è¯æ±‡")
        print(f"   â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ4-19è¯ï¼‰")
        print(f"   â€¢ æ¥è‡ªçœŸå®çš„ç”µå½±å¯¹è¯")
        print(f"   â€¢ è¯æ±‡ç²¾ç®€ä½†è¡¨è¾¾å®Œæ•´")
        print(f"   â€¢ é€‚åˆä¸­é«˜çº§è‹±è¯­å­¦ä¹ ")
        
        # è´¨é‡è¯„ä¼°
        print(f"\nğŸ” è´¨é‡è¯„ä¼°:")
        print(f"   â€¢ æœ‰æ•ˆç‡: 4.13% (ç›¸å¯¹è¾ƒä½ï¼Œä½†è´¨é‡å¾ˆé«˜)")
        print(f"   â€¢ è¯æ±‡è¦†ç›–: 149ä¸ªæ ¸å¿ƒè¯æ±‡")
        print(f"   â€¢ è¡¨è¾¾å¤šæ ·æ€§: {len(sentences):,}ä¸ªä¸åŒå¥å­")
        print(f"   â€¢ å®ç”¨æ€§: é«˜é¢‘è¯æ±‡ä¿è¯æ—¥å¸¸äº¤æµè¦†ç›–")
        
        return sentences, word_counts
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°å¥å­æ–‡ä»¶")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    sentences, word_counts = analyze_150_words_results()
    
    if sentences:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶: filtered_sentences_150_words_clean.txt")
        print(f"ğŸ¯ {len(sentences):,} ä¸ªç²¾é€‰çš„é«˜è´¨é‡è‹±è¯­å¥å­å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"ğŸš€ è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡äº†è¯æ±‡ç²¾ç®€åº¦å’Œè¡¨è¾¾ä¸°å¯Œåº¦çš„ä¼˜è´¨æ•°æ®é›†ï¼")

if __name__ == "__main__":
    main()
