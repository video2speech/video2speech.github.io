#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer

def analyze_final_sentences():
    """åˆ†ææœ€ç»ˆç­›é€‰çš„å¥å­"""
    
    print("=" * 80)
    print("ğŸ¯ æœ€ç»ˆå¥å­ç­›é€‰ç»“æœæ€»ç»“")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('sentences_newtopwords_filtered.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å¥å­æ•°: {len(sentences)}")
        
        # ä½¿ç”¨åˆ†è¯å™¨åˆ†ææ¯ä¸ªå¥å­
        tokenizer = TreebankWordTokenizer()
        word_counts = []
        
        for sentence in sentences:
            tokens = tokenizer.tokenize(sentence)
            # åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            words = [token for token in tokens if token not in ".,!?;:()\"'-"]
            word_counts.append(len(words))
        
        print(f"   å¹³å‡è¯æ•°: {sum(word_counts)/len(word_counts):.2f} è¯")
        print(f"   æœ€çŸ­å¥å­: {min(word_counts)} è¯")
        print(f"   æœ€é•¿å¥å­: {max(word_counts)} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        dist_4 = sum(1 for c in word_counts if c == 4)
        dist_5 = sum(1 for c in word_counts if c == 5)
        dist_6_10 = sum(1 for c in word_counts if 6 <= c <= 10)
        
        print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒ:")
        print(f"   4è¯å¥å­: {dist_4} ({dist_4/len(sentences)*100:.1f}%)")
        print(f"   5è¯å¥å­: {dist_5} ({dist_5/len(sentences)*100:.1f}%)")
        print(f"   6-10è¯å¥å­: {dist_6_10} ({dist_6_10/len(sentences)*100:.1f}%)")
        
        # æ˜¾ç¤ºä¸åŒé•¿åº¦çš„å¥å­æ ·æœ¬
        print(f"\nğŸ“ å¥å­æ ·æœ¬:")
        
        # 4è¯å¥å­æ ·æœ¬
        four_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 4]
        print(f"\n   4è¯å¥å­æ ·æœ¬:")
        for sentence in four_word_sentences[:8]:
            print(f"   â€¢ {sentence}")
        
        # 5è¯å¥å­æ ·æœ¬
        five_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 5]
        print(f"\n   5è¯å¥å­æ ·æœ¬:")
        for sentence in five_word_sentences[:8]:
            print(f"   â€¢ {sentence}")
        
        # 6-10è¯å¥å­æ ·æœ¬
        longer_sentences = [sentences[i] for i, c in enumerate(word_counts) if 6 <= c <= 10]
        print(f"\n   6-10è¯å¥å­æ ·æœ¬:")
        for sentence in longer_sentences[:8]:
            print(f"   â€¢ {sentence}")
        
        # ç­›é€‰æ¡ä»¶æ€»ç»“
        print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶æ€»ç»“:")
        print(f"   âœ… è¯æ±‡è¡¨: newtopwords.txt (48ä¸ªæœ€é«˜é¢‘è¯)")
        print(f"   âœ… è¯æ±‡é™åˆ¶: æ‰€æœ‰è¯æ±‡éƒ½å¿…é¡»åœ¨è¯æ±‡è¡¨ä¸­")
        print(f"   âœ… é•¿åº¦é™åˆ¶: å¿…é¡»å¤§äº3è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        print(f"   âœ… æ•°æ®æº: ç”µå½±å°è¯ (movie_lines.tsv)")
        print(f"   âœ… å¤„ç†è¡Œæ•°: 20,000è¡Œå¯¹è¯")
        
        print(f"\nğŸ’¡ è¿™283ä¸ªå¥å­éƒ½æ˜¯:")
        print(f"   â€¢ ä½¿ç”¨æœ€æ ¸å¿ƒçš„48ä¸ªè‹±è¯­é«˜é¢‘è¯")
        print(f"   â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ4-10è¯ï¼‰")
        print(f"   â€¢ æ¥è‡ªçœŸå®çš„ç”µå½±å¯¹è¯")
        print(f"   â€¢ éå¸¸é€‚åˆè‹±è¯­å­¦ä¹ å’Œè¯­éŸ³è®­ç»ƒ")
        
        return sentences, word_counts
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°å¥å­æ–‡ä»¶")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    sentences, word_counts = analyze_final_sentences()
    
    if sentences:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶: sentences_newtopwords_filtered.txt")
        print(f"ğŸ¯ {len(sentences)} ä¸ªé«˜è´¨é‡çš„åŸºç¡€è‹±è¯­å¥å­å·²å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    main()
