#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer, sent_tokenize
import nltk
from tqdm import tqdm

def check_duplicates_in_merged():
    """æ£€æŸ¥ merged_vocabulary.txt ä¸­æ˜¯å¦æœ‰é‡å¤è¯æ±‡"""
    
    print("=" * 80)
    print("æ£€æŸ¥ merged_vocabulary.txt ä¸­çš„é‡å¤è¯æ±‡")
    print("=" * 80)
    
    try:
        with open('merged_vocabulary.txt', 'r', encoding='utf-8') as f:
            words = [word.strip().lower() for word in f.readlines() if word.strip()]
        
        print(f"åŸå§‹è¯æ±‡æ•°: {len(words)}")
        
        # æ£€æŸ¥é‡å¤
        seen = set()
        duplicates = []
        unique_words = []
        
        for word in words:
            if word in seen:
                duplicates.append(word)
            else:
                seen.add(word)
                unique_words.append(word)
        
        print(f"å”¯ä¸€è¯æ±‡æ•°: {len(unique_words)}")
        print(f"é‡å¤è¯æ±‡æ•°: {len(duplicates)}")
        
        if duplicates:
            print(f"\nğŸ”„ å‘ç°çš„é‡å¤è¯æ±‡:")
            for word in duplicates:
                print(f"   â€¢ {word}")
            
            # ä¿å­˜å»é‡åçš„è¯æ±‡è¡¨
            with open('merged_vocabulary.txt', 'w', encoding='utf-8') as f:
                for word in unique_words:
                    f.write(word + '\n')
            
            print(f"\nâœ… å·²å»é‡å¹¶æ›´æ–°æ–‡ä»¶")
            print(f"ğŸ“Š {len(words)} â†’ {len(unique_words)} ä¸ªè¯æ±‡")
        else:
            print(f"\nâœ… æ²¡æœ‰å‘ç°é‡å¤è¯æ±‡ï¼Œæ–‡ä»¶å·²ç»æ˜¯å¹²å‡€çš„")
        
        return unique_words
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ° merged_vocabulary.txt æ–‡ä»¶")
        return None

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨: {vocab_file}")
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        print(f"æˆåŠŸåŠ è½½ {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    except Exception as e:
        print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
        return set()

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;\(\)\"]+', ' ', text)  # ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    
    return text.strip()

def check_sentence_vocabulary(sentence, vocab, tokenizer):
    """æ£€æŸ¥å¥å­ä¸­çš„æ‰€æœ‰è¯æ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­"""
    if not sentence or len(sentence.strip()) < 3:
        return False, [], []
    
    # åˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    # åˆ†ç¦»è¯æ±‡å’Œæ ‡ç‚¹ç¬¦å·
    word_tokens = []
    punctuation_tokens = []
    invalid_tokens = []
    
    for token in tokens:
        # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
        if token in ".,!?;:()\"'-":
            punctuation_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆä¹Ÿç®—ä½œè¯æ±‡ï¼‰
        elif token.isdigit():
            word_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        elif token.lower() in vocab:
            word_tokens.append(token)
        else:
            invalid_tokens.append(token)
    
    # æ£€æŸ¥æ¡ä»¶ï¼š
    # 1. æ‰€æœ‰tokenéƒ½æœ‰æ•ˆï¼ˆæ— invalid_tokensï¼‰
    # 2. è¯æ±‡æ•°é‡â‰¥4ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    is_valid = len(invalid_tokens) == 0 and len(word_tokens) >= 4
    
    all_tokens = word_tokens + punctuation_tokens
    
    return is_valid, all_tokens, invalid_tokens

def process_movie_lines(movie_file, vocab_file, output_file, max_lines=None):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    
    print("=" * 80)
    print("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤å™¨ - ä½¿ç”¨ merged_vocabulary.txt")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab = load_vocabulary(vocab_file)
    if not vocab:
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # è¯»å–ç”µå½±å°è¯
    try:
        print(f"\næ­£åœ¨è¯»å–ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
        df = pd.read_csv(movie_file, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æˆåŠŸè¯»å– {len(df)} è¡Œå¯¹è¯")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåˆ™æˆªå–
        if max_lines and max_lines < len(df):
            df = df.head(max_lines)
            print(f"å¤„ç†å‰ {max_lines} è¡Œå¯¹è¯")
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸€è¡Œ
    valid_sentences = []
    invalid_sentences = []
    
    print(f"\nå¼€å§‹å¤„ç†å¯¹è¯...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
        text = clean_text(row['text'])
        
        if not text:
            continue
        
        # å°†æ¯è¡Œæ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = sent_tokenize(text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            # è·³è¿‡å¤ªçŸ­çš„å¥å­
            if len(sentence) < 5:
                continue
            
            # æ£€æŸ¥å¥å­è¯æ±‡ï¼ˆâ‰¥4è¯ï¼‰
            is_valid, valid_tokens, invalid_tokens = check_sentence_vocabulary(sentence, vocab, tokenizer)
            
            if is_valid:
                # è®¡ç®—å®é™…è¯æ±‡æ•°é‡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
                word_count = len([token for token in valid_tokens if token not in ".,!?;:()\"'-"])
                
                valid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'tokens': valid_tokens,
                    'token_count': len(valid_tokens),
                    'word_count': word_count  # å®é™…è¯æ±‡æ•°é‡
                })
            else:
                invalid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'invalid_tokens': invalid_tokens,
                    'all_tokens': valid_tokens + invalid_tokens
                })
    
    # å»é‡å¤„ç†
    print(f"\nå»é‡å‰å¥å­æ•°: {len(valid_sentences)}")
    unique_sentences = []
    seen_sentences = set()
    
    for item in valid_sentences:
        sentence_lower = item['sentence'].lower()
        if sentence_lower not in seen_sentences:
            seen_sentences.add(sentence_lower)
            unique_sentences.append(item)
    
    print(f"å»é‡åå¥å­æ•°: {len(unique_sentences)}")
    print(f"é‡å¤å¥å­æ•°: {len(valid_sentences) - len(unique_sentences)}")
    
    # ä¿å­˜ç»“æœ
    print(f"\nå¤„ç†å®Œæˆï¼")
    print(f"æœ‰æ•ˆå¥å­: {len(unique_sentences)} (å·²å»é‡)")
    print(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}")
    print(f"æœ‰æ•ˆç‡: {len(unique_sentences)/(len(unique_sentences)+len(invalid_sentences))*100:.2f}%")
    
    # æŒ‰å¥å­é•¿åº¦åˆ†æï¼ˆä½¿ç”¨å®é™…è¯æ±‡æ•°é‡ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    if unique_sentences:
        word_lengths = [item['word_count'] for item in unique_sentences]
        print(f"\nå¥å­é•¿åº¦ç»Ÿè®¡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰:")
        print(f"  æœ€çŸ­: {min(word_lengths)} è¯")
        print(f"  æœ€é•¿: {max(word_lengths)} è¯")
        print(f"  å¹³å‡: {sum(word_lengths)/len(word_lengths):.2f} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        short = sum(1 for l in word_lengths if 4 <= l <= 5)
        medium = sum(1 for l in word_lengths if 6 <= l <= 10)
        long = sum(1 for l in word_lengths if l > 10)
        
        print(f"  çŸ­å¥ (4-5è¯): {short} ({short/len(word_lengths)*100:.1f}%)")
        print(f"  ä¸­å¥ (6-10è¯): {medium} ({medium/len(word_lengths)*100:.1f}%)")
        print(f"  é•¿å¥ (>10è¯): {long} ({long/len(word_lengths)*100:.1f}%)")
    
    # ä¿å­˜æœ‰æ•ˆå¥å­
    print(f"\næ­£åœ¨ä¿å­˜æœ‰æ•ˆå¥å­åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ç”µå½±å°è¯ - ä»…åŒ…å« merged_vocabulary.txt è¯æ±‡è¡¨çš„å¥å­ (â‰¥4è¯ï¼Œå·²å»é‡)\n")
        f.write(f"# æ€»å…± {len(unique_sentences)} ä¸ªæœ‰æ•ˆå¥å­\n")
        f.write(f"# è¯æ±‡è¡¨: {vocab_file} ({len(vocab)} ä¸ªè¯)\n")
        f.write(f"# é™åˆ¶æ¡ä»¶: æ‰€æœ‰è¯æ±‡éƒ½åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä¸”è¯æ±‡æ•°é‡â‰¥4ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        for i, item in enumerate(unique_sentences, 1):
            f.write(f"{i}. {item['sentence']}\n")
    
    # ç”Ÿæˆçº¯å¥å­æ–‡ä»¶ï¼ˆæ— åºå·ï¼‰
    clean_output_file = output_file.replace('.txt', '_clean.txt')
    with open(clean_output_file, 'w', encoding='utf-8') as f:
        for item in unique_sentences:
            f.write(item['sentence'] + '\n')
    
    print(f"æ­£åœ¨ä¿å­˜çº¯å¥å­æ–‡ä»¶åˆ°: {clean_output_file}")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ {output_file} - æœ‰ç¼–å·çš„å¥å­")
    print(f"   ğŸ“„ {clean_output_file} - çº¯å¥å­ï¼ˆæ— ç¼–å·ï¼‰")
    
    return len(unique_sentences), len(invalid_sentences)

def main():
    """ä¸»å‡½æ•°"""
    # é¦–å…ˆæ£€æŸ¥å¹¶å»é‡è¯æ±‡è¡¨
    print("ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥è¯æ±‡è¡¨é‡å¤...")
    unique_words = check_duplicates_in_merged()
    
    if not unique_words:
        return
    
    print(f"\nç¬¬äºŒæ­¥ï¼šä½¿ç”¨è¯æ±‡è¡¨ç­›é€‰å¥å­...")
    
    movie_file = "others/movie_lines.tsv"
    vocab_file = "merged_vocabulary.txt"
    output_file = "filtered_sentences_merged_vocab.txt"
    max_lines = 50000  # å¤„ç†æ›´å¤šè¡Œä»¥è·å¾—æ›´å¤šæœ‰æ•ˆå¥å­
    
    # è¿è¡Œå¤„ç†
    process_movie_lines(movie_file, vocab_file, output_file, max_lines)

if __name__ == "__main__":
    main()
