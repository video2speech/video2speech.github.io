#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer, sent_tokenize
import nltk
from tqdm import tqdm
from collections import Counter

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨: {vocab_file}")
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        print(f"æˆåŠŸåŠ è½½ {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    except Exception as e:
        print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
        return set()

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;\(\)\"]+', ' ', text)  # ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    
    return text.strip()

def check_sentence_vocabulary(sentence, vocab, tokenizer):
    """æ£€æŸ¥å¥å­ä¸­çš„æ‰€æœ‰è¯æ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­"""
    if not sentence or len(sentence.strip()) < 3:
        return False, [], []
    
    # åˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    # åˆ†ç¦»è¯æ±‡å’Œæ ‡ç‚¹ç¬¦å·
    word_tokens = []
    punctuation_tokens = []
    invalid_tokens = []
    
    for token in tokens:
        # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
        if token in ".,!?;:()\"'-":
            punctuation_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆä¹Ÿç®—ä½œè¯æ±‡ï¼‰
        elif token.isdigit():
            word_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        elif token.lower() in vocab:
            word_tokens.append(token)
        else:
            invalid_tokens.append(token)
    
    # æ£€æŸ¥æ¡ä»¶ï¼š
    # 1. æ‰€æœ‰tokenéƒ½æœ‰æ•ˆï¼ˆæ— invalid_tokensï¼‰
    # 2. è¯æ±‡æ•°é‡â‰¥4ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    is_valid = len(invalid_tokens) == 0 and len(word_tokens) >= 4
    
    all_tokens = word_tokens + punctuation_tokens
    
    return is_valid, all_tokens, invalid_tokens

def analyze_word_frequency_in_filtered_sentences(sentences, tokenizer):
    """åˆ†æç­›é€‰åå¥å­ä¸­çš„è¯é¢‘"""
    
    print(f"\nå¼€å§‹åˆ†æç­›é€‰åå¥å­çš„è¯é¢‘...")
    
    # ç»Ÿè®¡æ‰€æœ‰è¯æ±‡
    all_words = []
    word_counter = Counter()
    
    for sentence_info in sentences:
        sentence = sentence_info['sentence']
        # åˆ†è¯
        tokens = tokenizer.tokenize(sentence)
        
        # å¤„ç†æ¯ä¸ªè¯æ±‡
        for token in tokens:
            # è·³è¿‡æ ‡ç‚¹ç¬¦å·
            if token in ".,!?;:()\"'-":
                continue
            
            # è½¬æ¢ä¸ºå°å†™è¿›è¡Œç»Ÿè®¡
            word_lower = token.lower()
            all_words.append(word_lower)
            word_counter[word_lower] += 1
    
    return all_words, word_counter

def process_movie_lines(movie_file, vocab_file, output_file, max_lines=None):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    
    print("=" * 80)
    print("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤å™¨ - ä½¿ç”¨ 150_words_list.txt")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab = load_vocabulary(vocab_file)
    if not vocab:
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # è¯»å–ç”µå½±å°è¯
    try:
        print(f"\næ­£åœ¨è¯»å–ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
        df = pd.read_csv(movie_file, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æˆåŠŸè¯»å– {len(df)} è¡Œå¯¹è¯")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåˆ™æˆªå–
        if max_lines and max_lines < len(df):
            df = df.head(max_lines)
            print(f"å¤„ç†å‰ {max_lines} è¡Œå¯¹è¯")
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸€è¡Œ
    valid_sentences = []
    invalid_sentences = []
    
    print(f"\nå¼€å§‹å¤„ç†å¯¹è¯...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
        text = clean_text(row['text'])
        
        if not text:
            continue
        
        # å°†æ¯è¡Œæ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = sent_tokenize(text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            # è·³è¿‡å¤ªçŸ­çš„å¥å­
            if len(sentence) < 5:
                continue
            
            # æ£€æŸ¥å¥å­è¯æ±‡ï¼ˆâ‰¥4è¯ï¼‰
            is_valid, valid_tokens, invalid_tokens = check_sentence_vocabulary(sentence, vocab, tokenizer)
            
            if is_valid:
                # è®¡ç®—å®é™…è¯æ±‡æ•°é‡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
                word_count = len([token for token in valid_tokens if token not in ".,!?;:()\"'-"])
                
                valid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'tokens': valid_tokens,
                    'token_count': len(valid_tokens),
                    'word_count': word_count  # å®é™…è¯æ±‡æ•°é‡
                })
            else:
                invalid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'invalid_tokens': invalid_tokens,
                    'all_tokens': valid_tokens + invalid_tokens
                })
    
    # å»é‡å¤„ç†
    print(f"\nå»é‡å‰å¥å­æ•°: {len(valid_sentences)}")
    unique_sentences = []
    seen_sentences = set()
    
    for item in valid_sentences:
        sentence_lower = item['sentence'].lower()
        if sentence_lower not in seen_sentences:
            seen_sentences.add(sentence_lower)
            unique_sentences.append(item)
    
    print(f"å»é‡åå¥å­æ•°: {len(unique_sentences)}")
    print(f"é‡å¤å¥å­æ•°: {len(valid_sentences) - len(unique_sentences)}")
    
    # ä¿å­˜ç»“æœ
    print(f"\nå¤„ç†å®Œæˆï¼")
    print(f"æœ‰æ•ˆå¥å­: {len(unique_sentences)} (å·²å»é‡)")
    print(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}")
    print(f"æœ‰æ•ˆç‡: {len(unique_sentences)/(len(unique_sentences)+len(invalid_sentences))*100:.2f}%")
    
    # æŒ‰å¥å­é•¿åº¦åˆ†æï¼ˆä½¿ç”¨å®é™…è¯æ±‡æ•°é‡ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    if unique_sentences:
        word_lengths = [item['word_count'] for item in unique_sentences]
        print(f"\nå¥å­é•¿åº¦ç»Ÿè®¡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰:")
        print(f"  æœ€çŸ­: {min(word_lengths)} è¯")
        print(f"  æœ€é•¿: {max(word_lengths)} è¯")
        print(f"  å¹³å‡: {sum(word_lengths)/len(word_lengths):.2f} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        short = sum(1 for l in word_lengths if 4 <= l <= 5)
        medium = sum(1 for l in word_lengths if 6 <= l <= 10)
        long = sum(1 for l in word_lengths if l > 10)
        
        print(f"  çŸ­å¥ (4-5è¯): {short} ({short/len(word_lengths)*100:.1f}%)")
        print(f"  ä¸­å¥ (6-10è¯): {medium} ({medium/len(word_lengths)*100:.1f}%)")
        print(f"  é•¿å¥ (>10è¯): {long} ({long/len(word_lengths)*100:.1f}%)")
    
    # åˆ†æç­›é€‰åå¥å­çš„è¯é¢‘
    all_words, word_counter = analyze_word_frequency_in_filtered_sentences(unique_sentences, tokenizer)
    
    print(f"\nğŸ“Š ç­›é€‰åå¥å­çš„è¯é¢‘ç»Ÿè®¡:")
    print(f"  æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰")
    print(f"  å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª")
    
    # æŒ‰é¢‘ç‡æ’åº
    sorted_words = word_counter.most_common()
    
    if sorted_words:
        print(f"  æœ€é«˜é¢‘è¯: '{sorted_words[0][0]}' å‡ºç° {sorted_words[0][1]} æ¬¡")
        print(f"  æœ€ä½é¢‘è¯: '{sorted_words[-1][0]}' å‡ºç° {sorted_words[-1][1]} æ¬¡")
        
        # æ˜¾ç¤ºå‰20ä¸ªé«˜é¢‘è¯
        print(f"\nğŸ” å‰20ä¸ªé«˜é¢‘è¯:")
        print("-" * 50)
        print(f"{'æ’å':<4} {'è¯æ±‡':<15} {'é¢‘ç‡':<6} {'å æ¯”'}")
        print("-" * 50)
        
        for i, (word, freq) in enumerate(sorted_words[:20], 1):
            percentage = freq / len(all_words) * 100
            print(f"{i:>3}. {word:<15} {freq:>5} {percentage:>6.2f}%")
    
    # ä¿å­˜æœ‰æ•ˆå¥å­
    print(f"\næ­£åœ¨ä¿å­˜æœ‰æ•ˆå¥å­åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ç”µå½±å°è¯ - ä»…åŒ…å« 150_words_list.txt è¯æ±‡è¡¨çš„å¥å­ (â‰¥4è¯ï¼Œå·²å»é‡)\n")
        f.write(f"# æ€»å…± {len(unique_sentences)} ä¸ªæœ‰æ•ˆå¥å­\n")
        f.write(f"# è¯æ±‡è¡¨: {vocab_file} ({len(vocab)} ä¸ªè¯)\n")
        f.write(f"# é™åˆ¶æ¡ä»¶: æ‰€æœ‰è¯æ±‡éƒ½åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä¸”è¯æ±‡æ•°é‡â‰¥4ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        for i, item in enumerate(unique_sentences, 1):
            f.write(f"{i}. {item['sentence']}\n")
    
    # ç”Ÿæˆçº¯å¥å­æ–‡ä»¶ï¼ˆæ— åºå·ï¼‰
    clean_output_file = output_file.replace('.txt', '_clean.txt')
    with open(clean_output_file, 'w', encoding='utf-8') as f:
        for item in unique_sentences:
            f.write(item['sentence'] + '\n')
    
    print(f"æ­£åœ¨ä¿å­˜çº¯å¥å­æ–‡ä»¶åˆ°: {clean_output_file}")
    
    # ä¿å­˜è¯é¢‘åˆ†æç»“æœ
    freq_file = output_file.replace('.txt', '_frequency.txt')
    with open(freq_file, 'w', encoding='utf-8') as f:
        f.write(f"ç­›é€‰åå¥å­è¯é¢‘åˆ†æç»“æœ\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"å¥å­æ•°: {len(unique_sentences)}\n")
        f.write(f"æ€»è¯æ±‡æ•°: {len(all_words)} ä¸ªï¼ˆåŒ…æ‹¬é‡å¤ï¼‰\n")
        f.write(f"å”¯ä¸€è¯æ±‡æ•°: {len(word_counter)} ä¸ª\n\n")
        
        f.write("å®Œæ•´è¯é¢‘åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡é™åºæ’åˆ—ï¼‰:\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'æ’å':<6} {'è¯æ±‡':<20} {'é¢‘ç‡':<8} {'å æ¯”'}\n")
        f.write("-" * 60 + "\n")
        
        for i, (word, freq) in enumerate(sorted_words, 1):
            percentage = freq / len(all_words) * 100
            f.write(f"{i:>5}. {word:<20} {freq:>7} {percentage:>7.2f}%\n")
    
    # ä¿å­˜CSVæ ¼å¼
    csv_file = output_file.replace('.txt', '_frequency.csv')
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write("æ’å,è¯æ±‡,é¢‘ç‡,å æ¯”\n")
        for i, (word, freq) in enumerate(sorted_words, 1):
            percentage = freq / len(all_words) * 100
            f.write(f"{i},{word},{freq},{percentage:.2f}%\n")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ {output_file} - æœ‰ç¼–å·çš„å¥å­")
    print(f"   ğŸ“„ {clean_output_file} - çº¯å¥å­ï¼ˆæ— ç¼–å·ï¼‰")
    print(f"   ğŸ“Š {freq_file} - è¯é¢‘åˆ†ææŠ¥å‘Š")
    print(f"   ğŸ“Š {csv_file} - è¯é¢‘CSVæ•°æ®")
    
    return len(unique_sentences), len(invalid_sentences), sorted_words

def main():
    """ä¸»å‡½æ•°"""
    movie_file = "others/movie_lines.tsv"
    vocab_file = "materials/150_words_list.txt"
    output_file = "filtered_sentences_150_words_v2.txt"  # ä½¿ç”¨v2ç‰ˆæœ¬é¿å…è¦†ç›–
    max_lines = None  # å¤„ç†æ‰€æœ‰è¡Œ
    
    print("ğŸ¯ ç­›é€‰æ¡ä»¶:")
    print("   1. å¥å­é•¿åº¦ â‰¥ 4 ä¸ªè¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
    print("   2. æ‰€æœ‰è¯æ±‡éƒ½å¿…é¡»åœ¨ 150_words_list.txt ä¸­")
    print("   3. è‡ªåŠ¨å»é‡å¤„ç†")
    print()
    
    # è¿è¡Œå¤„ç†
    result = process_movie_lines(movie_file, vocab_file, output_file, max_lines)
    
    if result:
        valid_count, invalid_count, sorted_words = result
        print(f"\nğŸ¯ æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æœ‰æ•ˆå¥å­: {valid_count:,} ä¸ª")
        print(f"   æ— æ•ˆå¥å­: {invalid_count:,} ä¸ª")
        print(f"   è¯é¢‘åˆ†æ: {len(sorted_words)} ä¸ªä¸åŒè¯æ±‡")
        print(f"   æœ‰æ•ˆç‡: {valid_count/(valid_count+invalid_count)*100:.2f}%")

if __name__ == "__main__":
    main()