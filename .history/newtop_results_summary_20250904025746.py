#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from nltk.tokenize import TreebankWordTokenizer

def analyze_newtop_results():
    """åˆ†æä½¿ç”¨ newtop.txt ç­›é€‰çš„ç»“æœ"""
    
    print("=" * 80)
    print("ğŸ¯ newtop.txt ç­›é€‰ç»“æœæ€»ç»“")
    print("=" * 80)
    
    # è¯»å–å¥å­æ–‡ä»¶
    try:
        with open('sentences_newtop_filtered.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å¥å­æ•°: {len(sentences)} ä¸ªï¼ˆå·²å»é‡ï¼‰")
        print(f"   è¯æ±‡è¡¨: newtop.txt (199ä¸ªè¯)")
        print(f"   é•¿åº¦è¦æ±‚: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        
        # ä½¿ç”¨åˆ†è¯å™¨åˆ†ææ¯ä¸ªå¥å­
        tokenizer = TreebankWordTokenizer()
        word_counts = []
        
        for sentence in sentences:
            tokens = tokenizer.tokenize(sentence)
            # åªè®¡ç®—éæ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            words = [token for token in tokens if token not in ".,!?;:()\"'-"]
            word_counts.append(len(words))
        
        print(f"   å¹³å‡è¯æ•°: {sum(word_counts)/len(word_counts):.2f} è¯")
        print(f"   æœ€çŸ­å¥å­: {min(word_counts)} è¯")
        print(f"   æœ€é•¿å¥å­: {max(word_counts)} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        dist_4 = sum(1 for c in word_counts if c == 4)
        dist_5 = sum(1 for c in word_counts if c == 5)
        dist_6_10 = sum(1 for c in word_counts if 6 <= c <= 10)
        dist_11_plus = sum(1 for c in word_counts if c > 10)
        
        print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒ:")
        print(f"   4è¯å¥å­: {dist_4} ({dist_4/len(sentences)*100:.1f}%)")
        print(f"   5è¯å¥å­: {dist_5} ({dist_5/len(sentences)*100:.1f}%)")
        print(f"   6-10è¯å¥å­: {dist_6_10} ({dist_6_10/len(sentences)*100:.1f}%)")
        print(f"   >10è¯å¥å­: {dist_11_plus} ({dist_11_plus/len(sentences)*100:.1f}%)")
        
        # ä¸ä¹‹å‰ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸å…¶ä»–è¯æ±‡è¡¨å¯¹æ¯”:")
        print(f"   85è¯è¯æ±‡è¡¨ (â‰¥4è¯): 315ä¸ªå¥å­")
        print(f"   199è¯è¯æ±‡è¡¨ (â‰¥4è¯): {len(sentences)}ä¸ªå¥å­")
        print(f"   å¢é•¿: {len(sentences) - 315} ä¸ªå¥å­ ({(len(sentences) - 315)/315*100:.1f}%)")
        
        # æ˜¾ç¤ºä¸åŒé•¿åº¦çš„å¥å­æ ·æœ¬
        print(f"\nğŸ“ å¥å­æ ·æœ¬:")
        
        # 4è¯å¥å­æ ·æœ¬
        four_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 4]
        if four_word_sentences:
            print(f"\n   4è¯å¥å­æ ·æœ¬:")
            for sentence in four_word_sentences[:8]:
                print(f"   â€¢ {sentence}")
        
        # 5è¯å¥å­æ ·æœ¬
        five_word_sentences = [sentences[i] for i, c in enumerate(word_counts) if c == 5]
        if five_word_sentences:
            print(f"\n   5è¯å¥å­æ ·æœ¬:")
            for sentence in five_word_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # 6-10è¯å¥å­æ ·æœ¬
        medium_sentences = [sentences[i] for i, c in enumerate(word_counts) if 6 <= c <= 10]
        if medium_sentences:
            print(f"\n   6-10è¯å¥å­æ ·æœ¬:")
            for sentence in medium_sentences[:6]:
                print(f"   â€¢ {sentence}")
        
        # é•¿å¥æ ·æœ¬
        long_sentences = [sentences[i] for i, c in enumerate(word_counts) if c > 10]
        if long_sentences:
            print(f"\n   >10è¯å¥å­æ ·æœ¬:")
            for sentence in long_sentences[:4]:
                print(f"   â€¢ {sentence}")
        
        # ç­›é€‰æ¡ä»¶æ€»ç»“
        print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶æ€»ç»“:")
        print(f"   âœ… è¯æ±‡è¡¨: newtop.txt (199ä¸ªç²¾é€‰è¯æ±‡)")
        print(f"   âœ… é•¿åº¦é™åˆ¶: â‰¥4è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰")
        print(f"   âœ… è¯æ±‡é™åˆ¶: æ‰€æœ‰è¯æ±‡éƒ½å¿…é¡»åœ¨è¯æ±‡è¡¨ä¸­")
        print(f"   âœ… æ•°æ®æº: ç”µå½±å°è¯ (movie_lines.tsv)")
        print(f"   âœ… å¤„ç†è¡Œæ•°: 30,000è¡Œå¯¹è¯")
        print(f"   âœ… å·²å»é‡: ç§»é™¤äº†é‡å¤å¥å­")
        
        print(f"\nğŸ’¡ è¿™{len(sentences)}ä¸ªå¥å­çš„ç‰¹ç‚¹:")
        print(f"   â€¢ ä½¿ç”¨199ä¸ªç²¾é€‰è¯æ±‡ï¼ˆæ‰©å±•è¯æ±‡è¡¨ï¼‰")
        print(f"   â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ4-19è¯ï¼‰")
        print(f"   â€¢ æ¥è‡ªçœŸå®çš„ç”µå½±å¯¹è¯")
        print(f"   â€¢ è¯æ±‡æ›´ä¸°å¯Œï¼Œè¡¨è¾¾æ›´å¤šæ ·")
        print(f"   â€¢ é€‚åˆä¸­çº§è‹±è¯­å­¦ä¹ ")
        
        return sentences, word_counts
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°å¥å­æ–‡ä»¶")
        return None, None

def main():
    """ä¸»å‡½æ•°"""
    sentences, word_counts = analyze_newtop_results()
    
    if sentences:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶: sentences_newtop_filtered.txt")
        print(f"ğŸ¯ {len(sentences)} ä¸ªé«˜è´¨é‡çš„è‹±è¯­å¥å­å·²å‡†å¤‡å°±ç»ªï¼")
        print(f"ğŸš€ ç›¸æ¯”85è¯è¯æ±‡è¡¨å¢åŠ äº† {len(sentences) - 315} ä¸ªå¥å­ï¼")

if __name__ == "__main__":
    main()
