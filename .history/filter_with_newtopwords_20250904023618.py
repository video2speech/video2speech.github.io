#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer, sent_tokenize
import nltk
from tqdm import tqdm

def load_vocabulary(vocab_file):
    """åŠ è½½è¯æ±‡è¡¨"""
    print(f"æ­£åœ¨åŠ è½½è¯æ±‡è¡¨: {vocab_file}")
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            vocab = set(word.strip().lower() for word in f.readlines() if word.strip())
        
        print(f"æˆåŠŸåŠ è½½ {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    except Exception as e:
        print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
        return set()

def download_nltk_data():
    """ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt')
    
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        print("æ­£åœ¨ä¸‹è½½NLTK punkt_tabæ•°æ®...")
        nltk.download('punkt_tab')

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'[^\w\s\'\-\.\,\?\!\:\;\(\)\"]+', ' ', text)  # ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    
    return text.strip()

def is_valid_token(token, vocab):
    """æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆï¼ˆåœ¨è¯æ±‡è¡¨ä¸­æˆ–æ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰"""
    token_lower = token.lower()
    
    # æ ‡ç‚¹ç¬¦å·æ€»æ˜¯æœ‰æ•ˆçš„
    if token in ".,!?;:()\"'-":
        return True
    
    # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
    if token_lower in vocab:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆæ•°å­—ä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼‰
    if token.isdigit():
        return True
    
    return False

def check_sentence_vocabulary(sentence, vocab, tokenizer):
    """æ£€æŸ¥å¥å­ä¸­çš„æ‰€æœ‰è¯æ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­"""
    if not sentence or len(sentence.strip()) < 3:
        return False, [], []
    
    # åˆ†è¯
    tokens = tokenizer.tokenize(sentence)
    
    # åˆ†ç¦»è¯æ±‡å’Œæ ‡ç‚¹ç¬¦å·
    word_tokens = []
    punctuation_tokens = []
    invalid_tokens = []
    
    for token in tokens:
        # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
        if token in ".,!?;:()\"'-":
            punctuation_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆä¹Ÿç®—ä½œè¯æ±‡ï¼‰
        elif token.isdigit():
            word_tokens.append(token)
        # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        elif token.lower() in vocab:
            word_tokens.append(token)
        else:
            invalid_tokens.append(token)
    
    # æ£€æŸ¥æ¡ä»¶ï¼š
    # 1. æ‰€æœ‰tokenéƒ½æœ‰æ•ˆï¼ˆæ— invalid_tokensï¼‰
    # 2. è¯æ±‡æ•°é‡å¤§äºæˆ–ç­‰äº3ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    is_valid = len(invalid_tokens) == 0 and len(word_tokens) >= 3
    
    all_tokens = word_tokens + punctuation_tokens
    
    return is_valid, all_tokens, invalid_tokens

def process_movie_lines(movie_file, vocab_file, output_file, max_lines=None):
    """å¤„ç†ç”µå½±å°è¯æ–‡ä»¶"""
    
    print("=" * 80)
    print("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤å™¨ - ä½¿ç”¨æ–°è¯æ±‡è¡¨")
    print("=" * 80)
    
    # ä¸‹è½½NLTKæ•°æ®
    download_nltk_data()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab = load_vocabulary(vocab_file)
    if not vocab:
        return
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = TreebankWordTokenizer()
    
    # è¯»å–ç”µå½±å°è¯
    try:
        print(f"\næ­£åœ¨è¯»å–ç”µå½±å°è¯æ–‡ä»¶: {movie_file}")
        df = pd.read_csv(movie_file, sep='\t', header=None, 
                        names=['line_id', 'character_id', 'movie_id', 'character_name', 'text'],
                        encoding='utf-8', on_bad_lines='skip')
        
        print(f"æˆåŠŸè¯»å– {len(df)} è¡Œå¯¹è¯")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåˆ™æˆªå–
        if max_lines and max_lines < len(df):
            df = df.head(max_lines)
            print(f"å¤„ç†å‰ {max_lines} è¡Œå¯¹è¯")
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # å¤„ç†æ¯ä¸€è¡Œ
    valid_sentences = []
    invalid_sentences = []
    
    print(f"\nå¼€å§‹å¤„ç†å¯¹è¯...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è¿›åº¦"):
        text = clean_text(row['text'])
        
        if not text:
            continue
        
        # å°†æ¯è¡Œæ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = sent_tokenize(text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            # è·³è¿‡å¤ªçŸ­çš„å¥å­
            if len(sentence) < 5:
                continue
            
            # æ£€æŸ¥å¥å­è¯æ±‡ï¼ˆæ— é•¿åº¦é™åˆ¶ï¼Œä½†å¿…é¡»>3è¯ï¼‰
            is_valid, valid_tokens, invalid_tokens = check_sentence_vocabulary(sentence, vocab, tokenizer)
            
            if is_valid:
                # è®¡ç®—å®é™…è¯æ±‡æ•°é‡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
                word_count = len([token for token in valid_tokens if token not in ".,!?;:()\"'-"])
                
                valid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'tokens': valid_tokens,
                    'token_count': len(valid_tokens),
                    'word_count': word_count  # å®é™…è¯æ±‡æ•°é‡
                })
            else:
                invalid_sentences.append({
                    'line_id': row['line_id'],
                    'character_name': row['character_name'],
                    'sentence': sentence,
                    'invalid_tokens': invalid_tokens,
                    'all_tokens': valid_tokens + invalid_tokens
                })
    
    # ä¿å­˜ç»“æœ
    print(f"\nå¤„ç†å®Œæˆï¼")
    print(f"æœ‰æ•ˆå¥å­: {len(valid_sentences)}")
    print(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}")
    print(f"æœ‰æ•ˆç‡: {len(valid_sentences)/(len(valid_sentences)+len(invalid_sentences))*100:.2f}%")
    
    # æŒ‰å¥å­é•¿åº¦åˆ†æï¼ˆä½¿ç”¨å®é™…è¯æ±‡æ•°é‡ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
    if valid_sentences:
        word_lengths = [item['word_count'] for item in valid_sentences]
        print(f"\nå¥å­é•¿åº¦ç»Ÿè®¡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰:")
        print(f"  æœ€çŸ­: {min(word_lengths)} è¯")
        print(f"  æœ€é•¿: {max(word_lengths)} è¯")
        print(f"  å¹³å‡: {sum(word_lengths)/len(word_lengths):.2f} è¯")
        
        # é•¿åº¦åˆ†å¸ƒ
        short = sum(1 for l in word_lengths if l <= 5)
        medium = sum(1 for l in word_lengths if 6 <= l <= 10)
        long = sum(1 for l in word_lengths if l > 10)
        
        print(f"  çŸ­å¥ (3-5è¯): {short} ({short/len(word_lengths)*100:.1f}%)")
        print(f"  ä¸­å¥ (6-10è¯): {medium} ({medium/len(word_lengths)*100:.1f}%)")
        print(f"  é•¿å¥ (>10è¯): {long} ({long/len(word_lengths)*100:.1f}%)")
    
    # ä¿å­˜æœ‰æ•ˆå¥å­
    print(f"\næ­£åœ¨ä¿å­˜æœ‰æ•ˆå¥å­åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ç”µå½±å°è¯ - ä»…åŒ…å«æ‰©å±•è¯æ±‡è¡¨çš„å¥å­ (â‰¥3è¯ï¼Œä¸å«æ ‡ç‚¹)\n")
        f.write(f"# æ€»å…± {len(valid_sentences)} ä¸ªæœ‰æ•ˆå¥å­\n")
        f.write(f"# è¯æ±‡è¡¨: {vocab_file} ({len(vocab)} ä¸ªè¯)\n")
        f.write(f"# é™åˆ¶æ¡ä»¶: æ‰€æœ‰è¯æ±‡éƒ½åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä¸”è¯æ±‡æ•°é‡â‰¥3ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        for i, item in enumerate(valid_sentences, 1):
            f.write(f"{i}. {item['sentence']}\n")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = output_file.replace('.txt', '_stats.txt')
    print(f"æ­£åœ¨ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°: {stats_file}")
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("ç”µå½±å°è¯è¯æ±‡è¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š - æ–°è¯æ±‡è¡¨\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æºæ–‡ä»¶: {movie_file}\n")
        f.write(f"è¯æ±‡è¡¨: {vocab_file}\n")
        f.write(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)} ä¸ªè¯\n")
        f.write(f"é•¿åº¦é™åˆ¶: å¿…é¡»â‰¥3è¯ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰\n\n")
        f.write(f"å¤„ç†è¡Œæ•°: {len(df)}\n")
        f.write(f"æœ‰æ•ˆå¥å­: {len(valid_sentences)}\n")
        f.write(f"æ— æ•ˆå¥å­: {len(invalid_sentences)}\n")
        f.write(f"æœ‰æ•ˆç‡: {len(valid_sentences)/(len(valid_sentences)+len(invalid_sentences))*100:.2f}%\n\n")
        
        # å¥å­é•¿åº¦ç»Ÿè®¡ï¼ˆä½¿ç”¨å®é™…è¯æ±‡æ•°é‡ï¼‰
        if valid_sentences:
            word_lengths = [item['word_count'] for item in valid_sentences]
            f.write("æœ‰æ•ˆå¥å­é•¿åº¦ç»Ÿè®¡ï¼ˆä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰:\n")
            f.write(f"  å¹³å‡é•¿åº¦: {sum(word_lengths)/len(word_lengths):.2f} è¯\n")
            f.write(f"  æœ€çŸ­å¥å­: {min(word_lengths)} è¯\n")
            f.write(f"  æœ€é•¿å¥å­: {max(word_lengths)} è¯\n\n")
            
            # é•¿åº¦åˆ†å¸ƒ
            short = sum(1 for l in word_lengths if l <= 5)
            medium = sum(1 for l in word_lengths if 6 <= l <= 10)
            long = sum(1 for l in word_lengths if l > 10)
            
            f.write("é•¿åº¦åˆ†å¸ƒ:\n")
            f.write(f"  çŸ­å¥ (3-5è¯): {short} ({short/len(word_lengths)*100:.1f}%)\n")
            f.write(f"  ä¸­å¥ (6-10è¯): {medium} ({medium/len(word_lengths)*100:.1f}%)\n")
            f.write(f"  é•¿å¥ (>10è¯): {long} ({long/len(word_lengths)*100:.1f}%)\n\n")
            
            # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
            f.write("æœ‰æ•ˆå¥å­ç¤ºä¾‹:\n")
            for i, item in enumerate(valid_sentences[:15], 1):
                f.write(f"{i:2d}. ({item['word_count']:2d}è¯) {item['sentence'][:80]}{'...' if len(item['sentence']) > 80 else ''}\n")
        
        f.write("\næ— æ•ˆå¥å­ç¤ºä¾‹ (åŒ…å«è¯æ±‡è¡¨å¤–çš„è¯):\n")
        for i, item in enumerate(invalid_sentences[:10], 1):
            f.write(f"{i:2d}. {item['sentence'][:60]}{'...' if len(item['sentence']) > 60 else ''}\n")
            f.write(f"    æ— æ•ˆè¯: {item['invalid_tokens'][:5]}\n")
    
    # ä¿å­˜æ— æ•ˆå¥å­æ ·æœ¬
    invalid_sample_file = output_file.replace('.txt', '_invalid_samples.txt')
    print(f"æ­£åœ¨ä¿å­˜æ— æ•ˆå¥å­æ ·æœ¬åˆ°: {invalid_sample_file}")
    
    with open(invalid_sample_file, 'w', encoding='utf-8') as f:
        f.write(f"æ— æ•ˆå¥å­æ ·æœ¬ (å‰300ä¸ª)\n")
        f.write("=" * 50 + "\n\n")
        
        for i, item in enumerate(invalid_sentences[:300], 1):
            f.write(f"{i}. {item['sentence']}\n")
            f.write(f"   æ— æ•ˆè¯: {', '.join(item['invalid_tokens'][:10])}\n")
            f.write(f"   æ‰€æœ‰è¯: {', '.join(item['all_tokens'][:15])}\n\n")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“„ {output_file} - æœ‰æ•ˆå¥å­")
    print(f"   ğŸ“Š {stats_file} - ç»Ÿè®¡æŠ¥å‘Š") 
    print(f"   ğŸ” {invalid_sample_file} - æ— æ•ˆå¥å­æ ·æœ¬")
    
    return len(valid_sentences), len(invalid_sentences)

def main():
    """ä¸»å‡½æ•°"""
    movie_file = "others/movie_lines.tsv"
    vocab_file = "others/newtopwords.txt"
    output_file = "filtered_sentences_newtopwords.txt"
    max_lines = 20000  # å¤„ç†æ›´å¤šè¡Œä»¥è·å¾—æ›´å¤šæœ‰æ•ˆå¥å­
    
    # è¿è¡Œå¤„ç†
    process_movie_lines(movie_file, vocab_file, output_file, max_lines)

if __name__ == "__main__":
    main()
