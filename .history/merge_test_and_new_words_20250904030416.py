#!/usr/bin/env python3
# -*- coding: utf-8 -*-

def merge_vocabularies():
    """åˆå¹¶ test.txt å’Œæ–°å¢è¯æ±‡ï¼Œå»é‡å¹¶ä¿æŒåŸæ¥çš„é¡ºåº"""
    
    print("=" * 80)
    print("åˆå¹¶è¯æ±‡è¡¨ - test.txt + æ–°å¢50è¯")
    print("=" * 80)
    
    # è¯»å– test.txt ä¸­çš„è¯æ±‡
    try:
        with open('test.txt', 'r', encoding='utf-8') as f:
            test_words = [word.strip().lower() for word in f.readlines() if word.strip()]
        
        print(f"test.txt è¯æ±‡æ•°: {len(test_words)} ä¸ª")
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ° test.txt")
        return
    
    # æ–°å¢çš„è¯æ±‡ï¼ˆè½¬æ¢ä¸ºå°å†™ï¼‰
    new_words_raw = [
        "Am", "Are", "Bad", "Bring", "Clean", "Closer", "Comfortable", "Coming", "Computer", "Do",
        "Faith", "Family", "Feel", "Glasses", "Going", "Good", "Goodbye", "Have", "Hello", "Help",
        "Here", "Hope", "How", "Hungry", "I", "Is", "It", "Like", "Music", "My",
        "Need", "No", "Not", "Nurse", "Okay", "Outside", "Please", "Right", "Success", "Tell",
        "That", "They", "Thirsty", "Tired", "Up", "Very", "What", "Where", "Yes", "You"
    ]
    
    new_words = [word.lower() for word in new_words_raw]
    
    print(f"æ–°å¢è¯æ±‡: {len(new_words)} ä¸ª")
    
    # åˆå¹¶è¯æ±‡è¡¨ï¼Œä¿æŒåŸæ¥çš„é¡ºåºï¼Œå»é‡
    merged_words = []
    seen = set()
    
    # é¦–å…ˆæ·»åŠ  test.txt çš„è¯æ±‡
    for word in test_words:
        if word not in seen:
            merged_words.append(word)
            seen.add(word)
    
    # ç„¶åæ·»åŠ æ–°è¯æ±‡ï¼ˆå¦‚æœä¸é‡å¤çš„è¯ï¼‰
    new_added = []
    for word in new_words:
        if word not in seen:
            merged_words.append(word)
            seen.add(word)
            new_added.append(word)
    
    print(f"å®é™…æ–°å¢: {len(new_added)} ä¸ªè¯")
    print(f"é‡å¤è¯æ±‡: {len(new_words) - len(new_added)} ä¸ª")
    print(f"åˆå¹¶åæ€»æ•°: {len(merged_words)} ä¸ªè¯")
    
    # æ˜¾ç¤ºæ–°å¢çš„è¯æ±‡
    if new_added:
        print(f"\nğŸ†• æ–°å¢çš„è¯æ±‡:")
        for i, word in enumerate(new_added, 1):
            print(f"{i:2d}. {word}")
    
    # æ˜¾ç¤ºé‡å¤çš„è¯æ±‡
    duplicates = [word for word in new_words if word in test_words]
    if duplicates:
        print(f"\nğŸ”„ é‡å¤çš„è¯æ±‡ (å·²å­˜åœ¨äºtest.txtä¸­):")
        for i, word in enumerate(duplicates, 1):
            print(f"{i:2d}. {word}")
    
    # ä¿å­˜åˆå¹¶åçš„è¯æ±‡è¡¨
    output_file = 'merged_vocabulary.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        for word in merged_words:
            f.write(word + '\n')
    
    print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“„ å·²ä¿å­˜: {output_file}")
    print(f"ğŸ“Š åŸå§‹test.txt: {len(test_words)} â†’ åˆå¹¶å: {len(merged_words)} ä¸ªè¯")
    
    # æ˜¾ç¤ºåˆå¹¶åçš„å‰20ä¸ªè¯
    print(f"\nğŸ“ åˆå¹¶åè¯æ±‡è¡¨å‰20ä¸ªè¯:")
    for i, word in enumerate(merged_words[:20], 1):
        print(f"{i:2d}. {word}")
    
    if len(merged_words) > 20:
        print(f"... è¿˜æœ‰ {len(merged_words) - 20} ä¸ªè¯")
    
    # æ˜¾ç¤ºæœ€å20ä¸ªè¯ï¼ˆä¸»è¦æ˜¯æ–°å¢çš„ï¼‰
    if len(merged_words) > 20:
        print(f"\nğŸ“ æœ€å20ä¸ªè¯ï¼ˆä¸»è¦æ˜¯æ–°å¢è¯æ±‡ï¼‰:")
        start_idx = max(0, len(merged_words) - 20)
        for i, word in enumerate(merged_words[start_idx:], start_idx + 1):
            print(f"{i:2d}. {word}")
    
    return merged_words

def main():
    """ä¸»å‡½æ•°"""
    merged_words = merge_vocabularies()
    
    if merged_words:
        print(f"\nğŸ¯ è¯æ±‡è¡¨å·²æˆåŠŸåˆå¹¶ï¼")
        print(f"ğŸ“„ æ–‡ä»¶: merged_vocabulary.txt")
        print(f"ğŸ”¢ æ€»è¯æ±‡æ•°: {len(merged_words)} ä¸ª")

if __name__ == "__main__":
    main()
