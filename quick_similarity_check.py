#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
from difflib import SequenceMatcher
from itertools import combinations
import re

def load_sentences():
    """åŠ è½½å¥å­æ•°æ®"""
    sentences_file = "selected_sentences_analyzer.json"
    if os.path.exists(sentences_file):
        try:
            with open(sentences_file, 'r', encoding='utf-8') as f:
                sentences = json.load(f)
            print(f"ğŸ“ åŠ è½½äº† {len(sentences)} ä¸ªå¥å­")
            return sentences
        except:
            print("âŒ åŠ è½½å¥å­æ–‡ä»¶å¤±è´¥")
            return []
    else:
        print("âŒ æœªæ‰¾åˆ° selected_sentences_analyzer.json æ–‡ä»¶")
        return []

def clean_for_comparison(sentence):
    """æ¸…ç†å¥å­ç”¨äºæ¯”è¾ƒ"""
    sentence = sentence.lower()
    sentence = re.sub(r'[^\w\s]', '', sentence)
    sentence = re.sub(r'\s+', ' ', sentence).strip()
    return sentence

def calculate_similarity(sent1, sent2):
    """è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦"""
    clean1 = clean_for_comparison(sent1)
    clean2 = clean_for_comparison(sent2)
    
    # å­—ç¬¦åºåˆ—ç›¸ä¼¼åº¦
    char_sim = SequenceMatcher(None, clean1, clean2).ratio()
    
    # è¯æ±‡é‡å ç›¸ä¼¼åº¦
    words1 = set(clean1.split())
    words2 = set(clean2.split())
    
    if not words1 and not words2:
        word_sim = 1.0
    elif not words1 or not words2:
        word_sim = 0.0
    else:
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        word_sim = intersection / union if union > 0 else 0.0
    
    # ç»¼åˆç›¸ä¼¼åº¦
    combined_sim = (char_sim + word_sim) / 2
    return char_sim, word_sim, combined_sim

def main():
    sentences = load_sentences()
    
    if len(sentences) < 2:
        print("âŒ éœ€è¦è‡³å°‘2ä¸ªå¥å­æ‰èƒ½è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ")
        return
    
    print(f"\nğŸ” åˆ†æ {len(sentences)} ä¸ªå¥å­çš„ç›¸ä¼¼åº¦...")
    total_pairs = len(sentences) * (len(sentences) - 1) // 2
    print(f"æ€»å…±éœ€è¦æ¯”è¾ƒ {total_pairs} å¯¹å¥å­")
    
    similarities = []
    
    # è®¡ç®—æ‰€æœ‰å¥å­å¯¹çš„ç›¸ä¼¼åº¦
    for i, j in combinations(range(len(sentences)), 2):
        sent1 = sentences[i]
        sent2 = sentences[j]
        
        char_sim, word_sim, combined_sim = calculate_similarity(sent1, sent2)
        
        similarities.append({
            'index1': i + 1,
            'index2': j + 1,
            'sentence1': sent1,
            'sentence2': sent2,
            'char_similarity': char_sim,
            'word_similarity': word_sim,
            'combined_similarity': combined_sim
        })
    
    # æŒ‰ç»¼åˆç›¸ä¼¼åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
    similarities.sort(key=lambda x: x['combined_similarity'], reverse=True)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 100)
    print("ğŸ“Š å¥å­ç›¸ä¼¼åº¦åˆ†æç»“æœ (æŒ‰ç›¸ä¼¼åº¦æ’åº)")
    print("=" * 100)
    
    # æ˜¾ç¤ºå‰20ä¸ªæœ€ç›¸ä¼¼çš„å¥å­å¯¹
    print(f"\nğŸ” å‰ 20 ä¸ªæœ€ç›¸ä¼¼çš„å¥å­å¯¹:")
    print("-" * 100)
    
    for i, sim in enumerate(similarities[:20], 1):
        print(f"\n#{i:2d} ç›¸ä¼¼åº¦: {sim['combined_similarity']:.3f} (å­—ç¬¦: {sim['char_similarity']:.3f}, è¯æ±‡: {sim['word_similarity']:.3f})")
        print(f"    å¥å­ {sim['index1']:2d}: {sim['sentence1']}")
        print(f"    å¥å­ {sim['index2']:2d}: {sim['sentence2']}")
    
    # æŸ¥æ‰¾é«˜åº¦ç›¸ä¼¼çš„å¥å­ï¼ˆå¯èƒ½æ˜¯é‡å¤ï¼‰
    print(f"\nğŸ” é«˜åº¦ç›¸ä¼¼çš„å¥å­ (ç›¸ä¼¼åº¦ â‰¥ 0.8):")
    print("-" * 80)
    
    high_similarity = [s for s in similarities if s['combined_similarity'] >= 0.8]
    if high_similarity:
        for i, sim in enumerate(high_similarity, 1):
            print(f"\n#{i} ç›¸ä¼¼åº¦: {sim['combined_similarity']:.3f}")
            print(f"   å¥å­ {sim['index1']}: {sim['sentence1']}")
            print(f"   å¥å­ {sim['index2']}: {sim['sentence2']}")
    else:
        print("   æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„å¥å­")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æ€»å¥å­å¯¹æ•°: {len(similarities)}")
    high_sim = sum(1 for s in similarities if s['combined_similarity'] > 0.8)
    medium_sim = sum(1 for s in similarities if 0.5 < s['combined_similarity'] <= 0.8)
    low_sim = sum(1 for s in similarities if s['combined_similarity'] <= 0.5)
    
    print(f"   é«˜ç›¸ä¼¼åº¦ (>0.8): {high_sim} å¯¹ ({high_sim/len(similarities)*100:.1f}%)")
    print(f"   ä¸­ç›¸ä¼¼åº¦ (0.5-0.8): {medium_sim} å¯¹ ({medium_sim/len(similarities)*100:.1f}%)")
    print(f"   ä½ç›¸ä¼¼åº¦ (â‰¤0.5): {low_sim} å¯¹ ({low_sim/len(similarities)*100:.1f}%)")
    
    avg_sim = sum(s['combined_similarity'] for s in similarities) / len(similarities)
    print(f"   å¹³å‡ç›¸ä¼¼åº¦: {avg_sim:.3f}")
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶...")
    with open("sentence_similarity_report.txt", 'w', encoding='utf-8') as f:
        f.write("å¥å­ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æ€»å¥å­æ•°: {len(sentences)}\n")
        f.write(f"æ€»å¥å­å¯¹æ•°: {len(similarities)}\n\n")
        
        f.write("æ‰€æœ‰å¥å­å¯¹çš„ç›¸ä¼¼åº¦ (æŒ‰ç›¸ä¼¼åº¦æ’åº):\n")
        f.write("-" * 50 + "\n")
        
        for i, sim in enumerate(similarities, 1):
            f.write(f"\n#{i:3d} ç›¸ä¼¼åº¦: {sim['combined_similarity']:.3f} ")
            f.write(f"(å­—ç¬¦: {sim['char_similarity']:.3f}, è¯æ±‡: {sim['word_similarity']:.3f})\n")
            f.write(f"      å¥å­ {sim['index1']:2d}: {sim['sentence1']}\n")
            f.write(f"      å¥å­ {sim['index2']:2d}: {sim['sentence2']}\n")
    
    print("âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° sentence_similarity_report.txt")

if __name__ == "__main__":
    main()


