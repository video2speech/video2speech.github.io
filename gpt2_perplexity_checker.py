#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import torch
import numpy as np
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import warnings
warnings.filterwarnings("ignore")

class GPT2PerplexityChecker:
    def __init__(self, model_name="gpt2"):
        """
        åˆå§‹åŒ–GPT-2å›°æƒ‘åº¦æ£€æŸ¥å™¨
        å¯é€‰æ¨¡å‹:
        - "gpt2" (117M parameters, å¿«é€Ÿ)
        - "gpt2-medium" (345M parameters, ä¸­ç­‰)
        - "gpt2-large" (774M parameters, å¤§ä½†å‡†ç¡®)
        - "gpt2-xl" (1.5B parameters, æœ€å¤§æœ€å‡†ç¡®ä½†å¾ˆæ…¢)
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½GPT-2æ¨¡å‹: {model_name}")
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            # åŠ è½½tokenizerå’Œæ¨¡å‹
            print("   ğŸ“¥ åŠ è½½tokenizer...")
            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
            
            print("   ğŸ“¥ åŠ è½½æ¨¡å‹...")
            self.model = GPT2LMHeadModel.from_pretrained(model_name)
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®è¯„ä¼°æ¨¡å¼
            self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… GPT-2æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å®‰è£…äº†å¿…è¦çš„ä¾èµ–:")
            print("   pip install transformers torch")
            print("   å¦‚æœä½¿ç”¨GPU: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            raise
    
    def load_sentences(self, filename="selected_sentences_analyzer.json"):
        """åŠ è½½å¥å­æ•°æ®"""
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    sentences = json.load(f)
                print(f"ğŸ“ åŠ è½½äº† {len(sentences)} ä¸ªå¥å­")
                return sentences
            except Exception as e:
                print(f"âŒ åŠ è½½å¥å­æ–‡ä»¶å¤±è´¥: {e}")
                return []
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {filename}")
            return []
    
    def calculate_perplexity_single(self, text, max_length=512):
        """è®¡ç®—å•ä¸ªå¥å­çš„å›°æƒ‘åº¦"""
        try:
            # ç¼–ç æ–‡æœ¬
            encodings = self.tokenizer(
                text, 
                return_tensors='pt', 
                truncation=True, 
                max_length=max_length,
                padding=False
            )
            
            input_ids = encodings.input_ids.to(self.device)
            
            # ç¡®ä¿è¾“å…¥ä¸ä¸ºç©º
            if input_ids.size(1) == 0:
                return float('inf'), float('inf')
            
            # è®¡ç®—å›°æƒ‘åº¦
            with torch.no_grad():
                # ä½¿ç”¨input_idsä½œä¸ºlabelsæ¥è®¡ç®—loss
                outputs = self.model(input_ids, labels=input_ids)
                loss = outputs.loss
                
                # å›°æƒ‘åº¦ = exp(loss)
                perplexity = torch.exp(loss).item()
                
                # å¦‚æœlossæ˜¯nanæˆ–infï¼Œè¿”å›æ— ç©·å¤§
                if not torch.isfinite(loss):
                    return float('inf'), float('inf')
            
            return perplexity, loss.item()
            
        except Exception as e:
            print(f"âŒ è®¡ç®—å›°æƒ‘åº¦å¤±è´¥: {text[:50]}... - {e}")
            return float('inf'), float('inf')
    
    def calculate_perplexity_sliding_window(self, text, stride=512, max_length=1024):
        """ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—é•¿å¥å­çš„å›°æƒ‘åº¦"""
        try:
            encodings = self.tokenizer(text, return_tensors='pt')
            input_ids = encodings.input_ids.to(self.device)
            
            seq_len = input_ids.size(1)
            
            # å¦‚æœå¥å­çŸ­äºmax_lengthï¼Œç›´æ¥è®¡ç®—
            if seq_len <= max_length:
                return self.calculate_perplexity_single(text, max_length)
            
            # ä½¿ç”¨æ»‘åŠ¨çª—å£
            nlls = []
            prev_end_loc = 0
            
            for begin_loc in range(0, seq_len, stride):
                end_loc = min(begin_loc + max_length, seq_len)
                trg_len = end_loc - prev_end_loc
                
                input_ids_window = input_ids[:, begin_loc:end_loc]
                target_ids = input_ids_window.clone()
                target_ids[:, :-trg_len] = -100
                
                with torch.no_grad():
                    outputs = self.model(input_ids_window, labels=target_ids)
                    neg_log_likelihood = outputs.loss * trg_len
                
                nlls.append(neg_log_likelihood)
                prev_end_loc = end_loc
                
                if end_loc == seq_len:
                    break
            
            # è®¡ç®—å¹³å‡å›°æƒ‘åº¦
            total_nll = torch.stack(nlls).sum()
            perplexity = torch.exp(total_nll / seq_len).item()
            avg_loss = (total_nll / seq_len).item()
            
            return perplexity, avg_loss
            
        except Exception as e:
            print(f"âŒ æ»‘åŠ¨çª—å£è®¡ç®—å¤±è´¥: {text[:50]}... - {e}")
            return float('inf'), float('inf')
    
    def batch_calculate_perplexity(self, sentences, batch_size=4, use_sliding_window=False):
        """æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦"""
        results = []
        total = len(sentences)
        
        print(f"\nğŸ”„ å¼€å§‹ä½¿ç”¨GPT-2è®¡ç®— {total} ä¸ªå¥å­çš„å›°æƒ‘åº¦...")
        print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"ğŸªŸ æ»‘åŠ¨çª—å£: {'å¯ç”¨' if use_sliding_window else 'ç¦ç”¨'}")
        
        for i in range(0, total, batch_size):
            batch = sentences[i:i+batch_size]
            batch_results = []
            
            for j, sentence in enumerate(batch):
                current_idx = i + j + 1
                print(f"â³ å¤„ç†ä¸­ ({current_idx}/{total}): {sentence[:60]}{'...' if len(sentence) > 60 else ''}")
                
                # é€‰æ‹©è®¡ç®—æ–¹æ³•
                if use_sliding_window:
                    perplexity, loss = self.calculate_perplexity_sliding_window(sentence)
                else:
                    perplexity, loss = self.calculate_perplexity_single(sentence)
                
                # è®¡ç®—ä¸€äº›é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
                tokens = self.tokenizer.encode(sentence)
                token_count = len(tokens)
                word_count = len(sentence.split())
                
                batch_results.append({
                    'index': current_idx,
                    'sentence': sentence,
                    'perplexity': perplexity,
                    'loss': loss,
                    'token_count': token_count,
                    'word_count': word_count,
                    'avg_perplexity_per_token': perplexity / token_count if token_count > 0 else float('inf')
                })
            
            results.extend(batch_results)
            
            # æ˜¾ç¤ºè¿›åº¦å’Œå½“å‰æ‰¹æ¬¡ç»Ÿè®¡
            progress = (i + len(batch)) / total * 100
            batch_perplexities = [r['perplexity'] for r in batch_results if r['perplexity'] != float('inf')]
            if batch_perplexities:
                batch_avg = sum(batch_perplexities) / len(batch_perplexities)
                print(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({i + len(batch)}/{total}) | å½“å‰æ‰¹æ¬¡å¹³å‡å›°æƒ‘åº¦: {batch_avg:.2f}")
            else:
                print(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({i + len(batch)}/{total})")
            
            # æ¸…ç†GPUç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return results
    
    def analyze_results(self, results):
        """åˆ†æGPT-2å›°æƒ‘åº¦ç»“æœ"""
        if not results:
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        # è¿‡æ»¤æ‰æ— æ•ˆç»“æœ
        valid_results = [r for r in results if r['perplexity'] != float('inf')]
        
        if not valid_results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›°æƒ‘åº¦è®¡ç®—ç»“æœ")
            return
        
        perplexities = [r['perplexity'] for r in valid_results]
        
        print("\n" + "=" * 90)
        print(f"ğŸ“Š GPT-2 å›°æƒ‘åº¦åˆ†æç»“æœ (æ¨¡å‹: {self.model_name})")
        print("=" * 90)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»å¥å­æ•°: {len(results)}")
        print(f"   æœ‰æ•ˆè®¡ç®—: {len(valid_results)}")
        print(f"   å¹³å‡å›°æƒ‘åº¦: {np.mean(perplexities):.2f}")
        print(f"   ä¸­ä½æ•°å›°æƒ‘åº¦: {np.median(perplexities):.2f}")
        print(f"   æœ€ä½å›°æƒ‘åº¦: {np.min(perplexities):.2f}")
        print(f"   æœ€é«˜å›°æƒ‘åº¦: {np.max(perplexities):.2f}")
        print(f"   æ ‡å‡†å·®: {np.std(perplexities):.2f}")
        
        # å›°æƒ‘åº¦åˆ†å¸ƒ
        low_perp = sum(1 for p in perplexities if p < 20)
        med_perp = sum(1 for p in perplexities if 20 <= p < 50)
        high_perp = sum(1 for p in perplexities if p >= 50)
        
        print(f"\nğŸ“Š å›°æƒ‘åº¦åˆ†å¸ƒ:")
        print(f"   ä½å›°æƒ‘åº¦ (<20): {low_perp} å¥ ({low_perp/len(valid_results)*100:.1f}%)")
        print(f"   ä¸­å›°æƒ‘åº¦ (20-50): {med_perp} å¥ ({med_perp/len(valid_results)*100:.1f}%)")
        print(f"   é«˜å›°æƒ‘åº¦ (â‰¥50): {high_perp} å¥ ({high_perp/len(valid_results)*100:.1f}%)")
        
        # æŒ‰å›°æƒ‘åº¦æ’åº
        sorted_results = sorted(valid_results, key=lambda x: x['perplexity'])
        
        # æ˜¾ç¤ºæœ€è‡ªç„¶çš„å¥å­ï¼ˆä½å›°æƒ‘åº¦ï¼‰
        print(f"\nğŸŒŸ æœ€è‡ªç„¶çš„å¥å­ (ä½å›°æƒ‘åº¦):")
        print("-" * 80)
        for i, result in enumerate(sorted_results[:15], 1):
            print(f"{i:2d}. å›°æƒ‘åº¦: {result['perplexity']:7.2f} | è¯æ•°: {result['word_count']:2d} | {result['sentence']}")
        
        # æ˜¾ç¤ºæœ€ä¸è‡ªç„¶çš„å¥å­ï¼ˆé«˜å›°æƒ‘åº¦ï¼‰
        print(f"\nâš ï¸  æœ€ä¸è‡ªç„¶çš„å¥å­ (é«˜å›°æƒ‘åº¦):")
        print("-" * 80)
        for i, result in enumerate(sorted_results[-15:], 1):
            print(f"{i:2d}. å›°æƒ‘åº¦: {result['perplexity']:7.2f} | è¯æ•°: {result['word_count']:2d} | {result['sentence']}")
        
        return sorted_results
    
    def save_results(self, results, filename="gpt2_perplexity_report.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜JSONæ ¼å¼
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å¯è¯»æ–‡æœ¬æ ¼å¼
            txt_filename = filename.replace('.json', '.txt')
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write("GPT-2 å¥å­å›°æƒ‘åº¦åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"æ¨¡å‹: {self.model_name}\n")
                f.write(f"æ€»å¥å­æ•°: {len(results)}\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                valid_results = [r for r in results if r['perplexity'] != float('inf')]
                if valid_results:
                    perplexities = [r['perplexity'] for r in valid_results]
                    f.write(f"æœ‰æ•ˆè®¡ç®—: {len(valid_results)}\n")
                    f.write(f"å¹³å‡å›°æƒ‘åº¦: {np.mean(perplexities):.2f}\n")
                    f.write(f"ä¸­ä½æ•°å›°æƒ‘åº¦: {np.median(perplexities):.2f}\n")
                    f.write(f"æœ€ä½å›°æƒ‘åº¦: {np.min(perplexities):.2f}\n")
                    f.write(f"æœ€é«˜å›°æƒ‘åº¦: {np.max(perplexities):.2f}\n")
                f.write("\n")
                
                # æŒ‰å›°æƒ‘åº¦æ’åºçš„è¯¦ç»†ç»“æœ
                sorted_results = sorted([r for r in results if r['perplexity'] != float('inf')], 
                                      key=lambda x: x['perplexity'])
                
                f.write("æ‰€æœ‰å¥å­çš„å›°æƒ‘åº¦ (æŒ‰å›°æƒ‘åº¦æ’åº):\n")
                f.write("-" * 60 + "\n")
                
                for result in sorted_results:
                    f.write(f"\nå›°æƒ‘åº¦: {result['perplexity']:7.2f} | è¯æ•°: {result['word_count']:2d} | Tokenæ•°: {result['token_count']:3d}\n")
                    f.write(f"å¥å­: {result['sentence']}\n")
            
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°:")
            print(f"   ğŸ“„ {filename} (JSONæ ¼å¼)")
            print(f"   ğŸ“„ {txt_filename} (æ–‡æœ¬æ ¼å¼)")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    print("ğŸ¯ GPT-2 å¥å­å›°æƒ‘åº¦æ£€æŸ¥å™¨")
    print("=" * 60)
    
    # é€‰æ‹©æ¨¡å‹
    models = {
        "1": "gpt2",
        "2": "gpt2-medium", 
        "3": "gpt2-large",
        "4": "gpt2-xl"
    }
    
    print("ğŸ“š å¯ç”¨çš„GPT-2æ¨¡å‹:")
    print("   1. GPT-2 (117Må‚æ•°, å¿«é€Ÿ)")
    print("   2. GPT-2 Medium (345Må‚æ•°, ä¸­ç­‰)")
    print("   3. GPT-2 Large (774Må‚æ•°, å‡†ç¡®ä½†æ…¢)")
    print("   4. GPT-2 XL (1.5Bå‚æ•°, æœ€å‡†ç¡®ä½†å¾ˆæ…¢)")
    
    choice = input("\né€‰æ‹©æ¨¡å‹ (1-4, é»˜è®¤1): ").strip() or "1"
    model_name = models.get(choice, "gpt2")
    
    try:
        # åˆå§‹åŒ–æ£€æŸ¥å™¨
        checker = GPT2PerplexityChecker(model_name)
        
        # åŠ è½½å¥å­
        sentences = checker.load_sentences()
        if not sentences:
            return
        
        # è®¾ç½®å‚æ•°
        batch_size = int(input(f"\næ‰¹å¤„ç†å¤§å° (é»˜è®¤2): ").strip() or "2")
        
        use_sliding_window = input("æ˜¯å¦ä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†é•¿å¥å­? (y/N): ").strip().lower() in ['y', 'yes']
        
        # æ‰¹é‡è®¡ç®—å›°æƒ‘åº¦
        results = checker.batch_calculate_perplexity(
            sentences, 
            batch_size=batch_size,
            use_sliding_window=use_sliding_window
        )
        
        # åˆ†æç»“æœ
        sorted_results = checker.analyze_results(results)
        
        # ä¿å­˜ç»“æœ
        if sorted_results:
            save_choice = input("\nğŸ’¾ æ˜¯å¦ä¿å­˜ç»“æœ? (y/N): ").strip().lower()
            if save_choice in ['y', 'yes']:
                filename = f"gpt2_{model_name.replace('-', '_')}_perplexity_report.json"
                checker.save_results(results, filename)
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰")
        print("   2. å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
        print("   3. å‡å°æ‰¹å¤„ç†å¤§å°")
        print("   4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜/æ˜¾å­˜")

if __name__ == "__main__":
    main()


