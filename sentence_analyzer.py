#!/usr/bin/env python3
"""
äº¤äº’å¼å¥å­åˆ†æå™¨ - ç»Ÿè®¡è¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡
Interactive Sentence Analyzer - Word and Phoneme Frequency Analysis
"""

import nltk
import re
import json
import os
import string
from collections import Counter

# Download required NLTK data
try:
    nltk.data.find('corpora/cmudict')
except LookupError:
    nltk.download('cmudict')

from nltk.corpus import cmudict

class SentenceAnalyzer:
    def __init__(self):
        self.cmu_dict = cmudict.dict()
        self.selected_sentences = []
        self.save_file = "selected_sentences_analyzer.json"
        
        # æ ‡å‡†39ä¸ªè‹±è¯­éŸ³ç´  (ARPAbetæ ¼å¼)
        self.standard_phonemes = {
            # å…ƒéŸ³ (15ä¸ª)
            'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW',
            # è¾…éŸ³ (24ä¸ª)
            'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH'
        }
        
        # åŠ è½½ç›®æ ‡è¯æ±‡é›†
        self.pick_words = self.load_pick_words()
        
        self.load_selected_sentences()
    
    def load_pick_words(self):
        """åŠ è½½selected_words.jsonä¸­çš„è¯æ±‡"""
        pick_words = set()
        try:
            with open('/Users/terry/Downloads/video2speech.github.io/video2speech.github.io/selected_words.json', 'r', encoding='utf-8') as f:
                words_list = json.load(f)
                for word in words_list:
                    if word.strip():
                        pick_words.add(word.strip().lower())
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ° selected_words.json æ–‡ä»¶")
        except json.JSONDecodeError:
            print("âŒ selected_words.json æ–‡ä»¶æ ¼å¼é”™è¯¯")
        return pick_words
    
    def clean_phoneme(self, phoneme):
        """ç§»é™¤éŸ³ç´ çš„é‡éŸ³æ ‡è®°"""
        return re.sub(r'\d', '', phoneme)
    
    def get_word_phonemes(self, word):
        """è·å–è¯æ±‡å¯¹åº”çš„éŸ³ç´ """
        # å¤„ç†ç¼©å†™
        word_clean = word.lower().replace("'", "")
        
        contraction_map = {
            "nt": "not", "s": "is", "ve": "have", "re": "are", 
            "ll": "will", "d": "would", "m": "am", "em": "them"
        }
        
        if word.startswith("'") and len(word) > 1:
            contraction_part = word[1:].lower()
            if contraction_part in contraction_map:
                word_clean = contraction_map[contraction_part]
        
        # å°è¯•ä¸åŒçš„è¯æ±‡å˜å½¢
        for test_word in [word_clean, word.lower(), re.sub(r'[^\w]', '', word.lower())]:
            if test_word in self.cmu_dict:
                phonemes = self.cmu_dict[test_word][0]
                cleaned_phonemes = [self.clean_phoneme(p) for p in phonemes]
                return [p for p in cleaned_phonemes if p in self.standard_phonemes]
        
        return []
    
    def extract_words_from_sentence(self, sentence):
        """ä»å¥å­ä¸­æå–å•è¯"""
        # å¤„ç†ç¼©å†™
        sentence = sentence.replace("'t", " not")
        sentence = sentence.replace("'re", " are")  
        sentence = sentence.replace("'ll", " will")
        sentence = sentence.replace("'ve", " have")
        sentence = sentence.replace("'d", " would")
        sentence = sentence.replace("'m", " am")
        sentence = sentence.replace("'s", " is")
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        translator = str.maketrans('', '', string.punctuation)
        sentence = sentence.translate(translator)
        
        # åˆ†å‰²å•è¯å¹¶è½¬æ¢ä¸ºå°å†™
        words = sentence.lower().split()
        return words
    
    def save_selected_sentences(self):
        """ä¿å­˜é€‰ä¸­çš„å¥å­åˆ°æ–‡ä»¶"""
        with open(self.save_file, 'w', encoding='utf-8') as f:
            json.dump(self.selected_sentences, f, indent=2, ensure_ascii=False)
    
    def load_selected_sentences(self):
        """ä»æ–‡ä»¶åŠ è½½ä¹‹å‰é€‰ä¸­çš„å¥å­"""
        if os.path.exists(self.save_file):
            try:
                with open(self.save_file, 'r', encoding='utf-8') as f:
                    self.selected_sentences = json.load(f)
                if self.selected_sentences:
                    print(f"ğŸ“ åŠ è½½äº† {len(self.selected_sentences)} ä¸ªä¹‹å‰é€‰ä¸­çš„å¥å­")
            except:
                self.selected_sentences = []
        else:
            self.selected_sentences = []
    
    def clean_sentence(self, sentence):
        """æ¸…ç†å¥å­ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™åŸºæœ¬ç»“æ„ï¼Œè¿˜åŸå¤§å°å†™"""
        # å»é™¤å¤šä½™ç©ºæ ¼
        sentence = sentence.strip()
        
        # ä¿ç•™ç¼©å†™ä¸­çš„æ’‡å·ï¼Œä½†å»é™¤å…¶ä»–æ ‡ç‚¹ç¬¦å·
        # å…ˆå¤„ç†å¸¸è§ç¼©å†™
        sentence = re.sub(r'\b(don|won|can|isn|aren|wasn|weren|hasn|haven|hadn|shouldn|wouldn|couldn|didn)\'t\b', r'\1 not', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\b(I|you|we|they)\'re\b', r'\1 are', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\b(I|you|we|they|he|she|it)\'ll\b', r'\1 will', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\b(I|you|we|they|he|she|it)\'ve\b', r'\1 have', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\b(I|you|we|they|he|she|it)\'d\b', r'\1 would', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\bI\'m\b', 'I am', sentence, flags=re.IGNORECASE)
        sentence = re.sub(r'\b(he|she|it)\'s\b', r'\1 is', sentence, flags=re.IGNORECASE)
        
        # å»é™¤å‰©ä½™çš„æ ‡ç‚¹ç¬¦å·ï¼Œä½†ä¿ç•™ç©ºæ ¼
        sentence = re.sub(r'[^\w\s]', '', sentence)
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        sentence = re.sub(r'\s+', ' ', sentence).strip()
        
        # è¿˜åŸæ­£ç¡®çš„å¤§å°å†™ï¼šå¥é¦–å­—æ¯å¤§å†™ï¼ŒI å¤§å†™ï¼Œå…¶ä»–å°å†™
        if sentence:
            words = sentence.lower().split()
            # å¥é¦–å­—æ¯å¤§å†™
            if words:
                words[0] = words[0].capitalize()
            # I æ€»æ˜¯å¤§å†™
            for i, word in enumerate(words):
                if word == 'i':
                    words[i] = 'I'
            sentence = ' '.join(words)
        
        return sentence
    
    def add_sentence(self, sentence):
        """æ·»åŠ å¥å­åˆ°é€‰ä¸­åˆ—è¡¨"""
        original_sentence = sentence.strip()
        if not original_sentence:
            return False
        
        # æ¸…ç†å¥å­
        cleaned_sentence = self.clean_sentence(original_sentence)
        if not cleaned_sentence:
            return False
        
        # æ£€æŸ¥å»é‡ï¼ˆä¸è€ƒè™‘å¤§å°å†™ï¼‰
        cleaned_lower = cleaned_sentence.lower()
        for existing in self.selected_sentences:
            if existing.lower() == cleaned_lower:
                print(f"âš ï¸  å¥å­å·²å­˜åœ¨: {existing}")
                return False
        
        self.selected_sentences.append(cleaned_sentence)
        self.save_selected_sentences()
        print(f"âœ… æ·»åŠ æˆåŠŸ: {cleaned_sentence}")
        if cleaned_sentence != original_sentence:
            print(f"   (åŸå¥: {original_sentence})")
        return True
    
    def remove_sentence(self, sentence_or_number):
        """ä»é€‰ä¸­åˆ—è¡¨ç§»é™¤å¥å­ï¼ˆæ”¯æŒå¥å­å†…å®¹æˆ–ç¼–å·ï¼‰"""
        # å°è¯•æŒ‰ç¼–å·åˆ é™¤
        if sentence_or_number.isdigit():
            index = int(sentence_or_number) - 1
            if 0 <= index < len(self.selected_sentences):
                removed_sentence = self.selected_sentences.pop(index)
                self.save_selected_sentences()
                print(f"ğŸ—‘ï¸  ç§»é™¤æˆåŠŸ: #{sentence_or_number} {removed_sentence}")
                return True
            else:
                print(f"âŒ ç¼–å·æ— æ•ˆ: {sentence_or_number} (èŒƒå›´: 1-{len(self.selected_sentences)})")
                return False
        
        # æŒ‰å¥å­å†…å®¹åˆ é™¤
        if sentence_or_number in self.selected_sentences:
            self.selected_sentences.remove(sentence_or_number)
            self.save_selected_sentences()
            print(f"ğŸ—‘ï¸  ç§»é™¤æˆåŠŸ: {sentence_or_number}")
            return True
        else:
            print(f"âŒ å¥å­ä¸å­˜åœ¨: {sentence_or_number}")
            return False
    
    def analyze_sentences(self):
        """åˆ†æé€‰ä¸­å¥å­çš„è¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡"""
        if not self.selected_sentences:
            return Counter(), Counter(), set(), set()
        
        word_counter = Counter()
        phoneme_counter = Counter()
        covered_words = set()
        covered_phonemes = set()
        
        for sentence in self.selected_sentences:
            words = self.extract_words_from_sentence(sentence)
            
            for word in words:
                word_counter[word] += 1
                if word in self.pick_words:
                    covered_words.add(word)
                
                phonemes = self.get_word_phonemes(word)
                for phoneme in phonemes:
                    phoneme_counter[phoneme] += 1
                    covered_phonemes.add(phoneme)
        
        return word_counter, phoneme_counter, covered_words, covered_phonemes
    
    def display_analysis(self):
        """æ˜¾ç¤ºå½“å‰åˆ†æç»“æœ"""
        word_freq, phoneme_freq, covered_words, covered_phonemes = self.analyze_sentences()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š å¥å­åˆ†æç»“æœ")
        print("=" * 80)
        
        # æ˜¾ç¤ºé€‰ä¸­çš„å¥å­ï¼ˆæ˜¾ç¤ºæ¸…ç†åçš„ç‰ˆæœ¬ï¼‰
        print(f"ğŸ“ é€‰ä¸­å¥å­ ({len(self.selected_sentences)}):")
        if self.selected_sentences:
            for i, sentence in enumerate(self.selected_sentences, 1):
                print(f"   {i:2d}. {sentence}")
        else:
            print("   (æ— é€‰ä¸­å¥å­)")
        
        if not self.selected_sentences:
            return
        
        # è¯æ±‡é¢‘ç‡åˆ†æ
        print(f"\nğŸ“š è¯æ±‡é¢‘ç‡åˆ†æ (æŒ‰é¢‘ç‡æ’åº):")
        print("-" * 60)
        
        if word_freq:
            print("ğŸ”¤ å‡ºç°çš„è¯æ±‡:")
            for word, freq in word_freq.most_common():
                status = "âœ…" if word in self.pick_words else "â“"
                print(f"   {word:15s}: {freq:3d} æ¬¡ {status}")
        
        # ç›®æ ‡è¯æ±‡è¦†ç›–æƒ…å†µ
        uncovered_words = self.pick_words - covered_words
        print(f"\nğŸ“‹ ç›®æ ‡è¯æ±‡è¦†ç›–æƒ…å†µ:")
        print(f"   æ€»ç›®æ ‡è¯æ±‡: {len(self.pick_words)}")
        print(f"   å·²è¦†ç›–: {len(covered_words)} ({len(covered_words)/len(self.pick_words)*100:.1f}%)")
        print(f"   æœªè¦†ç›–: {len(uncovered_words)} ({len(uncovered_words)/len(self.pick_words)*100:.1f}%)")
        
        if uncovered_words:
            print("   ğŸŸ¡ æœªè¦†ç›–çš„ç›®æ ‡è¯æ±‡:")
            uncovered_list = sorted(list(uncovered_words))
            for i in range(0, len(uncovered_list), 8):
                group = uncovered_list[i:i+8]
                print(f"      {', '.join(group)}")
        
        # éŸ³ç´ é¢‘ç‡åˆ†æ
        print(f"\nğŸ”Š éŸ³ç´ é¢‘ç‡åˆ†æ (æŒ‰é¢‘ç‡æ’åº):")
        print("-" * 60)
        
        if phoneme_freq:
            print("ğŸµ å‡ºç°çš„éŸ³ç´ :")
            for phoneme, freq in phoneme_freq.most_common():
                print(f"   /{phoneme:3s}/: {freq:3d} æ¬¡")
        
        # éŸ³ç´ è¦†ç›–æƒ…å†µ
        uncovered_phonemes = self.standard_phonemes - covered_phonemes
        print(f"\nğŸ¯ 39éŸ³ç´ è¦†ç›–æƒ…å†µ:")
        print(f"   æ€»éŸ³ç´ : 39")
        print(f"   å·²è¦†ç›–: {len(covered_phonemes)} ({len(covered_phonemes)/39*100:.1f}%)")
        print(f"   æœªè¦†ç›–: {len(uncovered_phonemes)} ({len(uncovered_phonemes)/39*100:.1f}%)")
        
        if uncovered_phonemes:
            print("   ğŸŸ¡ æœªè¦†ç›–çš„éŸ³ç´ :")
            uncovered_list = sorted(list(uncovered_phonemes))
            for i in range(0, len(uncovered_list), 8):
                group = uncovered_list[i:i+8]
                phoneme_str = ', '.join([f"/{p}/" for p in group])
                print(f"      {phoneme_str}")
        
        # ç»Ÿè®¡æ‘˜è¦
        total_words = sum(word_freq.values())
        total_phonemes = sum(phoneme_freq.values())
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»è¯æ±‡å®ä¾‹: {total_words}")
        print(f"   ä¸åŒè¯æ±‡æ•°: {len(word_freq)}")
        print(f"   æ€»éŸ³ç´ å®ä¾‹: {total_phonemes}")
        print(f"   ä¸åŒéŸ³ç´ æ•°: {len(phoneme_freq)}")
    
    def run_interactive(self):
        """è¿è¡Œäº¤äº’ç•Œé¢"""
        print("ğŸ¯ äº¤äº’å¼å¥å­åˆ†æå™¨")
        print("=" * 30)
        print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   ğŸ“ ç›´æ¥è¾“å…¥å¥å­ â†’ æ·»åŠ åˆ°åˆ†æåˆ—è¡¨")
        print("   ğŸ—‘ï¸  remove <å¥å­/ç¼–å·> â†’ åˆ é™¤æŒ‡å®šå¥å­")
        print("   ğŸ“Š show â†’ æ˜¾ç¤ºå½“å‰åˆ†æç»“æœ")
        print("   ğŸ“‹ list â†’ åˆ—å‡ºæ‰€æœ‰é€‰ä¸­å¥å­")
        print("   ğŸ§¹ clear â†’ æ¸…ç©ºæ‰€æœ‰å¥å­")
        print("   ğŸ‘‹ quit â†’ é€€å‡ºç¨‹åº")
        print("   ğŸ’¡ åˆ é™¤ä¾‹å­: remove 1 æˆ– remove This is a test")
        
        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        self.display_analysis()
        
        while True:
            try:
                user_input = input(f"\nğŸ® è¾“å…¥å‘½ä»¤: ").strip()
                
                if not user_input:
                    continue
                
                parts = user_input.split(None, 1)
                cmd = parts[0].lower()
                
                if cmd in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                elif cmd == 'remove' and len(parts) > 1:
                    self.remove_sentence(parts[1])
                    self.display_analysis()
                
                elif cmd == 'show':
                    self.display_analysis()
                
                elif cmd == 'list':
                    print(f"\nğŸ“ å½“å‰é€‰ä¸­å¥å­ ({len(self.selected_sentences)}):")
                    if self.selected_sentences:
                        for i, sentence in enumerate(self.selected_sentences, 1):
                            print(f"   {i:2d}. {sentence}")
                    else:
                        print("   (æ— é€‰ä¸­å¥å­)")
                
                elif cmd == 'clear':
                    confirm = input("âš ï¸  ç¡®è®¤æ¸…ç©ºæ‰€æœ‰å¥å­? (y/N): ")
                    if confirm.lower() in ['y', 'yes']:
                        self.selected_sentences = []
                        self.save_selected_sentences()
                        print("âœ… å·²æ¸…ç©ºæ‰€æœ‰å¥å­")
                        self.display_analysis()
                
                else:
                    # å°è¯•ä½œä¸ºå¥å­æ·»åŠ 
                    self.add_sentence(user_input)
                    self.display_analysis()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SentenceAnalyzer()
    analyzer.run_interactive()

if __name__ == "__main__":
    main()
