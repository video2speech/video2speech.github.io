#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import re
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import defaultdict, Counter
import nltk
try:
    from nltk.corpus import cmudict
    cmu_dict = cmudict.dict()
except:
    print("æ­£åœ¨ä¸‹è½½CMUè¯å…¸...")
    nltk.download('cmudict')
    from nltk.corpus import cmudict
    cmu_dict = cmudict.dict()

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SentenceAnalysisVisualizer:
    def __init__(self):
        # CMUéŸ³ç´ é›†åˆï¼ˆ39ä¸ªï¼‰
        self.cmu_phonemes = {
            # å…ƒéŸ³ (15ä¸ª)
            'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW',
            # è¾…éŸ³ (24ä¸ª)
            'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH'
        }
        
        # åŠ è½½æ•°æ®
        self.selected_data = self.load_selected_data()
        self.all_sentences = self.load_all_sentences()
        
    def load_selected_data(self):
        """åŠ è½½é€‰ä¸­çš„50å¥æ•°æ®"""
        try:
            with open('selected_50_sentences.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½é€‰ä¸­å¥å­æ•°æ®å¤±è´¥: {e}")
            return None
    
    def load_all_sentences(self):
        """åŠ è½½å…¨éƒ¨350å¥æ•°æ®"""
        try:
            with open('selected_sentences_analyzer.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½å…¨éƒ¨å¥å­æ•°æ®å¤±è´¥: {e}")
            return []
    
    def clean_sentence(self, sentence):
        """æ¸…ç†å¥å­"""
        sentence = sentence.lower()
        sentence = re.sub(r'[^\w\s]', '', sentence)
        sentence = re.sub(r'\s+', ' ', sentence).strip()
        return sentence
    
    def get_word_phonemes(self, word):
        """è·å–è¯æ±‡çš„éŸ³ç´ """
        word_lower = word.lower()
        if word_lower in cmu_dict:
            # å–ç¬¬ä¸€ä¸ªå‘éŸ³
            phonemes = cmu_dict[word_lower][0]
            # æ¸…ç†éŸ³ç´ ï¼ˆå»é™¤é‡éŸ³æ ‡è®°ï¼‰
            clean_phonemes = []
            for phoneme in phonemes:
                clean_phoneme = re.sub(r'\d', '', phoneme)
                if clean_phoneme in self.cmu_phonemes:
                    clean_phonemes.append(clean_phoneme)
            return clean_phonemes
        return []
    
    def analyze_sentences(self, sentences, title=""):
        """åˆ†æå¥å­é›†åˆçš„è¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡"""
        print(f"ğŸ”„ åˆ†æ {title} ({len(sentences)} ä¸ªå¥å­)...")
        
        word_counter = Counter()
        phoneme_counter = Counter()
        
        for sentence in sentences:
            words = self.clean_sentence(sentence).split()
            for word in words:
                word_counter[word] += 1
                # éŸ³ç´ åˆ†æ
                phonemes = self.get_word_phonemes(word)
                for phoneme in phonemes:
                    phoneme_counter[phoneme] += 1
        
        return word_counter, phoneme_counter
    
    def save_sentences_to_txt(self, sentences, filename):
        """ä¿å­˜å¥å­åˆ°txtæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            for sentence in sentences:
                f.write(sentence + '\n')
        print(f"âœ… å¥å­å·²ä¿å­˜åˆ°: {filename}")
    
    def plot_word_distribution(self, word_counter, title, filename, top_n=30):
        """ç»˜åˆ¶è¯é¢‘åˆ†å¸ƒå›¾"""
        # è·å–å‰Nä¸ªé«˜é¢‘è¯
        top_words = word_counter.most_common(top_n)
        words = [item[0] for item in top_words]
        frequencies = [item[1] for item in top_words]
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(15, 8))
        bars = plt.bar(range(len(words)), frequencies, color='steelblue', alpha=0.7)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title(f'{title} - è¯é¢‘åˆ†å¸ƒ (å‰{top_n}ä¸ª)', fontsize=16, fontweight='bold')
        plt.xlabel('è¯æ±‡', fontsize=12)
        plt.ylabel('é¢‘ç‡', fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾
        plt.xticks(range(len(words)), words, rotation=45, ha='right')
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… è¯é¢‘åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {filename}")
    
    def plot_phoneme_distribution(self, phoneme_counter, title, filename):
        """ç»˜åˆ¶éŸ³ç´ é¢‘ç‡åˆ†å¸ƒå›¾"""
        # è·å–æ‰€æœ‰éŸ³ç´ ï¼ŒæŒ‰é¢‘ç‡æ’åº
        phoneme_items = phoneme_counter.most_common()
        phonemes = [f"/{item[0]}/" for item in phoneme_items]
        frequencies = [item[1] for item in phoneme_items]
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(16, 10))
        bars = plt.bar(range(len(phonemes)), frequencies, color='darkgreen', alpha=0.7)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title(f'{title} - éŸ³ç´ é¢‘ç‡åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        plt.xlabel('éŸ³ç´ ', fontsize=12)
        plt.ylabel('é¢‘ç‡', fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾
        plt.xticks(range(len(phonemes)), phonemes, rotation=45, ha='right')
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{int(height)}', ha='center', va='bottom', fontsize=9)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… éŸ³ç´ é¢‘ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {filename}")
    
    def process_selected_sentences(self):
        """å¤„ç†é€‰ä¸­çš„50ä¸ªå¥å­"""
        print("\n" + "="*60)
        print("ğŸ“Š ä»»åŠ¡1: å¤„ç†é€‰ä¸­çš„50ä¸ªå¥å­")
        print("="*60)
        
        if not self.selected_data:
            print("âŒ æ— æ³•åŠ è½½é€‰ä¸­å¥å­æ•°æ®")
            return
        
        sentences = self.selected_data['selected_sentences']
        
        # 1. ä¿å­˜å¥å­åˆ°txt
        self.save_sentences_to_txt(sentences, 'selected_50_sentences.txt')
        
        # 2. åˆ†æè¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡
        word_counter, phoneme_counter = self.analyze_sentences(sentences, "é€‰ä¸­çš„50ä¸ªå¥å­")
        
        # 3. ç»˜åˆ¶å›¾è¡¨
        self.plot_word_distribution(word_counter, "é€‰ä¸­çš„50ä¸ªå¥å­", 'selected_50_word_distribution.png')
        self.plot_phoneme_distribution(phoneme_counter, "é€‰ä¸­çš„50ä¸ªå¥å­", 'selected_50_phoneme_distribution.png')
        
        print(f"ğŸ“ˆ ç»Ÿè®¡: æ€»è¯æ±‡å®ä¾‹ {sum(word_counter.values())}, ä¸åŒè¯æ±‡ {len(word_counter)}")
        print(f"ğŸ”Š ç»Ÿè®¡: æ€»éŸ³ç´ å®ä¾‹ {sum(phoneme_counter.values())}, ä¸åŒéŸ³ç´  {len(phoneme_counter)}")
    
    def process_remaining_sentences(self):
        """å¤„ç†å‰©ä½™çš„300ä¸ªå¥å­ï¼ˆ350-50ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“Š ä»»åŠ¡2: å¤„ç†å‰©ä½™çš„300ä¸ªå¥å­")
        print("="*60)
        
        if not self.selected_data or not self.all_sentences:
            print("âŒ æ— æ³•åŠ è½½å¿…è¦æ•°æ®")
            return
        
        # è·å–é€‰ä¸­å¥å­çš„ç´¢å¼•
        selected_indices = set(self.selected_data['selected_sentence_indices'])
        
        # è·å–å‰©ä½™å¥å­
        remaining_sentences = []
        for i, sentence in enumerate(self.all_sentences):
            if i not in selected_indices:
                remaining_sentences.append(sentence)
        
        print(f"ğŸ“Š å‰©ä½™å¥å­æ•°é‡: {len(remaining_sentences)}")
        
        # 1. ä¿å­˜å¥å­åˆ°txt
        self.save_sentences_to_txt(remaining_sentences, 'remaining_300_sentences.txt')
        
        # 2. åˆ†æè¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡
        word_counter, phoneme_counter = self.analyze_sentences(remaining_sentences, "å‰©ä½™çš„300ä¸ªå¥å­")
        
        # 3. ç»˜åˆ¶å›¾è¡¨
        self.plot_word_distribution(word_counter, "å‰©ä½™çš„300ä¸ªå¥å­", 'remaining_300_word_distribution.png')
        self.plot_phoneme_distribution(phoneme_counter, "å‰©ä½™çš„300ä¸ªå¥å­", 'remaining_300_phoneme_distribution.png')
        
        print(f"ğŸ“ˆ ç»Ÿè®¡: æ€»è¯æ±‡å®ä¾‹ {sum(word_counter.values())}, ä¸åŒè¯æ±‡ {len(word_counter)}")
        print(f"ğŸ”Š ç»Ÿè®¡: æ€»éŸ³ç´ å®ä¾‹ {sum(phoneme_counter.values())}, ä¸åŒéŸ³ç´  {len(phoneme_counter)}")
    
    def process_all_sentences(self):
        """å¤„ç†å…¨éƒ¨350ä¸ªå¥å­"""
        print("\n" + "="*60)
        print("ğŸ“Š ä»»åŠ¡3: å¤„ç†å…¨éƒ¨350ä¸ªå¥å­")
        print("="*60)
        
        if not self.all_sentences:
            print("âŒ æ— æ³•åŠ è½½å…¨éƒ¨å¥å­æ•°æ®")
            return
        
        # 1. ä¿å­˜å¥å­åˆ°txt
        self.save_sentences_to_txt(self.all_sentences, 'all_350_sentences.txt')
        
        # 2. åˆ†æè¯é¢‘å’ŒéŸ³ç´ é¢‘ç‡
        word_counter, phoneme_counter = self.analyze_sentences(self.all_sentences, "å…¨éƒ¨350ä¸ªå¥å­")
        
        # 3. ç»˜åˆ¶å›¾è¡¨
        self.plot_word_distribution(word_counter, "å…¨éƒ¨350ä¸ªå¥å­", 'all_350_word_distribution.png')
        self.plot_phoneme_distribution(phoneme_counter, "å…¨éƒ¨350ä¸ªå¥å­", 'all_350_phoneme_distribution.png')
        
        print(f"ğŸ“ˆ ç»Ÿè®¡: æ€»è¯æ±‡å®ä¾‹ {sum(word_counter.values())}, ä¸åŒè¯æ±‡ {len(word_counter)}")
        print(f"ğŸ”Š ç»Ÿè®¡: æ€»éŸ³ç´ å®ä¾‹ {sum(phoneme_counter.values())}, ä¸åŒéŸ³ç´  {len(phoneme_counter)}")
    
    def run_all_tasks(self):
        """è¿è¡Œæ‰€æœ‰ä»»åŠ¡"""
        print("ğŸ¯ å¥å­åˆ†æä¸å¯è§†åŒ–å·¥å…·")
        print("="*60)
        print("ä»»åŠ¡æ¦‚è§ˆ:")
        print("1. é€‰ä¸­çš„50ä¸ªå¥å­ â†’ txt + è¯é¢‘å›¾ + éŸ³ç´ å›¾")
        print("2. å‰©ä½™çš„300ä¸ªå¥å­ â†’ txt + è¯é¢‘å›¾ + éŸ³ç´ å›¾") 
        print("3. å…¨éƒ¨350ä¸ªå¥å­ â†’ txt + è¯é¢‘å›¾ + éŸ³ç´ å›¾")
        
        # æ‰§è¡Œä¸‰ä¸ªä»»åŠ¡
        self.process_selected_sentences()
        self.process_remaining_sentences()
        self.process_all_sentences()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print("="*60)
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("ğŸ“„ TXTæ–‡ä»¶:")
        print("   - selected_50_sentences.txt")
        print("   - remaining_300_sentences.txt")
        print("   - all_350_sentences.txt")
        print("ğŸ“Š è¯é¢‘åˆ†å¸ƒå›¾:")
        print("   - selected_50_word_distribution.png")
        print("   - remaining_300_word_distribution.png")
        print("   - all_350_word_distribution.png")
        print("ğŸ”Š éŸ³ç´ åˆ†å¸ƒå›¾:")
        print("   - selected_50_phoneme_distribution.png")
        print("   - remaining_300_phoneme_distribution.png")
        print("   - all_350_phoneme_distribution.png")

def main():
    # æ£€æŸ¥matplotlibæ˜¯å¦å¯ç”¨
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…matplotlib:")
        print("   pip install matplotlib")
        return
    
    visualizer = SentenceAnalysisVisualizer()
    visualizer.run_all_tasks()

if __name__ == "__main__":
    main()


