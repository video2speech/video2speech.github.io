#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import math
import re
from collections import defaultdict, Counter

class SimplePerplexityChecker:
    def __init__(self):
        """åŸºäºn-gramçš„ç®€å•å›°æƒ‘åº¦è®¡ç®—å™¨"""
        self.word_counts = defaultdict(int)
        self.bigram_counts = defaultdict(int)
        self.trigram_counts = defaultdict(int)
        self.total_words = 0
        self.vocab_size = 0
        
    def load_sentences(self, filename="selected_sentences_analyzer.json"):
        """åŠ è½½å¥å­æ•°æ®"""
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    sentences = json.load(f)
                print(f"ğŸ“ åŠ è½½äº† {len(sentences)} ä¸ªå¥å­")
                return sentences
            except Exception as e:
                print(f"âŒ åŠ è½½å¥å­æ–‡ä»¶å¤±è´¥: {e}")
                return []
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {filename}")
            return []
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        # è½¬å°å†™
        text = text.lower()
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™åŸºæœ¬ç»“æ„
        text = re.sub(r'[^\w\s]', '', text)
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def tokenize(self, text):
        """åˆ†è¯"""
        clean_text = self.clean_text(text)
        tokens = clean_text.split()
        return ['<s>'] + tokens + ['</s>']  # æ·»åŠ å¥å­å¼€å§‹å’Œç»“æŸæ ‡è®°
    
    def build_language_model(self, sentences):
        """æ„å»ºè¯­è¨€æ¨¡å‹"""
        print("ğŸ”„ æ„å»ºè¯­è¨€æ¨¡å‹...")
        
        all_tokens = []
        for sentence in sentences:
            tokens = self.tokenize(sentence)
            all_tokens.extend(tokens)
            
            # ç»Ÿè®¡unigram
            for token in tokens:
                self.word_counts[token] += 1
            
            # ç»Ÿè®¡bigram
            for i in range(len(tokens) - 1):
                bigram = (tokens[i], tokens[i + 1])
                self.bigram_counts[bigram] += 1
            
            # ç»Ÿè®¡trigram
            for i in range(len(tokens) - 2):
                trigram = (tokens[i], tokens[i + 1], tokens[i + 2])
                self.trigram_counts[trigram] += 1
        
        self.total_words = len(all_tokens)
        self.vocab_size = len(self.word_counts)
        
        print(f"âœ… è¯­è¨€æ¨¡å‹æ„å»ºå®Œæˆ")
        print(f"   æ€»è¯æ•°: {self.total_words}")
        print(f"   è¯æ±‡é‡: {self.vocab_size}")
        print(f"   Bigramæ•°: {len(self.bigram_counts)}")
        print(f"   Trigramæ•°: {len(self.trigram_counts)}")
    
    def get_word_probability(self, word):
        """è·å–è¯çš„æ¦‚ç‡ï¼ˆå¸¦å¹³æ»‘ï¼‰"""
        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        count = self.word_counts[word]
        return (count + 1) / (self.total_words + self.vocab_size)
    
    def get_bigram_probability(self, w1, w2):
        """è·å–bigramæ¦‚ç‡ï¼ˆå¸¦å¹³æ»‘ï¼‰"""
        bigram_count = self.bigram_counts[(w1, w2)]
        w1_count = self.word_counts[w1]
        
        if w1_count == 0:
            return 1 / self.vocab_size  # å¹³æ»‘å¤„ç†
        
        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        return (bigram_count + 1) / (w1_count + self.vocab_size)
    
    def get_trigram_probability(self, w1, w2, w3):
        """è·å–trigramæ¦‚ç‡ï¼ˆå¸¦å¹³æ»‘ï¼‰"""
        trigram_count = self.trigram_counts[(w1, w2, w3)]
        bigram_count = self.bigram_counts[(w1, w2)]
        
        if bigram_count == 0:
            return self.get_bigram_probability(w2, w3)  # å›é€€åˆ°bigram
        
        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        return (trigram_count + 1) / (bigram_count + self.vocab_size)
    
    def calculate_sentence_probability(self, sentence, model_type="trigram"):
        """è®¡ç®—å¥å­æ¦‚ç‡"""
        tokens = self.tokenize(sentence)
        log_prob = 0.0
        
        if model_type == "unigram":
            for token in tokens:
                prob = self.get_word_probability(token)
                log_prob += math.log(prob)
        
        elif model_type == "bigram":
            for i in range(len(tokens) - 1):
                prob = self.get_bigram_probability(tokens[i], tokens[i + 1])
                log_prob += math.log(prob)
        
        elif model_type == "trigram":
            # å‰ä¸¤ä¸ªè¯ç”¨bigram
            if len(tokens) >= 2:
                prob = self.get_bigram_probability(tokens[0], tokens[1])
                log_prob += math.log(prob)
            
            # å…¶ä½™ç”¨trigram
            for i in range(2, len(tokens)):
                prob = self.get_trigram_probability(tokens[i-2], tokens[i-1], tokens[i])
                log_prob += math.log(prob)
        
        return log_prob
    
    def calculate_perplexity(self, sentence, model_type="trigram"):
        """è®¡ç®—å›°æƒ‘åº¦"""
        tokens = self.tokenize(sentence)
        log_prob = self.calculate_sentence_probability(sentence, model_type)
        
        # å›°æƒ‘åº¦ = 2^(-log_prob / N)ï¼Œè¿™é‡Œç”¨eä¸ºåº•
        perplexity = math.exp(-log_prob / len(tokens))
        
        return perplexity, log_prob
    
    def analyze_all_sentences(self, sentences, model_type="trigram"):
        """åˆ†ææ‰€æœ‰å¥å­çš„å›°æƒ‘åº¦"""
        print(f"\nğŸ”„ ä½¿ç”¨ {model_type} æ¨¡å‹è®¡ç®—å›°æƒ‘åº¦...")
        
        results = []
        total = len(sentences)
        
        for i, sentence in enumerate(sentences, 1):
            print(f"â³ å¤„ç†ä¸­ ({i}/{total}): {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
            
            perplexity, log_prob = self.calculate_perplexity(sentence, model_type)
            
            results.append({
                'index': i,
                'sentence': sentence,
                'perplexity': perplexity,
                'log_probability': log_prob,
                'length': len(sentence.split()),
                'model_type': model_type
            })
        
        return results
    
    def analyze_results(self, results):
        """åˆ†æå›°æƒ‘åº¦ç»“æœ"""
        if not results:
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        perplexities = [r['perplexity'] for r in results]
        
        print("\n" + "=" * 80)
        print("ğŸ“Š å›°æƒ‘åº¦åˆ†æç»“æœ")
        print("=" * 80)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»å¥å­æ•°: {len(results)}")
        print(f"   æ¨¡å‹ç±»å‹: {results[0]['model_type']}")
        print(f"   å¹³å‡å›°æƒ‘åº¦: {sum(perplexities)/len(perplexities):.2f}")
        print(f"   ä¸­ä½æ•°å›°æƒ‘åº¦: {sorted(perplexities)[len(perplexities)//2]:.2f}")
        print(f"   æœ€ä½å›°æƒ‘åº¦: {min(perplexities):.2f}")
        print(f"   æœ€é«˜å›°æƒ‘åº¦: {max(perplexities):.2f}")
        
        # æŒ‰å›°æƒ‘åº¦æ’åº
        sorted_results = sorted(results, key=lambda x: x['perplexity'])
        
        # æ˜¾ç¤ºæœ€è‡ªç„¶çš„å¥å­ï¼ˆä½å›°æƒ‘åº¦ï¼‰
        print(f"\nğŸŒŸ æœ€è‡ªç„¶çš„å¥å­ (ä½å›°æƒ‘åº¦):")
        print("-" * 60)
        for i, result in enumerate(sorted_results[:10], 1):
            print(f"{i:2d}. å›°æƒ‘åº¦: {result['perplexity']:8.2f} | {result['sentence']}")
        
        # æ˜¾ç¤ºæœ€ä¸è‡ªç„¶çš„å¥å­ï¼ˆé«˜å›°æƒ‘åº¦ï¼‰
        print(f"\nâš ï¸  æœ€ä¸è‡ªç„¶çš„å¥å­ (é«˜å›°æƒ‘åº¦):")
        print("-" * 60)
        for i, result in enumerate(sorted_results[-10:], 1):
            print(f"{i:2d}. å›°æƒ‘åº¦: {result['perplexity']:8.2f} | {result['sentence']}")
        
        return sorted_results
    
    def save_results(self, results, filename="simple_perplexity_report.json"):
        """ä¿å­˜ç»“æœ"""
        try:
            # JSONæ ¼å¼
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # æ–‡æœ¬æ ¼å¼
            txt_filename = filename.replace('.json', '.txt')
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write("å¥å­å›°æƒ‘åº¦åˆ†ææŠ¥å‘Š (åŸºäºN-gram)\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"æ¨¡å‹ç±»å‹: {results[0]['model_type']}\n")
                f.write(f"æ€»å¥å­æ•°: {len(results)}\n\n")
                
                sorted_results = sorted(results, key=lambda x: x['perplexity'])
                
                f.write("æ‰€æœ‰å¥å­çš„å›°æƒ‘åº¦ (æŒ‰å›°æƒ‘åº¦æ’åº):\n")
                f.write("-" * 50 + "\n")
                
                for result in sorted_results:
                    f.write(f"\nå›°æƒ‘åº¦: {result['perplexity']:8.2f} | è¯æ•°: {result['length']:2d} | {result['sentence']}\n")
            
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°:")
            print(f"   ğŸ“„ {filename}")
            print(f"   ğŸ“„ {txt_filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    print("ğŸ¯ ç®€å•å›°æƒ‘åº¦æ£€æŸ¥å™¨ (åŸºäºN-gram)")
    print("=" * 50)
    
    checker = SimplePerplexityChecker()
    
    # åŠ è½½å¥å­
    sentences = checker.load_sentences()
    if not sentences:
        return
    
    # æ„å»ºè¯­è¨€æ¨¡å‹
    checker.build_language_model(sentences)
    
    # é€‰æ‹©æ¨¡å‹ç±»å‹
    print("\nğŸ“š å¯ç”¨æ¨¡å‹:")
    print("   1. Unigram (å•è¯æ¦‚ç‡)")
    print("   2. Bigram (äºŒå…ƒè¯­æ³•)")
    print("   3. Trigram (ä¸‰å…ƒè¯­æ³•ï¼Œæ¨è)")
    
    choice = input("\né€‰æ‹©æ¨¡å‹ (1-3, é»˜è®¤3): ").strip() or "3"
    model_types = {"1": "unigram", "2": "bigram", "3": "trigram"}
    model_type = model_types.get(choice, "trigram")
    
    try:
        # è®¡ç®—å›°æƒ‘åº¦
        results = checker.analyze_all_sentences(sentences, model_type)
        
        # åˆ†æç»“æœ
        sorted_results = checker.analyze_results(results)
        
        # ä¿å­˜ç»“æœ
        if sorted_results:
            save_choice = input("\nğŸ’¾ æ˜¯å¦ä¿å­˜ç»“æœ? (y/N): ").strip().lower()
            if save_choice in ['y', 'yes']:
                checker.save_results(results)
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()


